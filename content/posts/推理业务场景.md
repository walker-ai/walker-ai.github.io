---
title: 推理业务场景
subtitle:
date: 2025-10-11T21:01:05+08:00
slug: 3d7627b
draft: true
author:
  name:
  link:
  email:
  avatar:
description:
keywords:
license:
comment: true
weight: 0
hiddenFromHomePage: false
hiddenFromSearch: false
hiddenFromRelated: false
hiddenFromFeed: false
summary:
resources:
  - name: featured-image
    src: featured-image.jpg
  - name: featured-image-preview
    src: featured-image-preview.jpg
toc: true
math: true
lightgallery: false
password:
message:
repost:
  enable: false
  url:

# See details front matter: https://fixit.lruihao.cn/documentation/content-management/introduction/#front-matter
---

<!--more-->

> 参考文献：[https://zhuanlan.zhihu.com/p/652697200](https://zhuanlan.zhihu.com/p/652697200)


### 常见业务关注场景 & 指标

#### “低延迟”场景

|场景示例	|核心需求	|优化目标|
|:--: | :--: | :--: |
|实时在线推理（如推荐系统召回、搜索引擎排序、人机对话 LLM）|	必须在百毫秒甚至几十毫秒内返回结果，否则用户流失。|	P99 延迟，确保绝大多数用户都有流畅体验。|
|自动驾驶/机器人控制	|决策必须瞬间完成，任何延迟都可能导致安全问题。	|平均延迟 和 最坏情况延迟（最小化 P100）。|
|金融高频交易	|极快的决策和执行速度是收益的关键。	|绝对的低延迟。|

#### "高吞吐量"场景

|场景示例	|核心需求	|优化目标|
|:--: | :--: | :--: |
|离线训练（ML Model Training）|	尽快完成模型训练，但对每一步迭代的时间不敏感。|	每秒浮点运算次数（FLOPS）、数据吞吐量。|
|批量预测/数据挖掘（如夜间跑批、数据 ETL）|	需要高效处理海量数据，在规定时间内完成所有任务。|	整体吞吐量（RPS/QPS），最大化 GPU 利用率。|
|异步或非实时任务（如邮件过滤、大规模图片打标签）|	任务可以排队等待一段时间。|	系统总容量和成本效率。|

#### FLOPs

FLOPs（Floating Point Operations）本身指浮点运算次数，可以简单理解为评估计算量的单位，即FLOPs可以用来衡量一个模型/算法的总体复杂度（即所需要进行的运算量）。FLOPs通常在论文和技术报告中流行的单位是

GFLOPs: 1GFLOPs = 10^9 FLOPs。有时候也用 MACs 或者 MAdd(Multiply–Accumulate Operations) 代替FLOPs作为评估模型计算量的单位，1 MACs大约等价于 2FLOPs。

从定义中可以看出FLOPs是一个与计算性能相关的指标，那么它的作用主要体现在当你在训练、部署模型时需要对模型的计算性能进行考虑。

- 比如训练一个模型（LLM）时，通常通过计算模型训练全部FLOPs与使用GPU峰值的FLOPS以及GPU的利用率，来大致估算训练所需的时间和卡数。

$$
\text{Total Time} = \dfrac{\text{FLOPs}_{模型单次step} * Step}{\text{FLOPS} _{GPU峰值} * \text{GPU数量} * \text{GPU利用率}} \approx \dfrac{\text{FLOPs} _{模型训练全部}}{\text{FLOPS} _{GPU峰值} * \text{GPU数量} * \text{GPU利用率}}
$$

- 比如能够通过计算模型每个部分FLOPs得到哪个部分计算量最大，从而分析如何对模型计算性能进行优化
- 比如当几个模型的效果体验差不多，尽量选择FLOPs小的模型进行部署来满足服务的要求

**如何计算FLOPs？**

1. 根据计算公式和模型结构手动推算
2. 借助第三方工具：calflops、ptflops、thop、torchstat、torchsumary、fvcore

- 手动推导模型的FLOPs时只推导前向传播，大部分情况默认模型后向传播的计算量是前向传播的2倍， 总共FLOPs是前向的3倍。(结论出自 —— [https://epochai.org/blog/backward-forward-FLOP-ratio](https://epochai.org/blog/backward-forward-FLOP-ratio))
- 由于LLM模型参数过大，占用显存过多，有时候为了降低显存在训练采用将中间参数保留在内存里——激活重计算。因此推导LLM训练时FLOPs如果考虑到中间参数的激活重计算的过程，需要计算整体FLOPs需要再加一份前向计算量，即1(前向） + 2(反向）+ 1(激活重计算）= 4 倍 计算量。 （结论出自 —— [https://arxiv.org/pdf/2205.05198.pdf](https://arxiv.org/pdf/2205.05198.pdf)）    
- 手动推导模型的FLOPs时，优先推导整个过程计算量占大头部分，通常忽略激活函数、layer normalize，softmax等等部分计算量。


**以 Transformer、MHA、GQA 为例**

##### MHA

$$
Q, K, V = xW_Q, xW_K, xW_V \\
softmax(\dfrac{QK^{T}}{\sqrt{d}})V
o = xW_O
$$

第一步，每个矩阵乘法的形状都是 `[b, s, h] * [h, h] = 2bsh^2`，共计 `6bsh^2`；
第二步，Q，K相乘为 `[b, s, h] * [h, h] = 2bsh^2`，之后得到的分数与V相乘是 `[b, s, s] * [b, s, h] = 2bs^2h`，共计 `4bs^2h`。
第三步，$xW_O$ 为 `[b, s, h] * [h, h] = 2bsh^2`。

所以 MHA 的总共计算量为 `8bsh^2 + 4bs^2h`

> MLP：两个MLP做的分别都是 `[b, s, h] * [h, 4h] = 8bsh^2`，`[b, s, 4h] * [h, h] = 8bsh^2`
> 所以一个transformer block的总共FLOPs为：24bsh^2 + 4bs^2h

#### P99延迟

指的是在所有请求中，有 **99%** 的请求，其处理时间（延迟）不会超过这个值。

例如，假设服务器的某个接口/api 收到 100 个请求，有 99 个请求的响应时间小于 120 毫秒，1 个请求耗时 > 120 毫秒，因此，我们可以说/api — P99 响应时间 = 120 毫秒

许多在线服务（搜索引擎、推荐系统、LLM聊天）都会以P99作为其内部或对客户的SLA（服务等级协议）目标
