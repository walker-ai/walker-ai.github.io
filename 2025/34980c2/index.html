<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>GPU及CUDA基本概念 - yitao's Blog</title><meta name=author content="yitao">
<meta name=description content="yitao的学习笔记"><meta name=keywords content='Blog,Blockchain,Book,Movie,Life,Journey'><meta itemprop=name content="GPU及CUDA基本概念"><meta itemprop=description content="yitao的学习笔记"><meta itemprop=datePublished content="2025-08-22T01:02:38+08:00"><meta itemprop=dateModified content="2025-08-22T01:02:38+08:00"><meta itemprop=wordCount content="13900"><meta itemprop=image content="https://yitaonote.com/logo.png"><meta itemprop=keywords content="找工作"><meta property="og:url" content="https://yitaonote.com/2025/34980c2/"><meta property="og:site_name" content="yitao's Blog"><meta property="og:title" content="GPU及CUDA基本概念"><meta property="og:description" content="yitao的学习笔记"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-22T01:02:38+08:00"><meta property="article:modified_time" content="2025-08-22T01:02:38+08:00"><meta property="og:image" content="https://yitaonote.com/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yitaonote.com/logo.png"><meta name=twitter:title content="GPU及CUDA基本概念"><meta name=twitter:description content="yitao的学习笔记"><meta name=application-name content="yitao's Blog"><meta name=apple-mobile-web-app-title content="yitao's Blog"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=https://yitaonote.com/2025/34980c2/ title="GPU及CUDA基本概念 - yitao's Blog"><link rel=prev type=text/html href=https://yitaonote.com/2025/adc4087/ title=秋招timeline><link rel=alternate type=text/markdown href=https://yitaonote.com/2025/34980c2/index.md title="GPU及CUDA基本概念 - yitao's Blog"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://unpkg.com/@fortawesome/fontawesome-free@6.7.1/css/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/@fortawesome/fontawesome-free@6.7.1/css/all.min.css></noscript><link rel=preload href=https://unpkg.com/animate.css@4.1.1/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"GPU及CUDA基本概念","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yitaonote.com\/2025\/34980c2\/"},"genre":"posts","wordcount":13900,"url":"https:\/\/yitaonote.com\/2025\/34980c2\/","datePublished":"2025-08-22T01:02:38+08:00","dateModified":"2025-08-22T01:02:38+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"yitao"},"description":""}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yitao's Blog"><img class=logo src='https://avatars.githubusercontent.com/u/75578057?s=400&u=523ba40a796b5dc3cdd41fae4a1f038537d2dc67&v=4' alt="yitao's Blog" height=32 width=32><span class=header-title-text>yitao's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/collection/><i class="fa-solid fa-bookmark fa-fw fa-sm" aria-hidden=true></i> 集子</a></li><li class=menu-item><a class=menu-link href=/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yitao's Blog"><img class=logo src='https://avatars.githubusercontent.com/u/75578057?s=400&u=523ba40a796b5dc3cdd41fae4a1f038537d2dc67&v=4' alt="yitao's Blog" height=26 width=26><span class=header-title-text>yitao's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/collection/><i class="fa-solid fa-bookmark fa-fw fa-sm" aria-hidden=true></i> 集子</a></li><li class=menu-item><a class=menu-link href=/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>GPU及CUDA基本概念</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
yitao</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/%E6%89%BE%E5%B7%A5%E4%BD%9C/ class=post-category title="分类 - 找工作"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> 找工作</a></span></div><div class=post-meta-line><span title="发布于 2025-08-22 01:02:38"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2025-08-22>2025-08-22</time></span>&nbsp;<span title="13900 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 14000 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 28 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#gpu-架构>GPU 架构</a><ul><li><a href=#smstreaming-multiprocessor结构>SM（Streaming Multiprocessor）结构</a></li></ul></li><li><a href=#hopper架构>Hopper架构</a></li><li><a href=#gpu线程调度>GPU线程调度</a><ul><li><a href=#sm分配>SM分配</a></li><li><a href=#线程调度执行>线程调度执行</a></li><li><a href=#延迟隐藏>延迟隐藏</a></li><li><a href=#warp的划分>warp的划分</a></li><li><a href=#warp-divergence>warp divergence</a></li><li><a href=#sm占用率>SM占用率</a></li></ul></li><li><a href=#roofline-model>Roofline Model</a><ul><li><a href=#概念>概念</a></li></ul></li><li><a href=#bank-conflict-与-避免方法>Bank Conflict 与 避免方法</a><ul><li><a href=#bank概念>Bank概念</a></li><li><a href=#bank-conflict>Bank Conflict</a></li><li><a href=#使用-padding-缓解-bank-conflict>使用 Padding 缓解 Bank Conflict</a></li><li><a href=#使用-swizzle-缓解-bank-conflict>使用 Swizzle 缓解 Bank Conflict</a></li></ul></li><li><a href=#cuda-stream--cuda-graph>CUDA Stream & CUDA Graph</a><ul><li><a href=#cuda-stream>CUDA Stream</a><ul><li><a href=#stream之间的操作cuda-event>stream之间的操作：cuda event</a></li></ul></li><li><a href=#cuda-graph>CUDA Graph</a></li></ul></li><li><a href=#ptx--sass--libdevice>PTX & SASS & libdevice</a><ul><li><a href=#ptxparallel-thread-execution>PTX(Parallel Thread Execution)</a></li><li><a href=#sass-streaming-assembly>SASS (Streaming ASSembly)</a></li><li><a href=#libdevice>libdevice</a></li><li><a href=#转换流程>转换流程</a></li></ul></li><li><a href=#cutlass--cute>CUTLASS & CUTE</a><ul><li><a href=#cutlass>CUTLASS</a></li><li><a href=#cute>CUTE</a></li></ul></li><li><a href=#global-memory的访存合并>Global Memory的访存合并</a><ul><li><a href=#传输延迟>传输延迟</a></li><li><a href=#合并访存>合并访存</a></li></ul></li><li><a href=#常用的profile工具和方法>常用的profile工具和方法</a></li></ul></nav></div></div><div class=content id=content><h2 id=gpu-架构 class=heading-element><span>GPU 架构</span>
<a href=#gpu-%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>参考文献：<a href=https://zhuanlan.zhihu.com/p/12083951223 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/12083951223</a></p></blockquote><h3 id=smstreaming-multiprocessor结构 class=heading-element><span>SM（Streaming Multiprocessor）结构</span>
<a href=#smstreaming-multiprocessor%e7%bb%93%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4mhy3ink1j31401t6tff.jpg alt=image></p><p>其中包含一些核心组件：</p><ul><li><strong>CUDA core</strong>（即<strong>Streaming Processor，SP</strong>）：其中包含整数处理单元和单精度浮点数处理单元，用于执行基本的数值运算。不同架构中CUDA core数量不同，这个数量在一定程度上体现了GPU的计算能力（并不是完全决定，还有如时钟频率，内存带宽，指令集等其他影响因素）</li></ul><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4mi3cr005j30ki0d7q3n.jpg alt=image></p><ul><li><p><strong>Register File</strong>：寄存器文件。存放指令操作数；也有一些特殊寄存器用于存放系统变量，例如grid维度，thread blcok维度，线程id等等。</p></li><li><p><strong>Loca/Store Units（LD/ST）</strong>：执行内存（显存、shared memory）读写数据命令。</p></li><li><p><strong>Special Function Units（SFU）</strong>：执行一些特殊函数，如sqrt，sin，cos等。</p></li><li><p><strong>Warp Scheduler</strong>：GPU线程调度执行时是以一组线程（warp）为单位的，Warp Scheduler从驻留在SM上的warp中选
择合适的warp用于执行。</p></li><li><p><strong>Dispatch Unit</strong>：负责从Warp Scheduler选中的线程中取出要执行的指令发射到CUDA core去执行。</p></li><li><p><strong>Shared Memory/L1 Cache</strong>：shared memory可以用于同一个thread block中的线程间互相通信，是可以通过编程读写的。L1 Cache则不能被编程，由GPU决定存放和淘汰内容。这里把Shared Memory和L1 Cache放在一起是因为它们在物理存储上是共用的，<strong>可以通过API控制两者的配比。</strong></p></li><li><p><strong>常量、纹理缓存与全局内存缓存</strong>：有的gpu架构这三类内存结构也同样存在于一个SM中：</p></li></ul><div class="details admonition tip open disabled"><div class="details-summary admonition-title"><i class="icon fa-fw fa-regular fa-lightbulb" aria-hidden=true></i>以上以 Fermi 架构举例，后续的新架构都是在其基础上的进一步发展，例如更多的CUDA core数量，更大的内存容量，更高的IO带宽，以及增加一些新的组件如Tensor Core，RT Core等</div><div class=details-content><div class=admonition-content></div></div></div><h2 id=hopper架构 class=heading-element><span>Hopper架构</span>
<a href=#hopper%e6%9e%b6%e6%9e%84 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>参考文献：<a href=https://zhuanlan.zhihu.com/p/1900569635532800833 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/1900569635532800833</a></p></blockquote><p>由于实习期间使用的都是 H20 集群，因此学习一下 Hopper 架构的特点</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4mng9yb41j31400lvn2c.jpg alt=image></p><h2 id=gpu线程调度 class=heading-element><span>GPU线程调度</span>
<a href=#gpu%e7%ba%bf%e7%a8%8b%e8%b0%83%e5%ba%a6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><p>一个CUDA kernel对应一个grid，一个grid分成若干个thread block，每个thread block中包含若干线程。那么从硬件角度看，GPU在调度Kernel时，粗粒度看分成两个阶段，即<strong>SM分配</strong>和<strong>线程调度执行</strong>。</p><h3 id=sm分配 class=heading-element><span>SM分配</span>
<a href=#sm%e5%88%86%e9%85%8d class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>GPU会根据SM资源情况来进行block的分配，只有当SM上剩余资源足够满足一个block的总资源需求时，SM才有可能被分配给当前block。这里的资源主要指<mark class=mark-default>共享内存</mark>和<mark class=mark-default>寄存器</mark>，即SM上的Shared Memory及寄存器是不能超额分配的；但CUDA core，SFU等指令执行单元则是可以超额分配的。</p><p>例如Fermi架构中每个SM上CUDA core只有32个，但是一个SM最大可以支持1536个线程，因为虽然不能同时执行这1536个线程的指令，但是可以对CUDA core进行时分复用，执行不同线程时进行上下文切换即可，GPU上的线程上下文切换效率远高于CPU上的线程切换，因为线程的寄存器已经事先分配好，切换时并不需要像CPU上那样把寄存器状态在主机内存上进行换入换出操作。</p><p>当然一个SM上最多能够支持多少个线程还是有硬性限制的，具体限制数值取决于具体架构。一个thread block分配到一个SM上之后就会驻留下来直到执行完毕，同一个grid中的thread block可能分配到不同的SM上，反过来同一个SM也可能被分配给来自不同grid的多个thread block；一个thread blcok分配到一个SM上之后，则称为Active thread block，一个SM被分配thread block之后则称为Active SM。</p><h3 id=线程调度执行 class=heading-element><span>线程调度执行</span>
<a href=#%e7%ba%bf%e7%a8%8b%e8%b0%83%e5%ba%a6%e6%89%a7%e8%a1%8c class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>在调度执行时，GPU并不会针对单个线程去调度，而是把一个thread block进一步划分为若干个Warp（线程束）为单位进行调度，一个warp包含32个线程（目前所有架构上都是32），每个Warp Sheduler每个时钟周期（cycle）选择一个Warp将其指令通过Dispatch Unit发射到执行单元。也就是说一个thread block中的线程只是在逻辑上是同步执行，但是在硬件层面则不一定。可以从资源分配和调度的角度对此与主机上的调度做一个类比，thread block类似于主机上的进程，是资源分配单位，warp类似于主机上的线程，是调度执行单位。</p><p>Warp有如下几种状态：</p><ul><li><strong>Active</strong>：一个Acitve的thread block中的Warp都是Active的，也即已经分配到SM的Warp都是Active Warp。</li><li><strong>Stalled</strong>：当前Cycle暂时不能执行下一条指令的Warp，有多种情况可能导致Warp处于Stalled状态，常见的有：<ul><li>thread block内的线程同步，部分先执行的Warp必须等待同一个block内的后执行的Warp到达同一个同步点。</li><li>线程需要的数据还没有传输完成，需要等待数据。</li><li>下一条指令依赖于上一条指令的输出，但是上一条指令还没有执行完成。</li></ul></li><li><strong>Eligible</strong>：当前Cycle已经准备好可以执行下一条指令的Warp。Warp Scheduler在选择Warp时只会从Eligible Warp中进行选取。</li><li><strong>Selected</strong>：被Warp Scheduler选中准备执行其中线程下一条指令的Warp。</li></ul><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4mmyhwmk8j30bp0klgm9.jpg alt=image></p><p>注意一个Warp中线程的同步性仅仅体现在调度上，即一个Warp中的所有线程是同时被Scheduler选中的，但是在指令发射和执行上则不一定了，至少可以从几个方面去看：</p><ul><li><p>Instruction replay：同一条指令发射多次称为instruction replay。CUDA指令可以分为不同类型，不同类型指令使用不同类型的执行单元执行，例如memory的读写指令是通过LD/ST units执行，特殊函数如sin，cos是通过SFU执行，即并非所有指令都是通过CUDA core执行。而对于某种类型执行单元来说，一个SM上可能都没有32个，例如Fermi中一个SM上只有4个SFU，那么一个Warp执行cos指令时，就需要发射和执行8次。</p></li><li><p>Warp divergence：当一个Warp中的线程因为分支条件(如if else)不同而走到不同的路径时会造成warp divergence，此时不同分支的线程会形成不同分组，这些分组串行在不同的cycle执行</p></li></ul><h3 id=延迟隐藏 class=heading-element><span>延迟隐藏</span>
<a href=#%e5%bb%b6%e8%bf%9f%e9%9a%90%e8%97%8f class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>延迟隐藏就是指通过并行执行，充分利用硬件执行单元，使得不同操作能够在时间上重叠，最大化某一种或几种操作的吞吐量（延迟隐藏的主要优化目标是吞吐量，而不是单个操作的时长）。比如这里的<strong>当Warp等待内存时，SM立即切换其他就绪Warp</strong>。</p><p>了解详情可以进一步学习：<strong>利特尔法则（little&rsquo;s law）</strong></p><div class="details admonition note open disabled"><div class="details-summary admonition-title"><i class="icon fa-fw fa-solid fa-pencil-alt" aria-hidden=true></i>以上三个步骤：<strong>SM资源分配、线程调度执行、延迟隐藏</strong> 构成了 block 调度执行的完整流程。</div><div class=details-content><div class=admonition-content></div></div></div><h3 id=warp的划分 class=heading-element><span>warp的划分</span>
<a href=#warp%e7%9a%84%e5%88%92%e5%88%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>CUDA线程与数组在内存中的排布类似，也分为逻辑视图和硬件视图，在逻辑视图上，一个thread block可能是1到3维的，但是在硬件视图上，这些线程上仍然是1维排布的，即按row-major方式将连续的32个线程组成一个Warp，用公式表达就是：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// thread_id是按row major方式将线程在block中的3维坐标转为1维线性坐标，
</span></span></span><span class=line><span class=cl><span class=c1>// 维度从内到外为x, y, z（下面这个转换方式对于1,2,3维thread block都是通用的）
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>thread_id</span> <span class=o>=</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>z</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// warp_id相同的线程就属于同一个Warp
</span></span></span><span class=line><span class=cl><span class=c1>// warpSize也是一个内置变量，可以在kernel函数中直接引用，表示warp大小，目前来说是32
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>warp_id</span> <span class=o>=</span> <span class=n>thread_id</span> <span class=o>/</span> <span class=n>warpSize</span></span></span></code></pre></td></tr></table></div></div><div class="alert alert-tip"><p class=alert-title><svg class="icon" viewBox="0 0 16 16" width="16" height="16"><path d="M8 1.5c-2.363.0-4 1.69-4 3.75.0.984.424 1.625.984 2.304l.214.253c.223.264.47.556.673.848.284.411.537.896.621 1.49a.75.75.0 01-1.484.211c-.04-.282-.163-.547-.37-.847a8.456 8.456.0 00-.542-.68c-.084-.1-.173-.205-.268-.32C3.201 7.75 2.5 6.766 2.5 5.25 2.5 2.31 4.863.0 8 0s5.5 2.31 5.5 5.25c0 1.516-.701 2.5-1.328 3.259-.095.115-.184.22-.268.319-.207.245-.383.453-.541.681-.208.3-.33.565-.37.847a.751.751.0 01-1.485-.212c.084-.593.337-1.078.621-1.489.203-.292.45-.584.673-.848.075-.088.147-.173.213-.253.561-.679.985-1.32.985-2.304.0-2.06-1.637-3.75-4-3.75zM5.75 12h4.5a.75.75.0 010 1.5h-4.5a.75.75.0 010-1.5zM6 15.25a.75.75.0 01.75-.75h2.5a.75.75.0 010 1.5h-2.5A.75.75.0 016 15.25z"/></svg>提示</p><p>当一个thread block中的线程总数不是32的整数倍时，从编程视角看，最后一个Warp是不满32个线程的，但是在GPU硬件上仍然会占用32个线程的资源，只是其中部分线程会被标识为不活跃线程，这种情况下就会造成一些资源的浪费，所以在实际使用时，要尽量保证thread block中的线程数为32的整数倍。</p></div><h3 id=warp-divergence class=heading-element><span>warp divergence</span>
<a href=#warp-divergence class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>Warp Divergence是指同一个Warp中的不同线程由于分支条件（例如if else）不同而进入不同的代码分支的情况。这个在CPU上的多线程不是太大问题，但是在CUDA中则会导致性能下降，因为在发生Warp Divergence时，Warp中走不同分支的线程会串行化执行，以一个2路分支为例，如下伪代码：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>condition</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>	<span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>假设Warp内前一半线程condition 为true ，后一半线程condition为false，那么执行时两部分线程就分开串行执行了，每一部分线程执行时，另一部分线程标识为不活跃线程，但是任然占用执行单元。这种情况下一方面因为串行化增加了整体执行时间，另一方面不活跃线程降低了资源利用率。如下图所示（coherent部分表示没有分支的代码）：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4mle9i5pqj30ru0flgmu.jpg alt=image></p><p>一个Warp一次执行一条公共指令。如果Warp中的线程由于数据依赖而发生条件分支发散，则warp会执行每个需要的分支路径，同时禁用不在该路径执行的线程。<strong>因此当一个 Warp中的32个线程都执行相同的执行路径时，Warp的效率是最高的。</strong></p><p>编译器可能会进行循环展开，或者会通过<strong>分支预测</strong>来优化短的if或switch块。在这些情况下，所有Warp都不会发散。程序员还可以使用#pragma unroll指令控制循环展开。如下图所示，每个线程执行不同数量的循环迭代，循环迭代的数量在四和八之间变化。在前四次迭代中，所有线程都是活动的并执行A。在剩余的迭代中，一些线程执行A，而其他线程因为已经完成它们的迭代而不活动。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4mlbow3lsj30v80gwjs3.jpg alt=image></p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4mlbvgqohj30vr0fkt9i.jpg alt=image></p><div class="details admonition warning open disabled"><div class="details-summary admonition-title"><i class="icon fa-fw fa-solid fa-exclamation-triangle" aria-hidden=true></i>Warp Divergence对性能有损害，所以在可能的前提下，应该尽量通过重新设计程序逻辑，算法等方式避免或减少这种情况的发生。</div><div class=details-content><div class=admonition-content></div></div></div><h3 id=sm占用率 class=heading-element><span>SM占用率</span>
<a href=#sm%e5%8d%a0%e7%94%a8%e7%8e%87 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h2 id=roofline-model class=heading-element><span>Roofline Model</span>
<a href=#roofline-model class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>参考文献：<a href=https://zhuanlan.zhihu.com/p/24066764550 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/24066764550</a></p></blockquote><h3 id=概念 class=heading-element><span>概念</span>
<a href=#%e6%a6%82%e5%bf%b5 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>Roofline Model能帮助我们判断cuda 程序是<strong>显存瓶颈(memory bound)<strong>还是</strong>计算瓶颈(compute bound)</strong>，以及判断当前资源利用率的情况。</p><p>Roofline Model的横坐标是计算强度(computational intensity)，单位是FLOP/B，即每读一个Byte所能产生的FLOP数。例如以下base matrix multiply代码:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>cudaMatrixMultiplyBase</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>A</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>C</span><span class=p>,</span> <span class=kt>int</span> <span class=n>M</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> <span class=kt>int</span> <span class=n>K</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>row</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>col</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>y</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>y</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>y</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>row</span> <span class=o>&lt;</span> <span class=n>M</span> <span class=o>&amp;&amp;</span> <span class=n>col</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=kt>float</span> <span class=n>sum</span> <span class=o>=</span> <span class=mf>0.0f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>k</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>k</span> <span class=o>&lt;</span> <span class=n>K</span><span class=p>;</span> <span class=o>++</span><span class=n>k</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>sum</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>row</span> <span class=o>*</span> <span class=n>K</span> <span class=o>+</span> <span class=n>k</span><span class=p>]</span> <span class=o>*</span> <span class=n>B</span><span class=p>[</span><span class=n>k</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>col</span><span class=p>];</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>row</span> <span class=o>*</span> <span class=n>N</span> <span class=o>+</span> <span class=n>col</span><span class=p>]</span> <span class=o>=</span> <span class=n>sum</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>它的<strong>computational intensity</strong>是0.25，因为每次2次计算（乘加）需要读8个Byte</p><p>Roofline Model是描述一个进程的性能与它所在的硬件的关系，横坐标是计算强度，纵坐标是FLOP/s，如下图所示：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4monuvqgvj31400r475a.jpg alt=image></p><p>其中 <strong>红线（屋檐）和绿线（屋顶）</strong> 与具体硬件有关，红线的斜率是内存带宽，不同的硬件斜率不同（带宽越大，斜率越高），越靠近红线说明对带宽资源利用率越高；<strong>任何一个点到原点的斜率不可能超过硬件最大带宽</strong> $\beta$</p><p>绿线是硬件的计算峰值，高度越靠近绿线，说明对计算资源利用率越高；<strong>任何一个点的P值(纵坐标)不可能超过</strong> $\pi$</p><p><strong>计算强度D是有可能超过 $I_{max}$ 的</strong>，假设某台机器A计算峰值10GFLOPS，内存带宽是10GB/s，那么 $I_{max}=1$ 。假设某个模型疯狂重复计算，对1GB的数据计算5次乘加，计算量 C=10GFLOPs，那么计算强度 $I=10$</p><hr><p>为了加深理解，分别举几个示例说明图中的7个点是如何产生的：</p><p>假设：机器A计算峰值10GFLOPS，内存带宽是10GB/s，那么 $I_{max}=1$，下面的示例都发生在机器A上：</p><p>有一个<strong>计算强度高的模型</strong> $D_h$，读取10GB的数据产生100GFLOPs的计算， $I=10$；</p><p>另外有一个<strong>计算强度低的模型</strong> $D_l$，读取10GB的数据只产生5GFLOPs的计算， $I=0.5$；</p><ul><li><p>$x_2, x_3$ 不可能在现实中出现，正如上面所说，$x_2$ 超过了带宽限制，$x_3$ 超过了硬件计算限制，</p></li><li><p>举个例：还是对于机器A，计算峰值10GFLOPS，内存带宽是10GB/s，假设有一个计算强度很高的模型，读取1GB数据计算100GFLOPs，计算强度为100，但实际上由于受到计算峰值的限制，P=10GFLOPS，此时计算的斜率为 $\dfrac{10}{100}=0.1<1=\beta$</p></li><li><p>$x_1, x_4$ 是在计算强度很低的模型中可能出现，以模型 $D_l$ 为例，$x_1$ 出现的原因是由于程序实现问题导致计算性能都很低：例如计算5GFLOPS用了2s（理想情况是0.5s）导致纵坐标P比较低：($I=0.5, P=2.5$)，<strong>属于工程提升空间很大，不需要增加硬件性能就可大幅度提升性能。</strong></p></li><li><p>$x_1$ 优化后变为 $x_4$，计算5GFLOPS只用了1s（理想情况是0.5s，但是受限于带宽，1s只能传输10GB数据），已经十分接近屋檐这条线了，但计算强度太低了，优化方向是提升计算强度（只能改变模型，属于模型的计算强度透支）来增加性能，如果算法没办法改进的情况下，那提升性能的方式只能堆更高的带宽了。 $x_4$ 属于<strong>内存带宽受限型（模型不动的情况下，无脑增加带宽）</strong>。</p></li><li><p>$x_5$ 属于把硬件资源都用满了（计算和带宽全都吃满），且模型的计算强度也被挖掘得刚刚好。其性能即受限于硬件带宽又受限于计算资源，增加任何一项硬件资源（带宽和计算）都没法提升性能。“刚刚好”的意思是计算强度和硬件刚好匹配，其即使增加了硬件计算和带宽，不改变模型的情况下，也没办法提升性能。它即受限于硬件，又受限于模型。</p></li><li><p>$x_6$ 也属于把硬件资源都用满了，但模型本身的计算强度还有很大潜力，属于硬件压制了模型。<strong>只需要堆计算资源性能就可以提升，模型和带宽都不用动，属于计算受限型</strong>。典型示例就是计算强度高的模型 $D_h$ ，计算强度为10，某个实现使得带宽和计算都已经跑满了（每秒读取10GB，计算10GFLOPs，受限于硬件算力和带宽）</p></li><li><p>$x_7$ 和 $x_1$一样，硬件利用率太低了，需要改进实现，则能成长为 $x_6$ 一样的潜力型选手。例如计算强度高的模型 $D_h$，计算强度为10，但某个实现使得计算没有打满（例如带宽没有打满，10GB数据花了2s，即便计算打满也导致GFLOPS为5GFLOPS，或者计算资源没有吃满，例如10GB数据用了1s传输，2s计算&mdash;重复计算、bankconflict之类的导致）</p></li></ul><p>总结：</p><ol><li><p>Roofline model中，<strong>越靠近屋檐（红色）或者屋顶（蓝色）的线，说明已经将硬件资源吃得很满</strong>，除非修改模型改变计算强度，或者修改硬件，否则很难提升。远离这些线的点，在实现是有提升空间的，离得远，说明实现对硬件的利用率越差。</p></li><li><p>Roofline model中，在 $I_{max}$ 左边的点，都是<strong>内存带宽限制性</strong>；在 $I_{max}$ 右边的点，属于<strong>计算限制型</strong>。如果模型在 $I_{max}$ 的左边且靠近屋檐（红色）线，那么不改模型算法的情况下提升带宽资源能增加性能，如果在 $I_{max}$右边且靠近屋顶（蓝色）的线，那么不改模型算法的情况下提升计算资源能增加性能</p></li><li><p><strong>不可能有点会处于超过屋檐（红色）或者屋顶（蓝色）的区域</strong></p></li></ol><h2 id=bank-conflict-与-避免方法 class=heading-element><span>Bank Conflict 与 避免方法</span>
<a href=#bank-conflict-%e4%b8%8e-%e9%81%bf%e5%85%8d%e6%96%b9%e6%b3%95 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p><a href="https://www.bilibili.com/video/BV1Xz4y157p7/?vd_source=d436f8a78656c9132eae4a84f939d219" target=_blank rel="external nofollow noopener noreferrer">参考视频</a></p></blockquote><h3 id=bank概念 class=heading-element><span>Bank概念</span>
<a href=#bank%e6%a6%82%e5%bf%b5 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>cuda内核在执行的时候往往是以warp为单位去调度和执行，一个warp 32 个线程。所以，为了能够高效访存，shared memory中也对应了分成了32个存储体，这个存储体称之为 <em>bank</em> 。</p><p>bank 是CUDA中一个重要概念，是内存的访问时一种划分方式，<mark class=mark-default>在GPU中，访问某个地址的内存时，为了减少读写次数，访问地址并不是随机的，而是一次性访问bank内的内存地址，类似于内存对齐一样</mark>。一般GPU认为如果一个程序要访问某个内存地址时，其附近的数据也有很大概率会在接下来会被访问到。</p><p><strong>（目前N卡都是32个bank）</strong>，分别对应 warp 中32个线程。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4mzx4rjhtj31ai0hogon.jpg alt=image></p><p>bank的宽度，代表的是一个bank所存储的数据的大小宽度。可以是：</p><ul><li>4 字节（32bit，单精度浮点数 float32）</li><li>8 字节（64bit，双精度浮点数 float64）</li></ul><p>每 31 个bank，就会进行一次 stride。</p><p>比如说 bank 的宽度是4字节，我们在 share mem 中申请了 <code>float A[256]</code> 大小的空间，那么：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>A[0], A[1], ..., A[31]   分别在 bank0, bank1, ..., bank31 中
</span></span><span class=line><span class=cl>A[32], A[33], ..., A[63] 也分别在 bank0, bank1, ..., bank31 中
</span></span><span class=line><span class=cl>所以 A[0] 和 A[32] 是共享一个bank的</span></span></code></pre></td></tr></table></div></div><h3 id=bank-conflict class=heading-element><span>Bank Conflict</span>
<a href=#bank-conflict class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>一个很理想的情况是，32个thread，分别访问share mem中32个不同的bank，没有 <strong>bank conflict</strong>，一个 memory 周期完成所有的 memory read/write（行优先访问）</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n0km2yugj31bs0lmtc4.jpg alt=image></p><p>那最不理想的情况就是，32个thread，访问 share mem 中的同一个bank，导致最严重的 bank conflict，需要32个memory 周期才能完成所有的 memory read/write（列优先访问）。如果在block内多个线程访问的地址落入到同一个bank内，那么就会访问同一个bank就会产生bank conflict，这些访问将是变成串行。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n0kps5bsj31bu0hc41t.jpg alt=image></p><h3 id=使用-padding-缓解-bank-conflict class=heading-element><span>使用 Padding 缓解 Bank Conflict</span>
<a href=#%e4%bd%bf%e7%94%a8-padding-%e7%bc%93%e8%a7%a3-bank-conflict class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>为了方便解释，这里使用了8个bank一次stride进行举例。（实际CUDA设计中依然是32个bank一次stride）</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n0kufl2zj31z00tuq89.jpg alt=image></p><p>可以在申请share mem时多申请一列（padding），这样就会更改share mem的布局，使得32个线程访问变成串行。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n0kz7u9jj31ok0u0jxc.jpg alt=image></p><div class="details admonition note open disabled"><div class="details-summary admonition-title"><i class="icon fa-fw fa-solid fa-pencil-alt" aria-hidden=true></i>但会产生未对齐的地址和引入无效数据，导致内存占用增加和干扰需要对齐地址的快速指令。</div><div class=details-content><div class=admonition-content></div></div></div><h3 id=使用-swizzle-缓解-bank-conflict class=heading-element><span>使用 Swizzle 缓解 Bank Conflict</span>
<a href=#%e4%bd%bf%e7%94%a8-swizzle-%e7%bc%93%e8%a7%a3-bank-conflict class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>参考文献：<a href=https://www.zhihu.com/question/667972067 target=_blank rel="external nofollow noopener noreferrer">https://www.zhihu.com/question/667972067</a></p></blockquote><p>Swizzle通过重新映射内存地址，可以将原本访问相同Bank的线程分散到不同的Bank。Swizzle的核心思想是通过<strong>物理地址映射</strong>避免Bank Conflict，同时保持逻辑地址不变。在CUTLASS中，Swizzle的实现通过一系列变换对内存布局进行重排。通过行列坐标的异或操作，Swizzle确保每个线程访问不同的Bank，从而实现了Bank Conflict Free的内存访问。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>half</span><span class=o>*</span> <span class=nf>naive_layout</span><span class=p>(</span><span class=n>half</span><span class=o>*</span> <span class=n>data</span><span class=p>,</span> <span class=kt>int</span> <span class=n>r</span><span class=p>,</span> <span class=kt>int</span> <span class=n>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>&amp;</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=n>r</span> <span class=o>*</span> <span class=n>columns</span> <span class=o>+</span> <span class=n>c</span><span class=p>]);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>half</span><span class=o>*</span> <span class=nf>padding_layout</span><span class=p>(</span><span class=n>half</span><span class=o>*</span> <span class=n>data</span><span class=p>,</span> <span class=kt>int</span> <span class=n>r</span><span class=p>,</span> <span class=kt>int</span> <span class=n>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=o>&amp;</span><span class=p>(</span><span class=n>data</span><span class=p>[</span><span class=n>r</span> <span class=o>*</span> <span class=p>(</span><span class=n>columns</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>+</span> <span class=n>c</span><span class=p>]);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// 一个可能的实现 Swizzle 的方式：
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>half</span><span class=o>*</span> <span class=nf>row_swizzled_layout</span><span class=p>(</span><span class=n>half</span><span class=o>*</span> <span class=n>data</span><span class=p>,</span> <span class=kt>int</span> <span class=n>r</span><span class=p>,</span> <span class=kt>int</span> <span class=n>c</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=n>address</span> <span class=o>=</span> <span class=p>(</span><span class=kt>uint64_t</span><span class=p>)</span><span class=o>&amp;</span><span class=n>data</span><span class=p>[</span><span class=n>r</span> <span class=o>*</span> <span class=n>columns</span> <span class=o>+</span> <span class=n>c</span><span class=p>];</span>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=n>half</span> <span class=o>*&gt;</span><span class=p>(</span><span class=n>address</span> <span class=o>^</span> <span class=p>(</span><span class=n>r</span> <span class=o>&lt;&lt;</span> <span class=mi>2</span><span class=p>));</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>可以用padding和swizzle的示意图来解释下这种物理地址映射的概念：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n1ktpuitj316o0li1ky.jpg alt=image>
⬆️bank conflict free
<img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n1grv22wj316o0li1ky.jpg alt=image>
⬆️bank conflict solved by padding
<img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n1guc2g0j316o0lg1ky.jpg alt=image>
⬆️bank conflict solved by swizzle</p><div class="details admonition note open disabled"><div class="details-summary admonition-title"><i class="icon fa-fw fa-solid fa-pencil-alt" aria-hidden=true></i>如果warp中的线程经过最多两次冲突就能得到所要的数据则成为2-way bank conflict，如果同一个warp中的所有线程访问一个bank中的32个不同的地址，则需要分32此，则称为32-way bank conflict。</div><div class=details-content><div class=admonition-content></div></div></div><div class="details admonition tip open disabled"><div class="details-summary admonition-title"><i class="icon fa-fw fa-regular fa-lightbulb" aria-hidden=true></i>bank conflict只发生在同一个warp的线程之间</div><div class=details-content><div class=admonition-content></div></div></div><blockquote><p>参考文献：</p></blockquote><ol><li><a href=https://zhuanlan.zhihu.com/p/4746910252 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/4746910252</a></li></ol><h2 id=cuda-stream--cuda-graph class=heading-element><span>CUDA Stream & CUDA Graph</span>
<a href=#cuda-stream--cuda-graph class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>参考文献：<a href=https://zhuanlan.zhihu.com/p/699754357 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/699754357</a></p></blockquote><h3 id=cuda-stream class=heading-element><span>CUDA Stream</span>
<a href=#cuda-stream class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>cuda编程里最重要的特性就是异步：CPU提交任务到GPU中异步执行。为了控制异步之间的并发顺序，cuda引入了stream和event的概念。本文尝试分析理解stream和event的含义，充分理解并使用stream、event，是正确、高效利用GPU的必要条件。</p><p><strong>只有一个CPU thread的情况</strong></p><p>当不考虑GPU的时候，CPU线程就在不断地执行指令。从时间维度上看，就是这样的一条线：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n263lmc5j30u603e74r.jpg alt=image></p><p><strong>一个CPU thread与一个GPU执行单元</strong></p><p>GPU相当于CPU的附属硬件，当我们增加了一个GPU执行单元的时候，由CPU下发任务给GPU，于是从时间维度上看，就出现了两条线：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n267qgnaj30u6090wge.jpg alt=image></p><p>这里在GPU和CPU之间还增加了一个GPU队列。当我们在CPU上调用launch kernel的时候，本质上就是把这个kernel放入到GPU的执行队列里。然后，驱动负责维护这个队列，每当执行引擎（硬件资源）空闲的时候，就执行队列里的kernel。</p><p>上图描述了这些关键时间点：</p><ul><li>CPU launch kernel 1，kernel 1入队，此时GPU空闲，于是kernel 1马上开始执行</li><li>CPU调用host function，与GPU无关</li><li>CPU launch kernel 2，kernel 2入队，此时GPU还在执行kernel 1，于是kernel 2继续待在队列里</li><li>GPU执行完kernel 1后，驱动程序发现队列里还有kernel 2，于是开始执行kernel 2（这一步不需要CPU参与）</li></ul><p>早期的GPU硬件上只有一个execution engine，因此，不论是哪个进程、哪个线程发起的kernel launch，都在同一个队列里排队。随着GPU的发展，GPU上面开始出现了多个execution engine。</p><p><strong>一个CPU thread与2个GPU执行单元</strong></p><p>当我们有两个或多个GPU执行单元的时候，我们就可以让GPU kernel之间也并行起来：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n28c1ynrj30uo0fs0v2.jpg alt=image></p><p>图中蓝色阴影区域就是两个GPU kernel并发执行的时间段。</p><p>这里要注意，kernel 1和kernel 2能不能并发执行，需要由用户来决定，硬件不能擅作主张，否则万一kernel 2要读取kernel 1算出来的数据，那并发执行的结果就是错的。</p><p>为了给用户提供这种控制权，于是我们就有了stream的概念。一个stream就对应于一个执行队列（加一个执行单元），用户可以自行决定是否把两个kernel分开放在两个队列里。</p><blockquote><p>stream是cuda为上层应用提供的抽象，应用可以创建任意多个stream，下发任意多个kernel。但如果正在执行的kernel数目超过了硬件的execution engine的数量，那么即使当前stream里没有kernel正在执行，下发的kernel也必须在队列里继续等待，直到有硬件资源可以执行。（注意：此时kernel 执行与CPU下发kernel之间依然是异步的）</p></blockquote><p>由此，我们可以总结得到：一个GPU kernel可以执行的必要条件是它在stream的队列首位且存在执行kernel的空闲硬件资源。</p><p>创建stream的时候，我们也可以用 <code>cudaStreamCreateWithPriority</code> 为它指定优先级。当多个GPU kernel都可以执行的时候，cuda driver负责调度，优先执行high priority的stream里的kernel。</p><h4 id=stream之间的操作cuda-event class=heading-element><span>stream之间的操作：cuda event</span>
<a href=#stream%e4%b9%8b%e9%97%b4%e7%9a%84%e6%93%8d%e4%bd%9ccuda-event class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>我们不仅可以从CPU thread操作stream，也可以在一个stream上面操作另一个stream。当然，stream只是一层抽象，我们依然要借用CPU thread的辅助，来指导“一个stream上面操作另一个stream”。具体的操作也很简单，就是一个stream上面的kernel想等待另一个stream上面的kernel执行结束之后再执行。</p><p>为了实现这个目标，我们需要记录另一个stream上面的队列状态，这就是cuda event。让我们来看一个具体的例子：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n2a78z2hj31400ehtcv.jpg alt=image></p><p>它实现的功能是：</p><ul><li>在stream 1上面执行kernel 1 和 kernel 3</li><li>在stream 2上面执行kernel 2，但是必须等到stream 1上面的kernel 1执行结束之后才能开始</li></ul><p>为此，我们在stream 1上面launch kernel 1之后，创建一个event来记录当前stream 1的状态（具体来说就是队列里有哪些kernel还没执行），然后把这个event放进stream 2的队列里，让stream 2执行一个想象中的“wait for event 1”的kernel，这个kernel直到kernel 1执行结束之后才离开队列。于是，在这个event之后加入队列的kernel 2，就必须等到kernel 1执行结束之后才能开始了。</p><p>这样的好处在于，我们不仅控制了kernel 1和 kernel 2的执行顺序，而且把kernel 2和kernel 3并发执行了，节省了时间（蓝色区域为两个kernel同时执行的时间段）。</p><h3 id=cuda-graph class=heading-element><span>CUDA Graph</span>
<a href=#cuda-graph class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>对GPU进行性能优化时，cudagraph是绕不开的话题。不仅是GPU，大部分的xpu都会提供类似graph mode的优化，相比于每次分别由CPU进行kernel launch的eager mode，graph mode通常都会有较大性能提升，然而也经常容易出现各种各样的奇怪问题。</p><div class="details admonition note open disabled"><div class="details-summary admonition-title"><i class="icon fa-fw fa-solid fa-pencil-alt" aria-hidden=true></i>由于实习期间对CUDA Graph接触的比较多，实际上 CUDA Graph主要是为了减少 kernel launch 造成的开销，于是乎将多个琐碎的小kernel改为一次launch，具体的操作方式分为：一次Capture->多次Replay</div><div class=details-content><div class=admonition-content></div></div></div><p><strong>graph capture期间禁止执行的函数</strong></p><p>当一个stream处在graph capture状态时，实际上CPU下发的kernel都没有执行，只是被记录下来了。因此，与GPU kernel执行状态相关的函数都不能使用，例如：</p><ul><li>对stream进行同步（cudaStreamSynchronize）</li><li>隐含stream同步的操作（对context进行同步、对device进行同步，都会强制内部包含的全部stream进行同步，如果其中有stream处在graph capture状态，就会报错）</li><li>隐含stream同步的操作,如当前graph capture的stream是blocking stream，则涉及null stream的操作都不可用，例如cudaMalloc</li><li>对stream上面record的event进行的状态查询、同步操作</li><li><mark class=mark-default>另外断点调试在这期间也是用不了的</mark></li></ul><p>更详细的限制可以看官方文档：<a href=https://docs.pytorch.org/docs/stable/notes/cuda.html target=_blank rel="external nofollow noopener noreferrer">https://docs.pytorch.org/docs/stable/notes/cuda.html</a></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span><span class=lnt>64
</span><span class=lnt>65
</span><span class=lnt>66
</span><span class=lnt>67
</span><span class=lnt>68
</span><span class=lnt>69
</span><span class=lnt>70
</span><span class=lnt>71
</span><span class=lnt>72
</span><span class=lnt>73
</span><span class=lnt>74
</span><span class=lnt>75
</span><span class=lnt>76
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>contextlib</span> <span class=kn>import</span> <span class=n>contextmanager</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@contextmanager</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>graph_capture</span><span class=p>(</span><span class=n>pool</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>stream</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>capture_error_mode</span><span class=p>:</span> <span class=nb>str</span> <span class=o>=</span> <span class=s2>&#34;global&#34;</span><span class=p>,</span> <span class=n>dump_path</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>CUDAGraph</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dump_path</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>g</span><span class=o>.</span><span class=n>enable_debug_mode</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>graph</span><span class=p>(</span><span class=n>cuda_graph</span><span class=o>=</span><span class=n>g</span><span class=p>,</span> <span class=n>pool</span><span class=o>=</span><span class=n>pool</span><span class=p>,</span> <span class=n>stream</span><span class=o>=</span><span class=n>stream</span><span class=p>,</span> <span class=n>capture_error_mode</span><span class=o>=</span><span class=n>capture_error_mode</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>yield</span> <span class=n>g</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>dump_path</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>g</span><span class=o>.</span><span class=n>debug_dump</span><span class=p>(</span><span class=n>dump_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>ctypes</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Load the CUDA runtime library</span>
</span></span><span class=line><span class=cl><span class=n>cudart</span> <span class=o>=</span> <span class=n>ctypes</span><span class=o>.</span><span class=n>CDLL</span><span class=p>(</span><span class=s1>&#39;libcudart.so&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define cudaMemcpyKind enumeration as in the CUDA API</span>
</span></span><span class=line><span class=cl><span class=n>cudaMemcpyHostToHost</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>cudaMemcpyHostToDevice</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>cudaMemcpyDeviceToHost</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>cudaMemcpyDeviceToDevice</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=n>cudaMemcpyDefault</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Setup the prototype of the cudaMemcpyAsync function</span>
</span></span><span class=line><span class=cl><span class=n>cudaMemcpyAsync</span> <span class=o>=</span> <span class=n>cudart</span><span class=o>.</span><span class=n>cudaMemcpyAsync</span>
</span></span><span class=line><span class=cl><span class=n>cudaMemcpyAsync</span><span class=o>.</span><span class=n>argtypes</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>ctypes</span><span class=o>.</span><span class=n>c_void_p</span><span class=p>,</span>          <span class=c1># void* dst</span>
</span></span><span class=line><span class=cl>    <span class=n>ctypes</span><span class=o>.</span><span class=n>c_void_p</span><span class=p>,</span>          <span class=c1># const void* src</span>
</span></span><span class=line><span class=cl>    <span class=n>ctypes</span><span class=o>.</span><span class=n>c_size_t</span><span class=p>,</span>          <span class=c1># size_t count</span>
</span></span><span class=line><span class=cl>    <span class=n>ctypes</span><span class=o>.</span><span class=n>c_int</span><span class=p>,</span>             <span class=c1># enum cudaMemcpyKind</span>
</span></span><span class=line><span class=cl>    <span class=n>ctypes</span><span class=o>.</span><span class=n>c_void_p</span>           <span class=c1># cudaStream_t stream</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>cudaMemcpyAsync</span><span class=o>.</span><span class=n>restype</span> <span class=o>=</span> <span class=n>ctypes</span><span class=o>.</span><span class=n>c_int</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Placeholder input used for capture</span>
</span></span><span class=line><span class=cl><span class=n>static_a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>5</span><span class=p>,),</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>static_a</span> <span class=o>=</span> <span class=n>static_a</span><span class=o>.</span><span class=n>pin_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>static_b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>5</span><span class=p>,),</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>static_b</span> <span class=o>=</span> <span class=n>static_b</span><span class=o>.</span><span class=n>pin_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>static_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>5</span><span class=p>,),</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cpu&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>static_output</span> <span class=o>=</span> <span class=n>static_output</span><span class=o>.</span><span class=n>pin_memory</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>compute</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span> <span class=o>=</span> <span class=n>static_a</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=n>non_blocking</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>b</span> <span class=o>=</span> <span class=n>static_b</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=n>non_blocking</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=p>(</span><span class=n>a</span> <span class=o>+</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span> <span class=o>=</span> <span class=n>cudaMemcpyAsync</span><span class=p>(</span><span class=n>static_output</span><span class=o>.</span><span class=n>data_ptr</span><span class=p>(),</span> <span class=n>output</span><span class=o>.</span><span class=n>data_ptr</span><span class=p>(),</span> <span class=n>output</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=o>*</span> <span class=n>output</span><span class=o>.</span><span class=n>element_size</span><span class=p>(),</span> <span class=n>cudaMemcpyDeviceToHost</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_stream</span><span class=p>()</span><span class=o>.</span><span class=n>cuda_stream</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>assert</span> <span class=n>result</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>static_output</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Warmup before capture</span>
</span></span><span class=line><span class=cl><span class=n>s</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>Stream</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>s</span><span class=o>.</span><span class=n>wait_stream</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_stream</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>stream</span><span class=p>(</span><span class=n>s</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>compute</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_stream</span><span class=p>()</span><span class=o>.</span><span class=n>wait_stream</span><span class=p>(</span><span class=n>s</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Captures the graph</span>
</span></span><span class=line><span class=cl><span class=c1># To allow capture, automatically sets a side stream as the current stream in the context</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>nvtx</span><span class=o>.</span><span class=n>range</span><span class=p>(</span><span class=s2>&#34;capture&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>graph_capture</span><span class=p>(</span><span class=n>dump_path</span><span class=o>=</span><span class=s2>&#34;graph.dot&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>g</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>compute</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run the graph</span>
</span></span><span class=line><span class=cl><span class=n>g</span><span class=o>.</span><span class=n>replay</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_stream</span><span class=p>()</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>static_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>static_a</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>static_b</span> <span class=o>+=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>g</span><span class=o>.</span><span class=n>replay</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_stream</span><span class=p>()</span><span class=o>.</span><span class=n>synchronize</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>static_output</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div><p>其中，因为PyTorch的功能的一些限制，我们的代码发生了变化：PyTorch里的 <code>a.to("cpu")</code>，会强制同步使得host to device完成同步，即使加上 <code>non_blocking=True</code> 也无法改变。这与cudagraph不兼容。为了解决这个问题，我们手动调用了 <code>cudaMemcpyAsync</code> 函数，实现了异步拷贝到CPU的功能。（如果要PyTorch里直接实现这一功能，需要async CPU的支持。）</p><p>执行以上代码，捕获的计算图为：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n2qraosuj31401r1gyi.jpg alt=image></p><h2 id=ptx--sass--libdevice class=heading-element><span>PTX & SASS & libdevice</span>
<a href=#ptx--sass--libdevice class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>参考文献：<a href=https://zhuanlan.zhihu.com/p/21358921473 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/21358921473</a></p></blockquote><h3 id=ptxparallel-thread-execution class=heading-element><span>PTX(Parallel Thread Execution)</span>
<a href=#ptxparallel-thread-execution class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ul><li>PTX 是NVIDIA设计的中间表示（IR），类似于GPU的汇编语言，但独立于具体硬件架构。</li><li>PTX代码由NVCC生成，可以被NVIDIA的驱动程序进一步编译为特定GPU架构的机器代码（SASS）。</li><li>PTX代码是文本格式，便于阅读和调试。</li></ul><p>示例：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n2w08wp2j30wf0j83zl.jpg alt=image></p><h3 id=sass-streaming-assembly class=heading-element><span>SASS (Streaming ASSembly)</span>
<a href=#sass-streaming-assembly class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ul><li>SASS 是NVIDIA GPU的机器代码，直接由GPU硬件执行。</li><li>SASS是二进制格式，特定于具体的GPU架构（如Ampere、Turing）。</li><li>SASS代码通常通过反汇编工具（如cuobjdump）查看。</li></ul><p>示例：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n2wezp5aj30w006fweo.jpg alt=image></p><h3 id=libdevice class=heading-element><span>libdevice</span>
<a href=#libdevice class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><ul><li>libdevice 是NVIDIA提供的一个数学函数库，包含高度优化的设备端数学函数（如sin、exp、log等）</li><li>这些函数以PTX或SASS形式提供，可以直接链接到CUDA程序中。</li><li>libdevice库的文件通常命名为libdevice.*.bc（LLVM bitcode格式）。</li></ul><p>在CUDA程序中，调用标准数学函数（如sin、cos）时，NVCC会自动链接libdevice库。开发者也可以通过 <code>-lcudadevrt</code>显式链接libdevice。</p><h3 id=转换流程 class=heading-element><span>转换流程</span>
<a href=#%e8%bd%ac%e6%8d%a2%e6%b5%81%e7%a8%8b class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>以下是CUDA代码从高级语言到最终机器代码的转换流程：</p><ol><li>CUDA C/C++ → PTX</li></ol><ul><li>NVCC将CUDA设备代码编译为PTX。</li><li>示例命令：</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>nvcc -ptx vector_add.cu -o vector_add.ptx</span></span></code></pre></td></tr></table></div></div><ol start=2><li>PTX → SASS</li></ol><ul><li>NVIDIA的驱动程序将PTX代码编译为特定GPU架构的SASS代码。</li><li>这一步在运行时或安装时完成。</li></ul><h2 id=cutlass--cute class=heading-element><span>CUTLASS & CUTE</span>
<a href=#cutlass--cute class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><h3 id=cutlass class=heading-element><span>CUTLASS</span>
<a href=#cutlass class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>CUTLASS（CUDA Template for Linear Algebra Subroutines）是指NVIDIA开源的一个高性能CUDA模板库，用于实现高效的线性代数计算，里面提供了包括访存，计算，流水线编排等多个level的模板。用户通过CUTLASS模板组装以及特化，可以搭建出自己需要的高性能CUDA Kernel。</p><p>核心思想是模块化和通用化。它不像传统的库（如 cuBLAS）那样提供预编译好的函数，而是提供了一套模板，让你能够根据自己的需求组合出最优化的计算内核。这使得开发者可以针对特定的矩阵尺寸、数据类型（如 FP16、FP32、INT8 等）和计算模式，生成定制化的、性能极高的内核，而无需从头手写汇编代码。</p><h3 id=cute class=heading-element><span>CUTE</span>
<a href=#cute class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>CUTE 是一个更底层的、用于描述和操作 GPU 上多维数据布局 (tensor layout) 的 C++ 库。 它不是一个完整的应用库，而是一套强大的、用于构建其他高级库（如 CUTLASS）的工具集。</p><p>CUTE 的核心思想是提供一套数学化的、声明式的张量抽象，让开发者可以精确地描述数据在内存中的排布方式。它解决了以下核心问题：</p><p>张量布局 (Tensor Layout)： 如何描述一个多维数组（张量）在内存中的存储方式，例如行主序、列主序，或者更复杂的分块和交错布局。</p><p>张量视图 (Tensor View)： 如何在不移动数据的情况下，创建张量的子视图，并对子视图进行操作。</p><p>跨步操作 (Striding)： 如何高效地计算多维数组中元素的地址。</p><p>你可以把 CUTE 理解为一个“张量布局的 DSL (Domain-Specific Language)”。 它提供了一种简洁的方式来表达复杂的数据访问模式，这对于编写 GPU 上的高性能通用代码至关重要。CUTLASS 内部就广泛使用了 CUTE 来管理和操作其张量数据，从而实现了其灵活性和性能。</p><h2 id=global-memory的访存合并 class=heading-element><span>Global Memory的访存合并</span>
<a href=#global-memory%e7%9a%84%e8%ae%bf%e5%ad%98%e5%90%88%e5%b9%b6 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><blockquote><p>参考文献：</p><ul><li><a href=https://zhuanlan.zhihu.com/p/675186810 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/675186810</a></li><li><a href=https://zhuanlan.zhihu.com/p/641639133 target=_blank rel="external nofollow noopener noreferrer">https://zhuanlan.zhihu.com/p/641639133</a></li></ul></blockquote><p>当前的GPU架构允许通过编译选项来控制是否启用一级缓存。当一级缓存被禁用时，对全局内存的加载请求将直接进入二级缓存；如果二级缓存未命中，将由DRAM完成请求。核函数从全局内存DRAM中读取数据有两种粒度， 使用一级缓存时，每次按照128字节进行缓存；不使用一级缓存时，每次按照32字节进行缓存。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=c1># 禁用一级缓存</span>
</span></span><span class=line><span class=cl>-Xptxas -dlcm<span class=o>=</span>cg
</span></span><span class=line><span class=cl><span class=c1># 启用一级缓存</span>
</span></span><span class=line><span class=cl>-Xptxas -dlcm<span class=o>=</span>ca</span></span></code></pre></td></tr></table></div></div><h3 id=传输延迟 class=heading-element><span>传输延迟</span>
<a href=#%e4%bc%a0%e8%be%93%e5%bb%b6%e8%bf%9f class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>在host端和device端之间存在latency，数据通过PCI-E总线从CPU传输给GPU，我们必须避免频繁的host、device间数据传输，即使是最新的PCIE 3.0 x16接口，其双向带宽也只有32GB/s</p><p>在device内部也存在latency，即数据从gpu的存储器到multi-processor（SM）的传输。</p><p>访问一次全局内存，将耗费400~600个cycle，成本是非常高的，所以必须谨慎对待全局内存的访问</p><p>我们把一次内存请求——也就是从内核函数发起请求，到硬件响应返回数据这个过程称为一个内存事务（加载和存储都行）。</p><h3 id=合并访存 class=heading-element><span>合并访存</span>
<a href=#%e5%90%88%e5%b9%b6%e8%ae%bf%e5%ad%98 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>数据从全局内存到SM（stream-multiprocessor）的传输，会进行cache，如果cache命中了，下一次的访问的耗时将大大减少。</p><blockquote><p>基本逻辑是： 首先判断这个 Kernel 的数据流路径，是否使用了 L1 cache，由此得出当前内存访问的最小粒度： 32 Bytes / 128 Bytes. 分析原始数据存储的结构，结合访存粒度，分析数据访问是否内存对齐，数据是否能合并访问。</p></blockquote><p>对于L1 cache，每次按照128字节进行缓存；对于L2 cache，每次按照32字节进行缓存。</p><p>意思是表示线程束中每个线程以一个字节（<code>1*32=32</code>）、16位（<code>2*32=64</code>）、32位（<code>4*32=128</code>）为单位读取数据。前提是，访问必须连续，并且访问的地址是以32字节对齐。</p><p>例子，假设每个thread读取一个float变量，那么一个warp（32个thread）将会执行32*4=128字节的合并访存指令，通过一次访存操作完成所有thread的读取请求。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n6sp6qf1j30ri081t96.jpg alt=image></p><p>对于L2 cache，合并访存的字节减少为32字节，那么L2 cache相对L1 cache的好处就是：</p><p><mark class=mark-default>在非对齐访问、分散访问（非连续访问）的情况下，提高吞吐量（cache的带宽利用率）</mark></p><p><strong>对齐/非对齐访问</strong></p><p>当一个内存事务的首个访问地址是缓存粒度（32或128字节）的偶数倍的时候：比如二级缓存32字节的偶数倍64，128字节的偶数倍256的时候，这个时候被称为对齐内存访问，非对齐访问就是除上述的其他情况，非对齐的内存访问会造成带宽浪费。</p><p><strong>合并/非合并访问</strong></p><p>当一个线程束内的线程访问的内存都在一个内存块（缓存粒度）里的时候，就会出现合并访问。</p><p>对齐合并访问的状态是理想化的，也是最高速的访问方式，当线程束内的所有线程访问的数据在一个内存块，并且数据是从内存块的首地址开始被需要的，那么对齐合并访问出现了。为了最大化全局内存访问的理想状态，尽量将线程束访问内存组织成对齐合并的方式，这样的效率是最高的。</p><p>一个线程束加载数据，使用一级缓存，并且这个事务所请求的所有数据在一个128字节的对齐的地址段上（对齐的地址段是我自己发明的名字，就是首地址是粒度的偶数倍，那么上面这句话的意思是，所有请求的数据在某个首地址是粒度偶数倍的后128个字节里），具体形式如下图，这里请求的数据是连续的，其实可以不连续，但是不要越界就好。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n5sr9oc7j30mm069glx.jpg alt=image></p><p>如果一个事务加载的数据分布在不一个对齐的地址段上，就会有以下两种情况：</p><ol><li>连续的，但是不在一个对齐的段上，比如，请求访问的数据分布在内存地址 1-128，那么0-127和128-255这两段数据要传递两次到SM</li><li>不连续的，也不在一个对齐的段上，比如，请求访问的数据分布在内存地址0-63和128-191上，明显这也需要两次加载。</li></ol><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i4n5t0v2sij30x209874z.jpg alt=image></p><p>数据分散开了，thread0的请求在128之前，后面还有请求在256之后，所以需要三个内存事务，而利用率，也就是从主存取回来的数据被使用到的比例，只有 128/(3*128) 的比例。这个比例低会造成带宽的浪费，最极端的表现，就是如果每个线程的请求都在不同的段，也就是一个128字节的事务只有1个字节是有用的，那么利用率只有 1/128.</p><p><strong>实例——写一个 内存访问非对齐的kernel</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>__global__</span> <span class=kt>void</span> <span class=nf>sumArraysGPU</span><span class=p>(</span><span class=kt>float</span> <span class=o>*</span><span class=n>A</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>B</span><span class=p>,</span> <span class=kt>float</span> <span class=o>*</span><span class=n>C</span><span class=p>,</span> <span class=kt>int</span> <span class=n>offset</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>tid</span> <span class=o>=</span> <span class=n>blockIdx</span><span class=p>.</span><span class=n>x</span> <span class=o>*</span> <span class=n>blockDim</span><span class=p>.</span><span class=n>x</span> <span class=o>+</span> <span class=n>threadIdx</span><span class=p>.</span><span class=n>x</span><span class=p>;</span>  
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>index</span> <span class=o>=</span> <span class=n>tid</span> <span class=o>+</span> <span class=n>offset</span><span class=p>;</span>  <span class=c1>//添加偏移量 从 gld memory 读取数据带地址偏移 这样可以控制内存是否对齐
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>if</span><span class=p>(</span><span class=n>index</span> <span class=o>&lt;</span> <span class=n>N</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>C</span><span class=p>[</span><span class=n>tid</span><span class=p>]</span> <span class=o>=</span> <span class=n>A</span><span class=p>[</span><span class=n>index</span><span class=p>]</span> <span class=o>+</span> <span class=n>B</span><span class=p>[</span><span class=n>index</span><span class=p>];</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span></span></span></code></pre></td></tr></table></div></div><p>这里用一个 sum kernel 进行举例，<code>float*A, float*, float*C</code> 都是从 glb mem 读取数据，每个 thread 会去load 一个数据进行处理。通过 offset 可以改变 thead load 数据的偏移地址，从而来试验不同 cache 粒度的 合并访问现象。</p><p>可以发现禁用 L1 cache 前后，另外 offset = 0 和 offset = 32 得到上述理论结果。</p><div class="details admonition tip open disabled"><div class="details-summary admonition-title"><i class="icon fa-fw fa-regular fa-lightbulb" aria-hidden=true></i>另外，面临非对齐访问，使用 L2 cache能够相对获得较高的 cache 利用率，这也很好理解，因为 L2 cache 的内存块大小更小，即使非对齐，有效地址偏移位数少</div><div class=details-content><div class=admonition-content></div></div></div><h2 id=常用的profile工具和方法 class=heading-element><span>常用的profile工具和方法</span>
<a href=#%e5%b8%b8%e7%94%a8%e7%9a%84profile%e5%b7%a5%e5%85%b7%e5%92%8c%e6%96%b9%e6%b3%95 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h2><ol><li>Nsight System</li><li>Nsight Compute</li><li>compute-sanitizer</li><li><a href=https://blog.vllm.ai/2025/08/11/cuda-debugging.html target=_blank rel="external nofollow noopener noreferrer">vllm core dump</a></li><li>perfetto 分析 torch profile 文件</li></ol></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2025-08-22 01:02:38">更新于 2025-08-22&nbsp;</span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/2025/34980c2/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Pocket" data-sharer=pocket data-url=https://yitaonote.com/2025/34980c2/><i class="fa-brands fa-get-pocket fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://yitaonote.com/2025/34980c2/ data-title=GPU及CUDA基本概念><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/2025/adc4087/ class=post-nav-item rel=prev title=秋招timeline><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>秋招timeline</a></div></div><div class=post-reward><div class=comment></div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward><i class="fa-solid fa-qrcode fa-fw" aria-hidden=true></i>赞赏</label><div class=reward-ways data-mode=static><div><img src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i429ilg93xj30u018zjut.jpg alt="yitao 支付宝"><span>支付宝</span></div><div><img src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i429iozq5hj30n00v8gne.jpg alt="yitao 微信"><span>微信</span></div></div></div><div id=comments><div id=giscus class=comment><script src=https://giscus.app/client.js data-repo=walker-ai/walker-ai.github.io data-repo-id=R_kgDOOM9c8w data-category=Announcements data-category-id=DIC_kwDOOM9c884CoX_P data-mapping=pathname data-strict=0 data-theme=preferred_color_scheme data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-lang=zh-CN data-loading=lazy crossorigin=anonymous async defer></script></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app/ rel="external nofollow noopener noreferrer">giscus</a>.</noscript></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.145.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.17-8212d6fd"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2025</span><span class=author itemprop=copyrightHolder>
<a href=/></a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div><div class="fixed-button view-comments d-none" role=button aria-label=查看评论><i class="fa-solid fa-comment fa-fw" aria-hidden=true></i></div></div><a href=https://github.com/walker-ai/walker-ai.github.io title="View source on GitHub" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true" width="56" height="56"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=https://unpkg.com/katex@0.16.10/dist/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/katex@0.16.10/dist/katex.min.css></noscript><link rel=stylesheet href=https://unpkg.com/pace-js@1.2.4/themes/blue/pace-theme-minimal.css><script src=https://unpkg.com/autocomplete.js@0.38.1/dist/autocomplete.min.js defer></script><script src=https://unpkg.com/fuse.js@6.6.2/dist/fuse.min.js defer></script><script src=https://unpkg.com/twemoji@14.0.2/dist/twemoji.min.js defer></script><script src=https://unpkg.com/sharer.js@0.5.1/sharer.min.js async defer></script><script src=https://unpkg.com/katex@0.16.10/dist/katex.min.js defer></script><script src=https://unpkg.com/katex@0.16.10/dist/contrib/auto-render.min.js defer></script><script src=https://unpkg.com/katex@0.16.10/dist/contrib/mhchem.min.js defer></script><script src=https://unpkg.com/pangu@4.0.7/dist/browser/pangu.min.js defer></script><script src=https://unpkg.com/pace-js@1.2.4/pace.min.js async defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:100},comment:{enable:!0,expired:!1,giscus:{darkTheme:"dark",lightTheme:"light",origin:"https://giscus.app"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/search.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:30,threshold:.3,type:"fuse",useExtendedSearch:!1},twemoji:!0,version:"v0.3.17-8212d6fd"}</script><script src=/js/theme.min.js defer></script></body></html>