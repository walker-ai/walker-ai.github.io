<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=2"><meta name=robots content="noodp"><title>分布式推理优化技术 - yitao's Blog</title><meta name=author content="yitao">
<meta name=description content="yitao的学习笔记"><meta name=keywords content='Blog,Blockchain,Book,Movie,Life,Journey'><meta itemprop=name content="分布式推理优化技术"><meta itemprop=description content="yitao的学习笔记"><meta itemprop=datePublished content="2025-08-19T23:07:32+08:00"><meta itemprop=dateModified content="2025-08-19T23:07:32+08:00"><meta itemprop=wordCount content="5793"><meta itemprop=image content="https://yitaonote.com/logo.png"><meta itemprop=keywords content="推理优化"><meta property="og:url" content="https://yitaonote.com/2025/7ae2a3f/"><meta property="og:site_name" content="yitao's Blog"><meta property="og:title" content="分布式推理优化技术"><meta property="og:description" content="yitao的学习笔记"><meta property="og:locale" content="zh_CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-08-19T23:07:32+08:00"><meta property="article:modified_time" content="2025-08-19T23:07:32+08:00"><meta property="og:image" content="https://yitaonote.com/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://yitaonote.com/logo.png"><meta name=twitter:title content="分布式推理优化技术"><meta name=twitter:description content="yitao的学习笔记"><meta name=application-name content="yitao's Blog"><meta name=apple-mobile-web-app-title content="yitao's Blog"><meta name=theme-color data-light=#f8f8f8 data-dark=#252627 content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical type=text/html href=https://yitaonote.com/2025/7ae2a3f/ title="分布式推理优化技术 - yitao's Blog"><link rel=prev type=text/html href=https://yitaonote.com/2025/acc2381/ title=Kruskal算法求最小生成树><link rel=alternate type=text/markdown href=https://yitaonote.com/2025/7ae2a3f/index.md title="分布式推理优化技术 - yitao's Blog"><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://unpkg.com/@fortawesome/fontawesome-free@6.7.1/css/all.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/@fortawesome/fontawesome-free@6.7.1/css/all.min.css></noscript><link rel=preload href=https://unpkg.com/animate.css@4.1.1/animate.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"分布式推理优化技术","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yitaonote.com\/2025\/7ae2a3f\/"},"genre":"posts","wordcount":5793,"url":"https:\/\/yitaonote.com\/2025\/7ae2a3f\/","datePublished":"2025-08-19T23:07:32+08:00","dateModified":"2025-08-19T23:07:32+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"yitao"},"description":""}</script><script src=/js/head/color-scheme.min.js></script></head><body data-header-desktop=sticky data-header-mobile=auto><div class=wrapper data-page-style=normal><header class="desktop animate__faster" id=header-desktop><div class=header-wrapper data-github-corner=right><div class=header-title><a href=/ title="yitao's Blog"><img class=logo src='https://avatars.githubusercontent.com/u/75578057?s=400&u=523ba40a796b5dc3cdd41fae4a1f038537d2dc67&v=4' alt="yitao's Blog" height=32 width=32><span class=header-title-text>yitao's Blog</span></a><span class=header-subtitle></span></div><nav><ul class=menu><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/collection/><i class="fa-solid fa-bookmark fa-fw fa-sm" aria-hidden=true></i> 集子</a></li><li class=menu-item><a class=menu-link href=/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item delimiter"></li><li class="menu-item search" id=search-desktop><input type=text placeholder=搜索文章标题或内容…… id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></li><li class="menu-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></li></ul></nav></div></header><header class="mobile animate__faster" id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="yitao's Blog"><img class=logo src='https://avatars.githubusercontent.com/u/75578057?s=400&u=523ba40a796b5dc3cdd41fae4a1f038537d2dc67&v=4' alt="yitao's Blog" height=26 width=26><span class=header-title-text>yitao's Blog</span></a><span class=header-subtitle></span></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><nav><ul class=menu id=menu-mobile><li class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容…… id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fa-solid fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fa-solid fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fa-solid fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></li><li class=menu-item><a class=menu-link href=/posts/><i class="fa-solid fa-archive fa-fw fa-sm" aria-hidden=true></i> 文章</a></li><li class=menu-item><a class=menu-link href=/categories/><i class="fa-solid fa-th fa-fw fa-sm" aria-hidden=true></i> 分类</a></li><li class=menu-item><a class=menu-link href=/tags/><i class="fa-solid fa-tags fa-fw fa-sm" aria-hidden=true></i> 标签</a></li><li class=menu-item><a class=menu-link href=/collection/><i class="fa-solid fa-bookmark fa-fw fa-sm" aria-hidden=true></i> 集子</a></li><li class=menu-item><a class=menu-link href=/about/><i class="fa-solid fa-info-circle fa-fw fa-sm" aria-hidden=true></i> 关于</a></li><li class="menu-item menu-system"><span class="menu-system-item theme-switch" title=切换主题><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></span></li></ul></nav></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=container><aside class="aside-collection animate__animated animate__fadeIn animate__faster" aria-label=合集></aside><article class="page single"><div class=header><h1 class="single-title animate__animated animate__flipInX"><span>分布式推理优化技术</span></h1></div><div class=post-meta><div class=post-meta-line><span class=post-author><span class=author><i class="fa-solid fa-user-circle" aria-hidden=true></i>
yitao</span></span><span class=post-included-in>&nbsp;收录于 <a href=/categories/%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96/ class=post-category title="分类 - 推理优化"><i class="fa-regular fa-folder fa-fw" aria-hidden=true></i> 推理优化</a></span></div><div class=post-meta-line><span title="发布于 2025-08-19 23:07:32"><i class="fa-solid fa-calendar-days fa-fw me-1" aria-hidden=true></i><time datetime=2025-08-19>2025-08-19</time></span>&nbsp;<span title="5793 字"><i class="fa-solid fa-pencil-alt fa-fw me-1" aria-hidden=true></i>约 5800 字</span>&nbsp;<span><i class="fa-regular fa-clock fa-fw me-1" aria-hidden=true></i>预计阅读 12 分钟</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept=false><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fa-solid fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#为什么不缓存q而要缓存kv>为什么不缓存Q，而要缓存KV？</a></li><li><a href=#为什么-deepseek-使用-fp8-而-qwen-使用-bf16>为什么 DeepSeek 使用 FP8 而 Qwen 使用 BF16</a></li><li><a href=#为什么-cpu-需要三层-cache-而-gpu-只需要两层>为什么 CPU 需要三层 Cache 而 GPU 只需要两层；</a></li><li><a href=#分布式并行方法>分布式并行方法</a><ul><li><a href=#基础并行策略dptppp>基础并行策略（DP、TP、PP）</a><ul><li><a href=#dp-data-parallelism-dp>DP (Data Parallelism, DP)</a></li><li><a href=#tp-tensor-parallelism-tp>TP (Tensor Parallelism, TP)</a></li><li><a href=#流水线并行-pipeline-parallelism-pp>流水线并行 (Pipeline Parallelism, PP)</a></li></ul></li><li><a href=#长序列并行策略序列上下文并行>长序列并行策略：序列、上下文并行</a><ul><li><a href=#序列并行-sequence-parallelism-sp>序列并行 (Sequence Parallelism, SP)</a></li><li><a href=#上下文并行-context-parallelism-cp>上下文并行 (Context Parallelism, CP)</a></li><li><a href=#专家并行-expert-parallelism-ep>专家并行 (Expert Parallelism, EP)</a></li></ul></li><li><a href=#综合策略与性能基准>综合策略与性能基准</a></li></ul></li></ul></li></ul></nav></div></div><div class=content id=content><h3 id=为什么不缓存q而要缓存kv class=heading-element><span>为什么不缓存Q，而要缓存KV？</span>
<a href=#%e4%b8%ba%e4%bb%80%e4%b9%88%e4%b8%8d%e7%bc%93%e5%ad%98q%e8%80%8c%e8%a6%81%e7%bc%93%e5%ad%98kv class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>参考文献：<a href=https://www.zhihu.com/question/653658936 target=_blank rel="external nofollow noopener noreferrer">https://www.zhihu.com/question/653658936</a></p></blockquote><p>因为分析attention计算可以知道，预测下一个token只用到了当前的query信息，而没有用到之前的query信息，所以不用缓存Q；
在序列的t位置，Q只有当前位置的，参与了计算，而K和V多个位置参与了计算，所以需要KV Cache，而不需要Q Cache。</p><h3 id=为什么-deepseek-使用-fp8-而-qwen-使用-bf16 class=heading-element><span>为什么 DeepSeek 使用 FP8 而 Qwen 使用 BF16</span>
<a href=#%e4%b8%ba%e4%bb%80%e4%b9%88-deepseek-%e4%bd%bf%e7%94%a8-fp8-%e8%80%8c-qwen-%e4%bd%bf%e7%94%a8-bf16 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><h3 id=为什么-cpu-需要三层-cache-而-gpu-只需要两层 class=heading-element><span>为什么 CPU 需要三层 Cache 而 GPU 只需要两层；</span>
<a href=#%e4%b8%ba%e4%bb%80%e4%b9%88-cpu-%e9%9c%80%e8%a6%81%e4%b8%89%e5%b1%82-cache-%e8%80%8c-gpu-%e5%8f%aa%e9%9c%80%e8%a6%81%e4%b8%a4%e5%b1%82 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><p>CPU三级缓存介绍：https://blog.csdn.net/weixin_43719763/article/details/128602160</p><p>CPU三级缓存主要为了用于不同级别的数据访问，L1/L2是私有的，而L3是共享的，这样可以减少核心之间的数据竞争，降低数据访问延迟。而GPU由于线程数量较多，每个线程对缓存的压力较小，无需为单个线程进行优化缓存层级。而且GPU数据重用集中在相邻线程（如卷积核滑动窗口），L1/L2 缓存+共享内存即可满足。</p><h3 id=分布式并行方法 class=heading-element><span>分布式并行方法</span>
<a href=#%e5%88%86%e5%b8%83%e5%bc%8f%e5%b9%b6%e8%a1%8c%e6%96%b9%e6%b3%95 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h3><blockquote><p>数据并行（DP）、张量并行（TP）、流水线并行（PP）、上下文并行（CP）、专家并行（EP）以及序列并行（SP）。</p></blockquote><p><strong>为什么分布式并行很重要？</strong></p><p>一个70B参数的FP16模型，仅模型权重就需要约140GB的存储空间，远超任何单张主流GPU的显存容量。此外，训练过程中产生的优化器状态、梯度和中间激活值，更是将内存需求推向了TB级别</p><p>训练如此大规模的模型需要巨大的计算资源，单一设备的计算能力根本无法满足需求。通过分布式并行，可以将计算任务分解到多个设备上，实现高效的并行计算。</p><h4 id=基础并行策略dptppp class=heading-element><span>基础并行策略（DP、TP、PP）</span>
<a href=#%e5%9f%ba%e7%a1%80%e5%b9%b6%e8%a1%8c%e7%ad%96%e7%95%a5dptppp class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><h5 id=dp-data-parallelism-dp class=heading-element><span>DP (Data Parallelism, DP)</span>
<a href=#dp-data-parallelism-dp class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h5><p>核心原理：数据并行是最直观、最常用的并行方式。其核心思想是“模型复制，数据分片”。即将完整的模型副本放置在每一个计算设备（如GPU）上，然后将一个大的训练批次（mini-batch）分割成多个更小的微批次（micro-batch），每个设备独立处理一个微批次的数据。</p><p>工作流程：</p><ul><li>分发： 将一个全局批次的数据切分，分发给各个GPU。</li><li>前向传播： 每个GPU使用其上的模型副本独立计算其微批次数据的前向传播，得到损失。</li><li>反向传播： 每个GPU独立计算梯度。梯度同步： 这是DP的关键步骤。所有GPU上的梯度需要通过一个集合通信操作（通常是All-Reduce）进行聚合（如求和或求平均），以确保每个GPU最终都得到全局梯度 。</li><li>权重更新： 每个GPU使用同步后的全局梯度独立更新其本地的模型副本，从而保证所有副本在下一步开始前保持一致。</li></ul><blockquote><p>这里涉及到本文的第一个“分布式通信”原语，全归约（All-Reduce），它处理 GPU 实例和节点之间的同步和通信。其实很好理解：就是每一个GPU把数据发送给其余GPU，每一个GPU对于接收到的数据进行求和。 Reduce（归约）：将多个GPU的数据发送到指定的GPU然后求和。</p></blockquote><p>DP方法的优缺点：</p><p>优点：</p><ul><li>实现简单，易于集成到现有训练框架中</li><li>适用于大多数标准模型架构</li><li>扩展性较好，可轻松扩展到多GPU集群</li></ul><p>缺点：</p><ul><li>内存瓶颈显著，无法解决"模型太大，单卡放不下"的问题</li><li>随着GPU数量增加，All-Reduce操作的通信开销成为主要瓶颈</li><li>扩展效率受限于通信带宽和延迟</li></ul><p>关键优化：ZeRO (Zero Redundancy Optimizer)</p><p>为了克服DP的内存冗余问题，ZeRO[1]技术应运而生。它通过在不同设备间分割和管理模型状态（而不仅仅是数据）来极大地优化内存使用。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4j6cnz3zdj314007xwfc.jpg alt=image></p><p>ZeRO-DP使得数据并行也能用于训练单卡无法容纳的大模型，是当前大规模训练框架（如DeepSpeed[2]）的核心技术之一。</p><h5 id=tp-tensor-parallelism-tp class=heading-element><span>TP (Tensor Parallelism, TP)</span>
<a href=#tp-tensor-parallelism-tp class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h5><p>在使用 ZeRO 对模型的参数、梯度和优化器状态进行了分片，但当激活内存超过内存预算时，遇到了瓶颈。张量并行（TP），这是一种分片权重、梯度和优化器状态以及激活的方法——而且无需在计算之前将它们全部收集起来。</p><p>张量并行利用了矩阵乘法的数学特性，A×B 。</p>$$
\begin{aligned}
A\cdot B &= A\cdot [B_{1}, B_{2} \cdots] = [AB_{1}, AB_{2} \cdots] \\\\
A\cdot B &= [A_{1}, A_{2} \cdots] \begin{bmatrix}
B_{1} \\
B_{2} \\
\vdots
\end{bmatrix}
= \sum_{i=1}^{n}A_{i}B_{i}
\end{aligned}
$$<p>这意味着可以通过单独计算 B 的每一列或单独计算每一行并将结果组合起来来进行矩阵乘法。示例如下：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4j6tem6ujj30fa0bndg5.jpg alt=image></p><p>在张量并行中，张量会沿着特定维度被分割成 N 个片段，并分布到 N 个 GPU 上。矩阵可以在列或行上进行分割，从而实现行或列并行。</p><p><strong>第一个选项是使用按列（也称为按列线性）分片</strong>：将完整输入矩阵复制到每个工作节点，需要一种称为广播（broadcast）的操作，并将权重矩阵按列拆分。然后输入与部分权重矩阵相乘，最后使用全聚合（all-gather）操作合并结果。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4j6uhusssj31400pdtap.jpg alt=image></p><blockquote><p>“分布式通信”原语：
广播（broadcast）：将一个GPU上的数据同时发送到所有其他GPU。
全聚合（all-gather）：每一个GPU把数据发送到其余所有GPU，每个GPU对于接受到的数据拼接为完整数据。</p></blockquote><p><strong>第二种选项称为按行（或行线性）分片</strong>：行线性意味着将权重矩阵按行分割成块。因此，需要使用散播（scatter）操作（第四种分布式通信原语！）而不是列线性分片中所使用的广播操作。每个工作节点上的结果已经处于正确的形状，但需要求和以得到最终结果，因此这种场景也需要一个全归约操作：</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4j6vkskdvj31400ocac0.jpg alt=image></p><blockquote><p>“分布式通信”原语：
散播（scatter）：把一个GPU上的数据切分为多块，按照顺序向其他GPU发送数据块。</p></blockquote><p>核心原理： 张量并行，又称模型内部并行（Intra-layer Model Parallelism），旨在解决单层网络计算量过大或参数过多无法在单个设备上处理的问题。它将模型中的单个张量（如权重矩阵）沿特定维度切分到多个GPU上，并在这些GPU上协同完成运算。</p><p>工作流程（以Transformer中的MLP层为例）：</p><p>一个标准的MLP层包含两次线性变换。假设我们将模型并行到N个GPU上：</p><ol><li>第一个线性层的权重矩阵A可以按列切分成 [A1, A2, &mldr;, AN]。输入X与每个Ai相乘得到Xi * Ai。这是一个并行的矩阵乘法，无需通信。</li><li>第二个线性层的权重矩阵B可以按行切分成 [B1; B2; &mldr;, BN]。在进行第二次矩阵乘法前，需要将第一步的输出结果 [Y1, Y2, &mldr;, YN] 通过All-Gather操作在所有GPU间聚合，得到完整的Y。然后每个GPU计算Y * Bi。</li><li>最后，将各个GPU上的结果通过Reduce-Scatter或简单的加和操作得到最终输出。</li></ol><p>Megatron-LM[3]框架是张量并行的杰出代表，它巧妙地设计了Transformer中注意力层和MLP层的并行计算方式，将通信操作与计算有效结合。</p><p>优缺点分析</p><table><thead><tr><th style=text-align:center>优点</th><th style=text-align:center>缺点</th></tr></thead><tbody><tr><td style=text-align:center>（1）直接减少了单个GPU上的内存占用（包括模型参数和激活值）（2）使得训练远超单卡内存限制的模型成为可能（3）计算粒度细，可以更好地利用GPU的并行计算能力</td><td style=text-align:center>（1）通信开销巨大且频繁，通常仅限于在单个计算节点内部进行高效扩展（2）对设备间的通信带宽要求极高（3）实现复杂度较高，需要仔细设计通信模式</td></tr></tbody></table><h5 id=流水线并行-pipeline-parallelism-pp class=heading-element><span>流水线并行 (Pipeline Parallelism, PP)</span>
<a href=#%e6%b5%81%e6%b0%b4%e7%ba%bf%e5%b9%b6%e8%a1%8c-pipeline-parallelism-pp class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h5><p>核心原理：流水线并行，又称层间模型并行（Inter-layer Model Parallelism），将模型的不同层（或层块）分配到不同的GPU上，形成一个计算"流水线"。数据在一个设备上完成部分层的计算后，其输出（即激活值）被传递到下一个设备，继续后续层的计算。</p><p>工作流程：</p><ol><li>朴素流水线： 最简单的方式是将一个批次的数据依次通过所有GPU。但这会导致严重的“流水线气泡”（pipeline bubble），即在任何时刻，只有一个GPU在工作，其他GPU都在等待，利用率极低。</li><li>微批次流水线 (Micro-batching)： 为了解决气泡问题，GPipe[4]等框架引入了微批次的概念 。它将一个大的批次（mini-batch）再切分成多个微批次（micro-batches），并将这些微批次依次送入流水线。这样，当第一个微批次在第二个GPU上计算时，第二个微批次就可以在第一个GPU上开始计算，实现了计算的重叠，从而显著减少了GPU的空闲时间。</li></ol><p>优缺点分析</p><table><thead><tr><th style=text-align:center>优点</th><th style=text-align:center>缺点</th></tr></thead><tbody><tr><td style=text-align:center>（1）同样能有效降低单个GPU的内存占用（2）通信开销相对较低，因为通信只发生在流水线中相邻的两个阶段之间（3）通信内容主要是激活值，而非整个模型参数（4）实现相对简单，易于调试</td><td style=text-align:center>（1）仍然存在无法完全消除的"气泡"问题，特别是在流水线的启动和排空阶段（2）要求对模型进行合理的切分，以实现各阶段的负载均衡（3）&ldquo;木桶效应"会限制整体性能，最慢的阶段决定整体吞吐量（4）流水线深度增加会导致额外的启动和结束延迟</td></tr></tbody></table><h4 id=长序列并行策略序列上下文并行 class=heading-element><span>长序列并行策略：序列、上下文并行</span>
<a href=#%e9%95%bf%e5%ba%8f%e5%88%97%e5%b9%b6%e8%a1%8c%e7%ad%96%e7%95%a5%e5%ba%8f%e5%88%97%e4%b8%8a%e4%b8%8b%e6%96%87%e5%b9%b6%e8%a1%8c class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>随着模型对长文本处理能力的需求日益增长，输入序列长度成为新的内存瓶颈。传统的TP和PP主要解决参数存储问题，而SP和CP则专注于解决因序列过长导致的激活值内存爆炸问题。</p><h5 id=序列并行-sequence-parallelism-sp class=heading-element><span>序列并行 (Sequence Parallelism, SP)</span>
<a href=#%e5%ba%8f%e5%88%97%e5%b9%b6%e8%a1%8c-sequence-parallelism-sp class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h5><p>核心原理： Transformer架构中的自注意力机制（Self-Attention）需要在序列维度上进行全局计算，这使得沿序列维度进行并行化变得困难。序列并行巧妙地绕过了这一点，它选择性地对那些在序列维度上计算独立的模块（如LayerNorm、Dropout和MLP中的逐点操作）的输入激活进行切分。[5]</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4j774cbv8j30ti0am75e.jpg alt=image></p><p>工作流程：</p><p>在进入非并行化的模块（如自注意力）之前，被切分的激活需要通过All-Gather操作在序列维度上拼接回完整的张量。
在完成该模块的计算后，输出的激活可以再次被切分，或者通过Reduce-Scatter操作将计算（如梯度累加）分散到各个设备上。</p><p>价值与定位</p><p>SP是TP的一个重要补充。TP虽然切分了权重，但每个GPU上仍然需要存储完整的激活张量，这在长序列场景下会成为内存瓶颈。SP通过切分激活，进一步降低了内存占用，使得在有限的显存下能够训练更长的序列或使用更大的批次大小。截至2024年的研究表明：将SP与PP结合（如Mnemosyne 2D并行化[6]）可以在长上下文预填充处理中达到80%以上的扩展效率。</p><p>应用场景</p><ol><li>长文本处理：对于需要长文档、代码或对话的历史记录等场景，SP可以显著降低内存占用，支持更长的上下文窗口</li><li>批量处理：在需要同时处理多个独立任务时，SP可以提高GPU利用率，减少空闲时间</li></ol><blockquote><p>“分布式通信”原语： 散播归约（Reduce-Scatter）：先做Scatter再做Reduce，先将GPU的数据分割为多个数据块，然后每一块数据按GPU序列进行散播Scatter。对所有的GPU都进行散射之后，当前GPU所接受到的所有数据块进行求和，也就是Reduce归约。</p></blockquote><h5 id=上下文并行-context-parallelism-cp class=heading-element><span>上下文并行 (Context Parallelism, CP)</span>
<a href=#%e4%b8%8a%e4%b8%8b%e6%96%87%e5%b9%b6%e8%a1%8c-context-parallelism-cp class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h5><p>核心原理： 上下文并行[7]是处理超长序列的一种更激进的策略。与SP只切分部分激活不同，CP将网络输入和所有中间激活都沿序列维度进行划分。</p><p><img loading=lazy src=https://cdn.ipfsscan.io/weibo/large/005wRZF3gy1i4j7bok2gij31400hvjtc.jpg alt=image></p><p>工作流程（以注意力机制为例）：</p><p>CP的实现方式如下：</p><ol><li>输入切分： 将长度为S的输入序列切分为N份，每个GPU处理长度为S/N的子序列。</li><li>本地Q, K, V计算： 每个GPU根据自己的子序列计算出局部的Q, K, V张量。</li><li>全局K, V同步： 在进行注意力计算之前，所有GPU需要通过一次All-Gather通信，将其本地的K和V张量广播给所有其他GPU。这样，每个GPU就拥有了完整的K和V张量，但只拥有局部的Q张量。</li><li>注意力计算： 每个GPU使用其局部的Q和全局的K, V来计算注意力得分和输出，这部分计算是并行的。</li><li>反向传播： 梯度计算也遵循类似的通信模式，通常涉及Reduce-Scatter操作。</li></ol><p>价值与定位：</p><p>CP的主要目标是最大程度地减少激活值内存占用，从而支持数万甚至数十万长度的上下文窗口。Amazon SageMaker 和 Megatron Core 等框架已经集成了CP功能 。其代价是在注意力层内部引入了额外的通信开销，但对于那些因激活内存而无法运行的超长序列场景，CP是必不可少的解决方案。</p><table><thead><tr><th style=text-align:center>优点</th><th style=text-align:center>缺点</th></tr></thead><tbody><tr><td style=text-align:center>（1）极大降低内存占用（2）适用于需要处理数十万tokens的场景（3）可以与其他并行策略结合使用</td><td style=text-align:center>（1）引入额外的通信开销，尤其在注意力层（2）实现复杂度高，需要特殊硬件支持（3）可能影响计算效率，增加延迟</td></tr></tbody></table><h5 id=专家并行-expert-parallelism-ep class=heading-element><span>专家并行 (Expert Parallelism, EP)</span>
<a href=#%e4%b8%93%e5%ae%b6%e5%b9%b6%e8%a1%8c-expert-parallelism-ep class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h5><p>核心原理： MoE架构用一组稀疏激活的“专家”网络（通常是FFN层）替换了传统Transformer中的密集FFN层。对于每个输入的token，一个可学习的“门控网络”（Gating Network）或“路由器”（Router）会动态地选择一小部分（如Top-2）专家来处理它 。这样，模型的总参数量可以非常巨大（所有专家参数之和），但每个token的实际计算量（FLOPs）却保持不变或仅少量增加。</p><p>专家并行 (EP) 实现：</p><p>EP是为MoE模型量身定制的并行策略。其核心思想是将不同的专家分配到不同的GPU上。</p><ol><li>专家分发： 将E个专家平均分配到N个GPU上，每个GPU拥有E/N个专家。</li><li>路由计算： 所有GPU上的token首先通过门控网络，计算出各自应该被发送到哪些专家。</li><li>全局通信 (All-to-All)： 这是EP中最关键也最昂贵的步骤。所有GPU参与一次All-to-All通信，将每个token发送到其目标专家所在的GPU。</li><li>专家计算： 每个GPU用其本地的专家处理接收到的token。</li><li>结果返回 (第二次All-to-All)： 计算完成后，再次通过All-to-All通信将处理结果发送回token原始的GPU。</li></ol><p>优缺点分析</p><table><thead><tr><th style=text-align:center>优点</th><th style=text-align:center>缺点</th></tr></thead><tbody><tr><td style=text-align:center>（1）实现计算成本与模型参数量的解耦（2）是目前扩展模型至万亿参数规模的最有效路径（3）支持模型参数量与计算量分离扩展（4）可以通过增加专家数量而非计算量来扩展模型</td><td style=text-align:center>（1）高通信开销：两次All-to-All通信对网络带宽要求极高（2）负载不均：路由策略可能导致某些专家过载而另一些空闲（3）训练不稳定：路由器的学习过程可能很脆弱，导致训练崩溃（4）实现复杂度高，需要特殊硬件和软件支持</td></tr></tbody></table><h4 id=综合策略与性能基准 class=heading-element><span>综合策略与性能基准</span>
<a href=#%e7%bb%bc%e5%90%88%e7%ad%96%e7%95%a5%e4%b8%8e%e6%80%a7%e8%83%bd%e5%9f%ba%e5%87%86 class=heading-mark><svg class="octicon octicon-link" viewBox="0 0 16 16" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5.0 114.95 4.95l-2.5 2.5a3.5 3.5.0 01-4.95.0.751.751.0 01.018-1.042.751.751.0 011.042-.018 1.998 1.998.0 002.83.0l2.5-2.5a2.002 2.002.0 00-2.83-2.83l-1.25 1.25a.751.751.0 01-1.042-.018.751.751.0 01-.018-1.042zm-4.69 9.64a1.998 1.998.0 002.83.0l1.25-1.25a.751.751.0 011.042.018.751.751.0 01.018 1.042l-1.25 1.25a3.5 3.5.0 11-4.95-4.95l2.5-2.5a3.5 3.5.0 014.95.0.751.751.0 01-.018 1.042.751.751.0 01-1.042.018 1.998 1.998.0 00-2.83.0l-2.5 2.5a1.998 1.998.0 000 2.83z"/></svg></a></h4><p>在实践中，训练一个顶尖的大模型（如70B以上）绝不会只使用一种并行策略，而是将多种策略组合起来，形成所谓的3D或4D并行。</p><p>常见组合范式：</p><p>一个典型的4D并行配置可能是：</p><ul><li>张量并行 (TP)： 在单台服务器内部的GPU之间使用，以利用高速的NVLink互联。</li><li>流水线并行 (PP)： 在多台服务器之间使用，以切分模型的深度。</li><li>数据并行 (DP)： 在整个集群的所有设备上应用，以扩大总批次大小，加速收敛。</li><li>专家/序列/上下文并行 (EP/SP/CP)： 根据模型架构（是否为MoE）和应用场景（是否为长上下文）选择性地加入，作为第四个维度。</li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span title="更新于 2025-08-19 23:07:32">更新于 2025-08-19&nbsp;</span></div></div><div class=post-info-line><div class=post-info-md><span><a href=/2025/7ae2a3f/index.md title=阅读原始文档 class=link-to-markdown>阅读原始文档</a></span></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Pocket" data-sharer=pocket data-url=https://yitaonote.com/2025/7ae2a3f/><i class="fa-brands fa-get-pocket fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://yitaonote.com/2025/7ae2a3f/ data-title=分布式推理优化技术><i class="fa-brands fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/2025/acc2381/ class=post-nav-item rel=prev title=Kruskal算法求最小生成树><i class="fa-solid fa-angle-left fa-fw" aria-hidden=true></i>Kruskal算法求最小生成树</a></div></div><div class=post-reward><div class=comment></div><input type=checkbox class=reward-input name=reward id=fi-reward hidden>
<label class=reward-button for=fi-reward><i class="fa-solid fa-qrcode fa-fw" aria-hidden=true></i>赞赏</label><div class=reward-ways data-mode=static><div><img src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i429ilg93xj30u018zjut.jpg alt="yitao 支付宝"><span>支付宝</span></div><div><img src=https://cdn.ipfsscan.io/weibo/large/005wRZF3ly1i429iozq5hj30n00v8gne.jpg alt="yitao 微信"><span>微信</span></div></div></div><div id=comments><div id=giscus class=comment><script src=https://giscus.app/client.js data-repo=walker-ai/walker-ai.github.io data-repo-id=R_kgDOOM9c8w data-category=Announcements data-category-id=DIC_kwDOOM9c884CoX_P data-mapping=pathname data-strict=0 data-theme=preferred_color_scheme data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-lang=zh-CN data-loading=lazy crossorigin=anonymous async defer></script></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app/ rel="external nofollow noopener noreferrer">giscus</a>.</noscript></div></article><aside class=toc id=toc-auto aria-label=目录><h2 class=toc-title>目录&nbsp;<i class="toc-icon fa-solid fa-angle-down fa-fw" aria-hidden=true></i></h2><div class=toc-content id=toc-content-auto></div></aside></main><footer class=footer><div class=footer-container><div class="footer-line powered">由 <a href=https://gohugo.io/ target=_blank rel="external nofollow noopener noreferrer" title="Hugo 0.145.0"><img class=hugo-icon src=/images/hugo.min.svg alt="Hugo logo"> Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/hugo-fixit/FixIt target=_blank rel=external title="FixIt v0.3.17-8212d6fd"><img class=fixit-icon src=/images/fixit.min.svg alt="FixIt logo"> FixIt</a></div><div class="footer-line copyright" itemscope itemtype=http://schema.org/CreativeWork><i class="fa-regular fa-copyright fa-fw" aria-hidden=true></i>
<span itemprop=copyrightYear>2018 - 2025</span><span class=author itemprop=copyrightHolder>
<a href=/></a></span><span class="license footer-divider"><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div class=widgets><div class="fixed-buttons animate__faster d-none"><div class="fixed-button back-to-top" role=button aria-label=回到顶部><i class="fa-solid fa-arrow-up fa-fw" aria-hidden=true></i><span class="variant-numeric d-none">0%</span></div><div class="fixed-button view-comments d-none" role=button aria-label=查看评论><i class="fa-solid fa-comment fa-fw" aria-hidden=true></i></div></div><a href=https://github.com/walker-ai/walker-ai.github.io title="View source on GitHub" target=_blank rel="external nofollow" class="github-corner right d-none-mobile"><svg viewBox="0 0 250 250" aria-hidden="true" width="56" height="56"><path d="M0 0 115 115h15l12 27L250 250V0z"/><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentcolor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4l13.9-13.8C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8z" fill="currentcolor" class="octo-body"/></svg></a><div id=mask></div><div class=reading-progress-bar style=left:0;top:0></div><noscript><div class=noscript-warning>该网站在启用 JavaScript 的情况下效果最佳。</div></noscript></div><link rel=preload href=https://unpkg.com/katex@0.16.10/dist/katex.min.css as=style onload='this.removeAttribute("onload"),this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://unpkg.com/katex@0.16.10/dist/katex.min.css></noscript><link rel=stylesheet href=https://unpkg.com/pace-js@1.2.4/themes/blue/pace-theme-minimal.css><script src=https://unpkg.com/autocomplete.js@0.38.1/dist/autocomplete.min.js defer></script><script src=https://unpkg.com/fuse.js@6.6.2/dist/fuse.min.js defer></script><script src=https://unpkg.com/twemoji@14.0.2/dist/twemoji.min.js defer></script><script src=https://unpkg.com/sharer.js@0.5.1/sharer.min.js async defer></script><script src=https://unpkg.com/katex@0.16.10/dist/katex.min.js defer></script><script src=https://unpkg.com/katex@0.16.10/dist/contrib/auto-render.min.js defer></script><script src=https://unpkg.com/katex@0.16.10/dist/contrib/mhchem.min.js defer></script><script src=https://unpkg.com/pangu@4.0.7/dist/browser/pangu.min.js defer></script><script src=https://unpkg.com/pace-js@1.2.4/pace.min.js async defer></script><script>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:100},comment:{enable:!0,expired:!1,giscus:{darkTheme:"dark",lightTheme:"light",origin:"https://giscus.app"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},pangu:{enable:!0,selector:"article"},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/search.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:30,threshold:.3,type:"fuse",useExtendedSearch:!1},twemoji:!0,version:"v0.3.17-8212d6fd"}</script><script src=/js/theme.min.js defer></script></body></html>