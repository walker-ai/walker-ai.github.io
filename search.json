[{"categories":["推理优化"],"content":"批处理 参考文献：https://zhuanlan.zhihu.com/p/1941421923872514352 ","date":"2025-08-24","objectID":"/2025/8cb0fe8/:1:0","tags":null,"title":"推理引擎请求调度优化","uri":"/2025/8cb0fe8/"},{"categories":["推理优化"],"content":"静态批处理（Static Batching） 请求被放入批处理队列中，当批处理队列满了之后再运行。 静态批处理是批处理请求最简单的实现。但它会大幅增加延迟，从而限制其使用场景。 如果说单独运行每个请求就像每个人开自己的车，那么批处理就像一辆公交车。如果公交车采用静态批处理，司机会等待车内乘客满载，然后开往目的地。这确保了公交车每次行驶时都满载。同样，用于模型推理的静态批处理会等到收到一定数量的请求后，再运行单个批处理来同时处理这些请求。 当对于某个业务流程来说延迟不是问题时，例如每天处理大量文档，静态批处理是最合适的。静态批处理会增加在系统其他位置协调请求的复杂性。使用静态批处理需要一个管理良好的请求队列来为模型提供数据，并且需要一个以大块形式接收模型输出的方法。 ","date":"2025-08-24","objectID":"/2025/8cb0fe8/:1:1","tags":null,"title":"推理引擎请求调度优化","uri":"/2025/8cb0fe8/"},{"categories":["推理优化"],"content":"动态批处理（Dynamic Batching） 请求在收到时被分批放置到队列中，在队列满了或自第一个请求以来经过足够的时间后进行批处理。 静态批处理非常适合日常作业或后台处理。但对于延迟敏感（这里主要是和用户的交互相关业务场景）的生产部署（例如：根据用户输入生成图像），静态批处理并不适用。 回到我们之前关于公交车的比喻，想象一下，在车流量不大的日子里，你是第一个上车的人。如果你必须等到车上坐满才能出发，那你得等很长时间。但如果司机在第一个乘客上车时启动一个计时器，并在车上坐满或计时器用完（以先到者为准）时出发，那会怎么样呢？这样，你最多只需要等几分钟。 动态批处理的工作方式相同。您可以使用以下命令设置动态批处理： 预设的最大批量大小，您希望在每次进行批处理之前达到该大小。 在运行部分批处理之前接收第一个请求后等待的窗口。 假设你设置的模型服务器的批处理大小为 16 个请求，窗口为 100 毫秒。当服务器收到第一个请求时，它将： 在 100 毫秒内接收 15 个以上请求并立即运行完整批次，或者 接收少于 15 个请求，并在 100 毫秒后运行部分批处理。 动态批处理非常适合 Stable Diffusion XL 等模型的实时流量，因为每个推理请求所需的时间大致相同。具体部署的正确设置取决于流量模式和延迟要求，但动态批处理可为您提供多种选项的灵活性。 ","date":"2025-08-24","objectID":"/2025/8cb0fe8/:1:2","tags":null,"title":"推理引擎请求调度优化","uri":"/2025/8cb0fe8/"},{"categories":["推理优化"],"content":"连续批处理（Continuous Batching） 来源于 Orca OSDI'22 请求按令牌逐个进行处理，当旧请求完成并释放 GPU 上的空间时，新请求就会得到处理。 虽然动态批处理非常适合图像生成等场景，其中每个输出大约需要相同的时间来创建，但我们可以通过连续批处理为 LLM 做得更好。 LLM 会创建一系列 token 作为输出。这些输出序列的长度会有所不同——模型可以回答一个简单的问题，也可以通过逐步推理进行详细的分析。如果使用动态批处理方法，则每批请求都需要等待最长的输出完成后才能开始下一批请求。这会导致 GPU 资源闲置。 连续批处理在令牌级别而非请求级别进行。LLM 推理的瓶颈在于模型权重的加载。因此，对于连续批处理，模型服务器会按顺序加载模型的每一层，并将其应用于每个请求的下一个令牌。在连续批处理中，相同的模型权重可用于生成一个响应的第 5 个令牌和另一个响应的第 85 个令牌。 在公交车的例子中，连续批处理类似于现实世界中公交线路的运作方式。当司机沿着路线行驶时，乘客的乘坐时间会有所不同。当一位乘客到达目的地时，就会为另一位乘客腾出座位。 通过消除等待每个批次的最长响应完成的空闲时间，连续批处理比动态批处理提高了 GPU 的利用率。 ","date":"2025-08-24","objectID":"/2025/8cb0fe8/:1:3","tags":null,"title":"推理引擎请求调度优化","uri":"/2025/8cb0fe8/"},{"categories":["推理优化"],"content":"PD分离 参考文献：https://zhuanlan.zhihu.com/p/19796399275 ","date":"2025-08-24","objectID":"/2025/8cb0fe8/:2:0","tags":null,"title":"推理引擎请求调度优化","uri":"/2025/8cb0fe8/"},{"categories":["推理优化"],"content":"SGL和vllm区别 参考：https://www.zhihu.com/question/666943660/answer/1940915117643530378 ","date":"2025-08-24","objectID":"/2025/8cb0fe8/:3:0","tags":null,"title":"推理引擎请求调度优化","uri":"/2025/8cb0fe8/"},{"categories":["找工作"],"content":"GPU 架构 参考文献：https://zhuanlan.zhihu.com/p/12083951223 ","date":"2025-08-22","objectID":"/2025/34980c2/:1:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"SM（Streaming Multiprocessor）结构 其中包含一些核心组件： CUDA core（即Streaming Processor，SP）：其中包含整数处理单元和单精度浮点数处理单元，用于执行基本的数值运算。不同架构中CUDA core数量不同，这个数量在一定程度上体现了GPU的计算能力（并不是完全决定，还有如时钟频率，内存带宽，指令集等其他影响因素） Register File：寄存器文件。存放指令操作数；也有一些特殊寄存器用于存放系统变量，例如grid维度，thread blcok维度，线程id等等。 Loca/Store Units（LD/ST）：执行内存（显存、shared memory）读写数据命令。 Special Function Units（SFU）：执行一些特殊函数，如sqrt，sin，cos等。 Warp Scheduler：GPU线程调度执行时是以一组线程（warp）为单位的，Warp Scheduler从驻留在SM上的warp中选 择合适的warp用于执行。 Dispatch Unit：负责从Warp Scheduler选中的线程中取出要执行的指令发射到CUDA core去执行。 Shared Memory/L1 Cache：shared memory可以用于同一个thread block中的线程间互相通信，是可以通过编程读写的。L1 Cache则不能被编程，由GPU决定存放和淘汰内容。这里把Shared Memory和L1 Cache放在一起是因为它们在物理存储上是共用的，可以通过API控制两者的配比。 常量、纹理缓存与全局内存缓存：有的gpu架构这三类内存结构也同样存在于一个SM中： 以上以 Fermi 架构举例，后续的新架构都是在其基础上的进一步发展，例如更多的CUDA core数量，更大的内存容量，更高的IO带宽，以及增加一些新的组件如Tensor Core，RT Core等 ","date":"2025-08-22","objectID":"/2025/34980c2/:1:1","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"Hopper架构 参考文献：https://zhuanlan.zhihu.com/p/1900569635532800833 由于实习期间使用的都是 H20 集群，因此学习一下 Hopper 架构的特点 ","date":"2025-08-22","objectID":"/2025/34980c2/:2:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"GPU线程调度 一个CUDA kernel对应一个grid，一个grid分成若干个thread block，每个thread block中包含若干线程。那么从硬件角度看，GPU在调度Kernel时，粗粒度看分成两个阶段，即SM分配和线程调度执行。 ","date":"2025-08-22","objectID":"/2025/34980c2/:3:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"SM分配 GPU会根据SM资源情况来进行block的分配，只有当SM上剩余资源足够满足一个block的总资源需求时，SM才有可能被分配给当前block。这里的资源主要指共享内存和寄存器，即SM上的Shared Memory及寄存器是不能超额分配的；但CUDA core，SFU等指令执行单元则是可以超额分配的。 例如Fermi架构中每个SM上CUDA core只有32个，但是一个SM最大可以支持1536个线程，因为虽然不能同时执行这1536个线程的指令，但是可以对CUDA core进行时分复用，执行不同线程时进行上下文切换即可，GPU上的线程上下文切换效率远高于CPU上的线程切换，因为线程的寄存器已经事先分配好，切换时并不需要像CPU上那样把寄存器状态在主机内存上进行换入换出操作。 当然一个SM上最多能够支持多少个线程还是有硬性限制的，具体限制数值取决于具体架构。一个thread block分配到一个SM上之后就会驻留下来直到执行完毕，同一个grid中的thread block可能分配到不同的SM上，反过来同一个SM也可能被分配给来自不同grid的多个thread block；一个thread blcok分配到一个SM上之后，则称为Active thread block，一个SM被分配thread block之后则称为Active SM。 ","date":"2025-08-22","objectID":"/2025/34980c2/:3:1","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"线程调度执行 在调度执行时，GPU并不会针对单个线程去调度，而是把一个thread block进一步划分为若干个Warp（线程束）为单位进行调度，一个warp包含32个线程（目前所有架构上都是32），每个Warp Sheduler每个时钟周期（cycle）选择一个Warp将其指令通过Dispatch Unit发射到执行单元。也就是说一个thread block中的线程只是在逻辑上是同步执行，但是在硬件层面则不一定。可以从资源分配和调度的角度对此与主机上的调度做一个类比，thread block类似于主机上的进程，是资源分配单位，warp类似于主机上的线程，是调度执行单位。 Warp有如下几种状态： Active：一个Acitve的thread block中的Warp都是Active的，也即已经分配到SM的Warp都是Active Warp。 Stalled：当前Cycle暂时不能执行下一条指令的Warp，有多种情况可能导致Warp处于Stalled状态，常见的有： thread block内的线程同步，部分先执行的Warp必须等待同一个block内的后执行的Warp到达同一个同步点。 线程需要的数据还没有传输完成，需要等待数据。 下一条指令依赖于上一条指令的输出，但是上一条指令还没有执行完成。 Eligible：当前Cycle已经准备好可以执行下一条指令的Warp。Warp Scheduler在选择Warp时只会从Eligible Warp中进行选取。 Selected：被Warp Scheduler选中准备执行其中线程下一条指令的Warp。 注意一个Warp中线程的同步性仅仅体现在调度上，即一个Warp中的所有线程是同时被Scheduler选中的，但是在指令发射和执行上则不一定了，至少可以从几个方面去看： Instruction replay：同一条指令发射多次称为instruction replay。CUDA指令可以分为不同类型，不同类型指令使用不同类型的执行单元执行，例如memory的读写指令是通过LD/ST units执行，特殊函数如sin，cos是通过SFU执行，即并非所有指令都是通过CUDA core执行。而对于某种类型执行单元来说，一个SM上可能都没有32个，例如Fermi中一个SM上只有4个SFU，那么一个Warp执行cos指令时，就需要发射和执行8次。 Warp divergence：当一个Warp中的线程因为分支条件(如if else)不同而走到不同的路径时会造成warp divergence，此时不同分支的线程会形成不同分组，这些分组串行在不同的cycle执行 ","date":"2025-08-22","objectID":"/2025/34980c2/:3:2","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"延迟隐藏 延迟隐藏就是指通过并行执行，充分利用硬件执行单元，使得不同操作能够在时间上重叠，最大化某一种或几种操作的吞吐量（延迟隐藏的主要优化目标是吞吐量，而不是单个操作的时长）。比如这里的当Warp等待内存时，SM立即切换其他就绪Warp。 了解详情可以进一步学习：利特尔法则（little’s law） 以上三个步骤：SM资源分配、线程调度执行、延迟隐藏 构成了 block 调度执行的完整流程。 ","date":"2025-08-22","objectID":"/2025/34980c2/:3:3","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"warp的划分 CUDA线程与数组在内存中的排布类似，也分为逻辑视图和硬件视图，在逻辑视图上，一个thread block可能是1到3维的，但是在硬件视图上，这些线程上仍然是1维排布的，即按row-major方式将连续的32个线程组成一个Warp，用公式表达就是： // thread_id是按row major方式将线程在block中的3维坐标转为1维线性坐标， // 维度从内到外为x, y, z（下面这个转换方式对于1,2,3维thread block都是通用的） thread_id = threadIdx.z * blockDim.y * blockDim.x + threadIdx.y * blockDim.x + threadIdx.x // warp_id相同的线程就属于同一个Warp // warpSize也是一个内置变量，可以在kernel函数中直接引用，表示warp大小，目前来说是32 warp_id = thread_id / warpSize 提示 当一个thread block中的线程总数不是32的整数倍时，从编程视角看，最后一个Warp是不满32个线程的，但是在GPU硬件上仍然会占用32个线程的资源，只是其中部分线程会被标识为不活跃线程，这种情况下就会造成一些资源的浪费，所以在实际使用时，要尽量保证thread block中的线程数为32的整数倍。 ","date":"2025-08-22","objectID":"/2025/34980c2/:3:4","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"warp divergence Warp Divergence是指同一个Warp中的不同线程由于分支条件（例如if else）不同而进入不同的代码分支的情况。这个在CPU上的多线程不是太大问题，但是在CUDA中则会导致性能下降，因为在发生Warp Divergence时，Warp中走不同分支的线程会串行化执行，以一个2路分支为例，如下伪代码： if (condition) { ... } else { ... } 假设Warp内前一半线程condition 为true ，后一半线程condition为false，那么执行时两部分线程就分开串行执行了，每一部分线程执行时，另一部分线程标识为不活跃线程，但是任然占用执行单元。这种情况下一方面因为串行化增加了整体执行时间，另一方面不活跃线程降低了资源利用率。如下图所示（coherent部分表示没有分支的代码）： 一个Warp一次执行一条公共指令。如果Warp中的线程由于数据依赖而发生条件分支发散，则warp会执行每个需要的分支路径，同时禁用不在该路径执行的线程。因此当一个 Warp中的32个线程都执行相同的执行路径时，Warp的效率是最高的。 编译器可能会进行循环展开，或者会通过分支预测来优化短的if或switch块。在这些情况下，所有Warp都不会发散。程序员还可以使用#pragma unroll指令控制循环展开。如下图所示，每个线程执行不同数量的循环迭代，循环迭代的数量在四和八之间变化。在前四次迭代中，所有线程都是活动的并执行A。在剩余的迭代中，一些线程执行A，而其他线程因为已经完成它们的迭代而不活动。 Warp Divergence对性能有损害，所以在可能的前提下，应该尽量通过重新设计程序逻辑，算法等方式避免或减少这种情况的发生。 ","date":"2025-08-22","objectID":"/2025/34980c2/:3:5","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"SM占用率 ","date":"2025-08-22","objectID":"/2025/34980c2/:3:6","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"Roofline Model 参考文献：https://zhuanlan.zhihu.com/p/24066764550 ","date":"2025-08-22","objectID":"/2025/34980c2/:4:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"概念 Roofline Model能帮助我们判断cuda 程序是显存瓶颈(memory bound)还是计算瓶颈(compute bound)，以及判断当前资源利用率的情况。 Roofline Model的横坐标是计算强度(computational intensity)，单位是FLOP/B，即每读一个Byte所能产生的FLOP数。例如以下base matrix multiply代码: __global__ void cudaMatrixMultiplyBase(float *A, float *B, float *C, int M, int N, int K) { int row = blockIdx.x * blockDim.x + threadIdx.x; int col = blockIdx.y * blockDim.y + threadIdx.y; if (row \u003c M \u0026\u0026 col \u003c N) { float sum = 0.0f; for (int k = 0; k \u003c K; ++k) { sum += A[row * K + k] * B[k * N + col]; } C[row * N + col] = sum; } } 它的computational intensity是0.25，因为每次2次计算（乘加）需要读8个Byte Roofline Model是描述一个进程的性能与它所在的硬件的关系，横坐标是计算强度，纵坐标是FLOP/s，如下图所示： 其中 红线（屋檐）和绿线（屋顶） 与具体硬件有关，红线的斜率是内存带宽，不同的硬件斜率不同（带宽越大，斜率越高），越靠近红线说明对带宽资源利用率越高；任何一个点到原点的斜率不可能超过硬件最大带宽 $\\beta$ 绿线是硬件的计算峰值，高度越靠近绿线，说明对计算资源利用率越高；任何一个点的P值(纵坐标)不可能超过 $\\pi$ 计算强度D是有可能超过 $I_{max}$ 的，假设某台机器A计算峰值10GFLOPS，内存带宽是10GB/s，那么 $I_{max}=1$ 。假设某个模型疯狂重复计算，对1GB的数据计算5次乘加，计算量 C=10GFLOPs，那么计算强度 $I=10$ 为了加深理解，分别举几个示例说明图中的7个点是如何产生的： 假设：机器A计算峰值10GFLOPS，内存带宽是10GB/s，那么 $I_{max}=1$，下面的示例都发生在机器A上： 有一个计算强度高的模型 $D_h$，读取10GB的数据产生100GFLOPs的计算， $I=10$； 另外有一个计算强度低的模型 $D_l$，读取10GB的数据只产生5GFLOPs的计算， $I=0.5$； $x_2, x_3$ 不可能在现实中出现，正如上面所说，$x_2$ 超过了带宽限制，$x_3$ 超过了硬件计算限制， 举个例：还是对于机器A，计算峰值10GFLOPS，内存带宽是10GB/s，假设有一个计算强度很高的模型，读取1GB数据计算100GFLOPs，计算强度为100，但实际上由于受到计算峰值的限制，P=10GFLOPS，此时计算的斜率为 $\\dfrac{10}{100}=0.1\u003c1=\\beta$ $x_1, x_4$ 是在计算强度很低的模型中可能出现，以模型 $D_l$ 为例，$x_1$ 出现的原因是由于程序实现问题导致计算性能都很低：例如计算5GFLOPS用了2s（理想情况是0.5s）导致纵坐标P比较低：($I=0.5, P=2.5$)，属于工程提升空间很大，不需要增加硬件性能就可大幅度提升性能。 $x_1$ 优化后变为 $x_4$，计算5GFLOPS只用了1s（理想情况是0.5s，但是受限于带宽，1s只能传输10GB数据），已经十分接近屋檐这条线了，但计算强度太低了，优化方向是提升计算强度（只能改变模型，属于模型的计算强度透支）来增加性能，如果算法没办法改进的情况下，那提升性能的方式只能堆更高的带宽了。 $x_4$ 属于内存带宽受限型（模型不动的情况下，无脑增加带宽）。 $x_5$ 属于把硬件资源都用满了（计算和带宽全都吃满），且模型的计算强度也被挖掘得刚刚好。其性能即受限于硬件带宽又受限于计算资源，增加任何一项硬件资源（带宽和计算）都没法提升性能。“刚刚好”的意思是计算强度和硬件刚好匹配，其即使增加了硬件计算和带宽，不改变模型的情况下，也没办法提升性能。它即受限于硬件，又受限于模型。 $x_6$ 也属于把硬件资源都用满了，但模型本身的计算强度还有很大潜力，属于硬件压制了模型。只需要堆计算资源性能就可以提升，模型和带宽都不用动，属于计算受限型。典型示例就是计算强度高的模型 $D_h$ ，计算强度为10，某个实现使得带宽和计算都已经跑满了（每秒读取10GB，计算10GFLOPs，受限于硬件算力和带宽） $x_7$ 和 $x_1$一样，硬件利用率太低了，需要改进实现，则能成长为 $x_6$ 一样的潜力型选手。例如计算强度高的模型 $D_h$，计算强度为10，但某个实现使得计算没有打满（例如带宽没有打满，10GB数据花了2s，即便计算打满也导致GFLOPS为5GFLOPS，或者计算资源没有吃满，例如10GB数据用了1s传输，2s计算—重复计算、bankconflict之类的导致） 总结： Roofline model中，越靠近屋檐（红色）或者屋顶（蓝色）的线，说明已经将硬件资源吃得很满，除非修改模型改变计算强度，或者修改硬件，否则很难提升。远离这些线的点，在实现是有提升空间的，离得远，说明实现对硬件的利用率越差。 Roofline model中，在 $I_{max}$ 左边的点，都是内存带宽限制性；在 $I_{max}$ 右边的点，属于计算限制型。如果模型在 $I_{max}$ 的左边且靠近屋檐（红色）线，那么不改模型算法的情况下提升带宽资源能增加性能，如果在 $I_{max}$右边且靠近屋顶（蓝色）的线，那么不改模型算法的情况下提升计算资源能增加性能 不可能有点会处于超过屋檐（红色）或者屋顶（蓝色）的区域 ","date":"2025-08-22","objectID":"/2025/34980c2/:4:1","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"Bank Conflict 与 避免方法 参考视频 ","date":"2025-08-22","objectID":"/2025/34980c2/:5:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"Bank概念 cuda内核在执行的时候往往是以warp为单位去调度和执行，一个warp 32 个线程。所以，为了能够高效访存，shared memory中也对应了分成了32个存储体，这个存储体称之为 bank 。 bank 是CUDA中一个重要概念，是内存的访问时一种划分方式，在GPU中，访问某个地址的内存时，为了减少读写次数，访问地址并不是随机的，而是一次性访问bank内的内存地址，类似于内存对齐一样。一般GPU认为如果一个程序要访问某个内存地址时，其附近的数据也有很大概率会在接下来会被访问到。 （目前N卡都是32个bank），分别对应 warp 中32个线程。 bank的宽度，代表的是一个bank所存储的数据的大小宽度。可以是： 4 字节（32bit，单精度浮点数 float32） 8 字节（64bit，双精度浮点数 float64） 每 31 个bank，就会进行一次 stride。 比如说 bank 的宽度是4字节，我们在 share mem 中申请了 float A[256] 大小的空间，那么： A[0], A[1], ..., A[31] 分别在 bank0, bank1, ..., bank31 中 A[32], A[33], ..., A[63] 也分别在 bank0, bank1, ..., bank31 中 所以 A[0] 和 A[32] 是共享一个bank的 ","date":"2025-08-22","objectID":"/2025/34980c2/:5:1","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"Bank Conflict 一个很理想的情况是，32个thread，分别访问share mem中32个不同的bank，没有 bank conflict，一个 memory 周期完成所有的 memory read/write（行优先访问） 那最不理想的情况就是，32个thread，访问 share mem 中的同一个bank，导致最严重的 bank conflict，需要32个memory 周期才能完成所有的 memory read/write（列优先访问）。如果在block内多个线程访问的地址落入到同一个bank内，那么就会访问同一个bank就会产生bank conflict，这些访问将是变成串行。 ","date":"2025-08-22","objectID":"/2025/34980c2/:5:2","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"使用 Padding 缓解 Bank Conflict 为了方便解释，这里使用了8个bank一次stride进行举例。（实际CUDA设计中依然是32个bank一次stride） 可以在申请share mem时多申请一列（padding），这样就会更改share mem的布局，使得32个线程访问变成串行。 但会产生未对齐的地址和引入无效数据，导致内存占用增加和干扰需要对齐地址的快速指令。 ","date":"2025-08-22","objectID":"/2025/34980c2/:5:3","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"使用 Swizzle 缓解 Bank Conflict 参考文献：https://www.zhihu.com/question/667972067 Swizzle通过重新映射内存地址，可以将原本访问相同Bank的线程分散到不同的Bank。Swizzle的核心思想是通过物理地址映射避免Bank Conflict，同时保持逻辑地址不变。在CUTLASS中，Swizzle的实现通过一系列变换对内存布局进行重排。通过行列坐标的异或操作，Swizzle确保每个线程访问不同的Bank，从而实现了Bank Conflict Free的内存访问。 half* naive_layout(half* data, int r, int c) { return \u0026(data[r * columns + c]); } half* padding_layout(half* data, int r, int c) { return \u0026(data[r * (columns + 1) + c]); } // 一个可能的实现 Swizzle 的方式： half* row_swizzled_layout(half* data, int r, int c) { auto address = (uint64_t)\u0026data[r * columns + c]; return reinterpret_cast\u003chalf *\u003e(address ^ (r \u003c\u003c 2)); } 可以用padding和swizzle的示意图来解释下这种物理地址映射的概念： ⬆️bank conflict free ⬆️bank conflict solved by padding ⬆️bank conflict solved by swizzle 如果warp中的线程经过最多两次冲突就能得到所要的数据则成为2-way bank conflict，如果同一个warp中的所有线程访问一个bank中的32个不同的地址，则需要分32此，则称为32-way bank conflict。 bank conflict只发生在同一个warp的线程之间 参考文献： https://zhuanlan.zhihu.com/p/4746910252 ","date":"2025-08-22","objectID":"/2025/34980c2/:5:4","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"CUDA Stream \u0026 CUDA Graph 参考文献：https://zhuanlan.zhihu.com/p/699754357 ","date":"2025-08-22","objectID":"/2025/34980c2/:6:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"CUDA Stream cuda编程里最重要的特性就是异步：CPU提交任务到GPU中异步执行。为了控制异步之间的并发顺序，cuda引入了stream和event的概念。本文尝试分析理解stream和event的含义，充分理解并使用stream、event，是正确、高效利用GPU的必要条件。 只有一个CPU thread的情况 当不考虑GPU的时候，CPU线程就在不断地执行指令。从时间维度上看，就是这样的一条线： 一个CPU thread与一个GPU执行单元 GPU相当于CPU的附属硬件，当我们增加了一个GPU执行单元的时候，由CPU下发任务给GPU，于是从时间维度上看，就出现了两条线： 这里在GPU和CPU之间还增加了一个GPU队列。当我们在CPU上调用launch kernel的时候，本质上就是把这个kernel放入到GPU的执行队列里。然后，驱动负责维护这个队列，每当执行引擎（硬件资源）空闲的时候，就执行队列里的kernel。 上图描述了这些关键时间点： CPU launch kernel 1，kernel 1入队，此时GPU空闲，于是kernel 1马上开始执行 CPU调用host function，与GPU无关 CPU launch kernel 2，kernel 2入队，此时GPU还在执行kernel 1，于是kernel 2继续待在队列里 GPU执行完kernel 1后，驱动程序发现队列里还有kernel 2，于是开始执行kernel 2（这一步不需要CPU参与） 早期的GPU硬件上只有一个execution engine，因此，不论是哪个进程、哪个线程发起的kernel launch，都在同一个队列里排队。随着GPU的发展，GPU上面开始出现了多个execution engine。 一个CPU thread与2个GPU执行单元 当我们有两个或多个GPU执行单元的时候，我们就可以让GPU kernel之间也并行起来： 图中蓝色阴影区域就是两个GPU kernel并发执行的时间段。 这里要注意，kernel 1和kernel 2能不能并发执行，需要由用户来决定，硬件不能擅作主张，否则万一kernel 2要读取kernel 1算出来的数据，那并发执行的结果就是错的。 为了给用户提供这种控制权，于是我们就有了stream的概念。一个stream就对应于一个执行队列（加一个执行单元），用户可以自行决定是否把两个kernel分开放在两个队列里。 stream是cuda为上层应用提供的抽象，应用可以创建任意多个stream，下发任意多个kernel。但如果正在执行的kernel数目超过了硬件的execution engine的数量，那么即使当前stream里没有kernel正在执行，下发的kernel也必须在队列里继续等待，直到有硬件资源可以执行。（注意：此时kernel 执行与CPU下发kernel之间依然是异步的） 由此，我们可以总结得到：一个GPU kernel可以执行的必要条件是它在stream的队列首位且存在执行kernel的空闲硬件资源。 创建stream的时候，我们也可以用 cudaStreamCreateWithPriority 为它指定优先级。当多个GPU kernel都可以执行的时候，cuda driver负责调度，优先执行high priority的stream里的kernel。 stream之间的操作：cuda event 我们不仅可以从CPU thread操作stream，也可以在一个stream上面操作另一个stream。当然，stream只是一层抽象，我们依然要借用CPU thread的辅助，来指导“一个stream上面操作另一个stream”。具体的操作也很简单，就是一个stream上面的kernel想等待另一个stream上面的kernel执行结束之后再执行。 为了实现这个目标，我们需要记录另一个stream上面的队列状态，这就是cuda event。让我们来看一个具体的例子： 它实现的功能是： 在stream 1上面执行kernel 1 和 kernel 3 在stream 2上面执行kernel 2，但是必须等到stream 1上面的kernel 1执行结束之后才能开始 为此，我们在stream 1上面launch kernel 1之后，创建一个event来记录当前stream 1的状态（具体来说就是队列里有哪些kernel还没执行），然后把这个event放进stream 2的队列里，让stream 2执行一个想象中的“wait for event 1”的kernel，这个kernel直到kernel 1执行结束之后才离开队列。于是，在这个event之后加入队列的kernel 2，就必须等到kernel 1执行结束之后才能开始了。 这样的好处在于，我们不仅控制了kernel 1和 kernel 2的执行顺序，而且把kernel 2和kernel 3并发执行了，节省了时间（蓝色区域为两个kernel同时执行的时间段）。 ","date":"2025-08-22","objectID":"/2025/34980c2/:6:1","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"CUDA Graph 对GPU进行性能优化时，cudagraph是绕不开的话题。不仅是GPU，大部分的xpu都会提供类似graph mode的优化，相比于每次分别由CPU进行kernel launch的eager mode，graph mode通常都会有较大性能提升，然而也经常容易出现各种各样的奇怪问题。 由于实习期间对CUDA Graph接触的比较多，实际上 CUDA Graph主要是为了减少 kernel launch 造成的开销，于是乎将多个琐碎的小kernel改为一次launch，具体的操作方式分为：一次Capture-\u003e多次Replay graph capture期间禁止执行的函数 当一个stream处在graph capture状态时，实际上CPU下发的kernel都没有执行，只是被记录下来了。因此，与GPU kernel执行状态相关的函数都不能使用，例如： 对stream进行同步（cudaStreamSynchronize） 隐含stream同步的操作（对context进行同步、对device进行同步，都会强制内部包含的全部stream进行同步，如果其中有stream处在graph capture状态，就会报错） 隐含stream同步的操作,如当前graph capture的stream是blocking stream，则涉及null stream的操作都不可用，例如cudaMalloc 对stream上面record的event进行的状态查询、同步操作 另外断点调试在这期间也是用不了的 更详细的限制可以看官方文档：https://docs.pytorch.org/docs/stable/notes/cuda.html import torch from contextlib import contextmanager @contextmanager def graph_capture(pool=None, stream=None, capture_error_mode: str = \"global\", dump_path=None): g = torch.cuda.CUDAGraph() if dump_path is not None: g.enable_debug_mode() with torch.cuda.graph(cuda_graph=g, pool=pool, stream=stream, capture_error_mode=capture_error_mode): yield g if dump_path is not None: g.debug_dump(dump_path) import ctypes # Load the CUDA runtime library cudart = ctypes.CDLL('libcudart.so') # Define cudaMemcpyKind enumeration as in the CUDA API cudaMemcpyHostToHost = 0 cudaMemcpyHostToDevice = 1 cudaMemcpyDeviceToHost = 2 cudaMemcpyDeviceToDevice = 3 cudaMemcpyDefault = 4 # Setup the prototype of the cudaMemcpyAsync function cudaMemcpyAsync = cudart.cudaMemcpyAsync cudaMemcpyAsync.argtypes = [ ctypes.c_void_p, # void* dst ctypes.c_void_p, # const void* src ctypes.c_size_t, # size_t count ctypes.c_int, # enum cudaMemcpyKind ctypes.c_void_p # cudaStream_t stream ] cudaMemcpyAsync.restype = ctypes.c_int # Placeholder input used for capture static_a = torch.zeros((5,), device=\"cpu\") static_a = static_a.pin_memory() static_b = torch.zeros((5,), device=\"cpu\") static_b = static_b.pin_memory() static_output = torch.zeros((5,), device=\"cpu\") static_output = static_output.pin_memory() def compute(): a = static_a.to(\"cuda\", non_blocking=True) b = static_b.to(\"cuda\", non_blocking=True) output = (a + b) result = cudaMemcpyAsync(static_output.data_ptr(), output.data_ptr(), output.numel() * output.element_size(), cudaMemcpyDeviceToHost, torch.cuda.current_stream().cuda_stream) assert result == 0 return static_output # Warmup before capture s = torch.cuda.Stream() s.wait_stream(torch.cuda.current_stream()) with torch.cuda.stream(s): for _ in range(3): compute() torch.cuda.current_stream().wait_stream(s) # Captures the graph # To allow capture, automatically sets a side stream as the current stream in the context with torch.cuda.nvtx.range(\"capture\"): with graph_capture(dump_path=\"graph.dot\") as g: compute() # Run the graph g.replay() torch.cuda.current_stream().synchronize() print(static_output) static_a += 1 static_b += 2 g.replay() torch.cuda.current_stream().synchronize() print(static_output) 其中，因为PyTorch的功能的一些限制，我们的代码发生了变化：PyTorch里的 a.to(\"cpu\")，会强制同步使得host to device完成同步，即使加上 non_blocking=True 也无法改变。这与cudagraph不兼容。为了解决这个问题，我们手动调用了 cudaMemcpyAsync 函数，实现了异步拷贝到CPU的功能。（如果要PyTorch里直接实现这一功能，需要async CPU的支持。） 执行以上代码，捕获的计算图为： ","date":"2025-08-22","objectID":"/2025/34980c2/:6:2","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"PTX \u0026 SASS \u0026 libdevice 参考文献：https://zhuanlan.zhihu.com/p/21358921473 ","date":"2025-08-22","objectID":"/2025/34980c2/:7:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"PTX(Parallel Thread Execution) PTX 是NVIDIA设计的中间表示（IR），类似于GPU的汇编语言，但独立于具体硬件架构。 PTX代码由NVCC生成，可以被NVIDIA的驱动程序进一步编译为特定GPU架构的机器代码（SASS）。 PTX代码是文本格式，便于阅读和调试。 示例： ","date":"2025-08-22","objectID":"/2025/34980c2/:7:1","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"SASS (Streaming ASSembly) SASS 是NVIDIA GPU的机器代码，直接由GPU硬件执行。 SASS是二进制格式，特定于具体的GPU架构（如Ampere、Turing）。 SASS代码通常通过反汇编工具（如cuobjdump）查看。 示例： ","date":"2025-08-22","objectID":"/2025/34980c2/:7:2","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"libdevice libdevice 是NVIDIA提供的一个数学函数库，包含高度优化的设备端数学函数（如sin、exp、log等） 这些函数以PTX或SASS形式提供，可以直接链接到CUDA程序中。 libdevice库的文件通常命名为libdevice.*.bc（LLVM bitcode格式）。 在CUDA程序中，调用标准数学函数（如sin、cos）时，NVCC会自动链接libdevice库。开发者也可以通过 -lcudadevrt显式链接libdevice。 ","date":"2025-08-22","objectID":"/2025/34980c2/:7:3","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"转换流程 以下是CUDA代码从高级语言到最终机器代码的转换流程： CUDA C/C++ → PTX NVCC将CUDA设备代码编译为PTX。 示例命令： nvcc -ptx vector_add.cu -o vector_add.ptx PTX → SASS NVIDIA的驱动程序将PTX代码编译为特定GPU架构的SASS代码。 这一步在运行时或安装时完成。 ","date":"2025-08-22","objectID":"/2025/34980c2/:7:4","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"CUTLASS \u0026 CUTE ","date":"2025-08-22","objectID":"/2025/34980c2/:8:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"CUTLASS CUTLASS（CUDA Template for Linear Algebra Subroutines）是指NVIDIA开源的一个高性能CUDA模板库，用于实现高效的线性代数计算，里面提供了包括访存，计算，流水线编排等多个level的模板。用户通过CUTLASS模板组装以及特化，可以搭建出自己需要的高性能CUDA Kernel。 核心思想是模块化和通用化。它不像传统的库（如 cuBLAS）那样提供预编译好的函数，而是提供了一套模板，让你能够根据自己的需求组合出最优化的计算内核。这使得开发者可以针对特定的矩阵尺寸、数据类型（如 FP16、FP32、INT8 等）和计算模式，生成定制化的、性能极高的内核，而无需从头手写汇编代码。 ","date":"2025-08-22","objectID":"/2025/34980c2/:8:1","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"CUTE CUTE 是一个更底层的、用于描述和操作 GPU 上多维数据布局 (tensor layout) 的 C++ 库。 它不是一个完整的应用库，而是一套强大的、用于构建其他高级库（如 CUTLASS）的工具集。 CUTE 的核心思想是提供一套数学化的、声明式的张量抽象，让开发者可以精确地描述数据在内存中的排布方式。它解决了以下核心问题： 张量布局 (Tensor Layout)： 如何描述一个多维数组（张量）在内存中的存储方式，例如行主序、列主序，或者更复杂的分块和交错布局。 张量视图 (Tensor View)： 如何在不移动数据的情况下，创建张量的子视图，并对子视图进行操作。 跨步操作 (Striding)： 如何高效地计算多维数组中元素的地址。 你可以把 CUTE 理解为一个“张量布局的 DSL (Domain-Specific Language)”。 它提供了一种简洁的方式来表达复杂的数据访问模式，这对于编写 GPU 上的高性能通用代码至关重要。CUTLASS 内部就广泛使用了 CUTE 来管理和操作其张量数据，从而实现了其灵活性和性能。 ","date":"2025-08-22","objectID":"/2025/34980c2/:8:2","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"Global Memory的访存合并 参考文献： https://zhuanlan.zhihu.com/p/675186810 https://zhuanlan.zhihu.com/p/641639133 当前的GPU架构允许通过编译选项来控制是否启用一级缓存。当一级缓存被禁用时，对全局内存的加载请求将直接进入二级缓存；如果二级缓存未命中，将由DRAM完成请求。核函数从全局内存DRAM中读取数据有两种粒度， 使用一级缓存时，每次按照128字节进行缓存；不使用一级缓存时，每次按照32字节进行缓存。 # 禁用一级缓存 -Xptxas -dlcm=cg # 启用一级缓存 -Xptxas -dlcm=ca ","date":"2025-08-22","objectID":"/2025/34980c2/:9:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"传输延迟 在host端和device端之间存在latency，数据通过PCI-E总线从CPU传输给GPU，我们必须避免频繁的host、device间数据传输，即使是最新的PCIE 3.0 x16接口，其双向带宽也只有32GB/s 在device内部也存在latency，即数据从gpu的存储器到multi-processor（SM）的传输。 访问一次全局内存，将耗费400~600个cycle，成本是非常高的，所以必须谨慎对待全局内存的访问 我们把一次内存请求——也就是从内核函数发起请求，到硬件响应返回数据这个过程称为一个内存事务（加载和存储都行）。 ","date":"2025-08-22","objectID":"/2025/34980c2/:9:1","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"合并访存 数据从全局内存到SM（stream-multiprocessor）的传输，会进行cache，如果cache命中了，下一次的访问的耗时将大大减少。 基本逻辑是： 首先判断这个 Kernel 的数据流路径，是否使用了 L1 cache，由此得出当前内存访问的最小粒度： 32 Bytes / 128 Bytes. 分析原始数据存储的结构，结合访存粒度，分析数据访问是否内存对齐，数据是否能合并访问。 对于L1 cache，每次按照128字节进行缓存；对于L2 cache，每次按照32字节进行缓存。 意思是表示线程束中每个线程以一个字节（1*32=32）、16位（2*32=64）、32位（4*32=128）为单位读取数据。前提是，访问必须连续，并且访问的地址是以32字节对齐。 例子，假设每个thread读取一个float变量，那么一个warp（32个thread）将会执行32*4=128字节的合并访存指令，通过一次访存操作完成所有thread的读取请求。 对于L2 cache，合并访存的字节减少为32字节，那么L2 cache相对L1 cache的好处就是： 在非对齐访问、分散访问（非连续访问）的情况下，提高吞吐量（cache的带宽利用率） 对齐/非对齐访问 当一个内存事务的首个访问地址是缓存粒度（32或128字节）的偶数倍的时候：比如二级缓存32字节的偶数倍64，128字节的偶数倍256的时候，这个时候被称为对齐内存访问，非对齐访问就是除上述的其他情况，非对齐的内存访问会造成带宽浪费。 合并/非合并访问 当一个线程束内的线程访问的内存都在一个内存块（缓存粒度）里的时候，就会出现合并访问。 对齐合并访问的状态是理想化的，也是最高速的访问方式，当线程束内的所有线程访问的数据在一个内存块，并且数据是从内存块的首地址开始被需要的，那么对齐合并访问出现了。为了最大化全局内存访问的理想状态，尽量将线程束访问内存组织成对齐合并的方式，这样的效率是最高的。 一个线程束加载数据，使用一级缓存，并且这个事务所请求的所有数据在一个128字节的对齐的地址段上（对齐的地址段是我自己发明的名字，就是首地址是粒度的偶数倍，那么上面这句话的意思是，所有请求的数据在某个首地址是粒度偶数倍的后128个字节里），具体形式如下图，这里请求的数据是连续的，其实可以不连续，但是不要越界就好。 如果一个事务加载的数据分布在不一个对齐的地址段上，就会有以下两种情况： 连续的，但是不在一个对齐的段上，比如，请求访问的数据分布在内存地址 1-128，那么0-127和128-255这两段数据要传递两次到SM 不连续的，也不在一个对齐的段上，比如，请求访问的数据分布在内存地址0-63和128-191上，明显这也需要两次加载。 数据分散开了，thread0的请求在128之前，后面还有请求在256之后，所以需要三个内存事务，而利用率，也就是从主存取回来的数据被使用到的比例，只有 128/(3*128) 的比例。这个比例低会造成带宽的浪费，最极端的表现，就是如果每个线程的请求都在不同的段，也就是一个128字节的事务只有1个字节是有用的，那么利用率只有 1/128. 实例——写一个 内存访问非对齐的kernel __global__ void sumArraysGPU(float *A, float *B, float *C, int offset, int N) { int tid = blockIdx.x * blockDim.x + threadIdx.x; int index = tid + offset; //添加偏移量 从 gld memory 读取数据带地址偏移 这样可以控制内存是否对齐 if(index \u003c N) { C[tid] = A[index] + B[index]; } } 这里用一个 sum kernel 进行举例，float*A, float*, float*C 都是从 glb mem 读取数据，每个 thread 会去load 一个数据进行处理。通过 offset 可以改变 thead load 数据的偏移地址，从而来试验不同 cache 粒度的 合并访问现象。 可以发现禁用 L1 cache 前后，另外 offset = 0 和 offset = 32 得到上述理论结果。 另外，面临非对齐访问，使用 L2 cache能够相对获得较高的 cache 利用率，这也很好理解，因为 L2 cache 的内存块大小更小，即使非对齐，有效地址偏移位数少 ","date":"2025-08-22","objectID":"/2025/34980c2/:9:2","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"常用的profile工具和方法 Nsight System Nsight Compute compute-sanitizer vllm core dump perfetto 分析 torch profile 文件 ","date":"2025-08-22","objectID":"/2025/34980c2/:10:0","tags":null,"title":"GPU及CUDA基本概念","uri":"/2025/34980c2/"},{"categories":["找工作"],"content":"记录秋招找工作的投递 timeline ","date":"2025-08-21","objectID":"/2025/adc4087/:0:0","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"人才计划 ","date":"2025-08-21","objectID":"/2025/adc4087/:1:0","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"❌ 腾讯青云-PCG 投递 2025-07-08一面 2025-07-24二面 2025-08-04","date":"2025-08-21","objectID":"/2025/adc4087/:1:1","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"❌ 快STAR 投递 2025-08-04一面 2025-08-07","date":"2025-08-21","objectID":"/2025/adc4087/:1:2","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"❌ 百度AIDU 投递 2025-07-08","date":"2025-08-21","objectID":"/2025/adc4087/:1:3","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ 科大讯飞-飞星计划 投递 2025-08-20","date":"2025-08-21","objectID":"/2025/adc4087/:1:4","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"提前批 ","date":"2025-08-21","objectID":"/2025/adc4087/:2:0","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"❌ 滴滴提前批 投递 2025-07-16一面（当时人在国外参会，没收到邮件，打电话来调整时间直接推到一周后，过段时间挂） 2025-07-31","date":"2025-08-21","objectID":"/2025/adc4087/:2:1","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ 拼多多提前批 投递 2025-08-03测评 2025-08-13笔试 2025-08-17","date":"2025-08-21","objectID":"/2025/adc4087/:2:2","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ MiniMax提前批 投递 2025-08-05一面 2025-08-28","date":"2025-08-21","objectID":"/2025/adc4087/:2:3","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"正式批 ","date":"2025-08-21","objectID":"/2025/adc4087/:3:0","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"❌ 商汤 投递 2025-08-09","date":"2025-08-21","objectID":"/2025/adc4087/:3:1","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"❌ 米哈游 投递 2025-08-09","date":"2025-08-21","objectID":"/2025/adc4087/:3:2","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"❌ 虾皮 投递 2025-08-21","date":"2025-08-21","objectID":"/2025/adc4087/:3:3","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"❌ 猿辅导 投递 2025-08-20","date":"2025-08-21","objectID":"/2025/adc4087/:3:4","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ 美团 投递 2025-08-13笔试 2025-08-16一面 2025-08-25","date":"2025-08-21","objectID":"/2025/adc4087/:3:5","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ 得物 投递 2025-08-12","date":"2025-08-21","objectID":"/2025/adc4087/:3:6","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ 腾讯 投递 2025-08-19","date":"2025-08-21","objectID":"/2025/adc4087/:3:7","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ 元戎启行 投递 2025-08-20","date":"2025-08-21","objectID":"/2025/adc4087/:3:8","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ 快手 投递 2025-08-23","date":"2025-08-21","objectID":"/2025/adc4087/:3:9","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ B站 投递 2025-08-23","date":"2025-08-21","objectID":"/2025/adc4087/:3:10","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["找工作"],"content":"⏰ 小红书 投递 2025-08-24","date":"2025-08-21","objectID":"/2025/adc4087/:3:11","tags":null,"title":"秋招timeline","uri":"/2025/adc4087/"},{"categories":["推理优化"],"content":"分布式通信原语 在大模型分布式训练或推理时，常用到一些分布式通信原语，这里做一个简单总结 参考文献：https://zhuanlan.zhihu.com/p/717814079 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:0","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"Broadcast(一对多) 在集合通信中，如果某个节点想把自身的数据发送到集群中的其他节点，那么就可以使用广播 Broadcast 的操作。Broadcast 代表广播行为，执行 Broadcast 时，数据从主节点 2 广播至其他各个指定的节点（0~3）。 Broadcast 操作是将某节点的输入广播到其他节点上，分布式机器学习中常用于网络参数的初始化。 如图中，从单个 sender 数据发送到其他节点上，将 2 卡大小为 1xN 的 Tensor 进行广播， 最终每张卡输出均为[1xN]的矩阵。操作过后，所有的进程上数据一致。 应用场景 Broadcast 将一张 GPU 卡上的数据同步到其他所有的 GPU 卡上，其应用场景有： 数据并行的初始化：1）确保每张卡上的初始参数是一致的；2）确保每张卡上模型相同。 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:1","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"Scatter(一对多) Scatter 操作表示一种散播行为，将主节点的数据进行划分并散布至其他指定的节点。Scatter 与 Broadcast 非常相似，都是一对多的通信方式，不同的是 Broadcast 的2号进程将相同的信息发送给所有的进程，而 Scatter 则是将数据的不同部分，按需发送给所有的进程。在分布式场景下，一般是将源进程上数据列表中的N个值按顺序分散到通讯组中的N个进程中。 应用场景 模型并行里初始化时将模型 scatter 到不同的 GPU 上；数据并行，将数据按卡数分发到不同的 GPU 上。 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:2","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"Gather(多对一) Gather 操作将多个 sender 上的数据收集到单个节点上，Gather 可以理解为反向的 Scatter。 Gather 操作会从多个进程里面收集数据到一个进程上面。 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:3","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"Reduce(多对一) Reuduce 称为规约运算，是一系列简单运算操作的统称，细分可以包括：SUM、MIN、 MAX、PROD、LOR 等类型的规约操作。Reduce 从多个 sender 那里接收数据，最终 combine 到一个节点上。相比与all_reduce操作，不同点在于reduce在执行时需要指定目标进程，只有目标进程会收到最终的归约向量。 应用场景 如数据并行时的梯度规约 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:4","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"All Gather(多对多) All Gather 会收集所有进程上的数据，并同步给所有进程。从最基础的角度来看，All Gather 相当于一个 Gather 操作之后跟着一个 Broadcast 操作。 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:5","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"AllReduce(多对多) Reduce 是一系列简单运算操作的统称，All Reduce 则是在所有的进程上都应用同样的 Reduce 操作。All Reduce 操作可通过单节点上 Reduce + Broadcast 操作完成。在 NCCL 中的 All Reduce 中，则是从多个 sender 那里接收数据，最终合并和分发到每一个节点上。 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:6","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"ReduceScatter(多对多) 归约分散操作，该操作行为上可以理解为在一个通信组内依次执行一个reduce操作和一个scatter操作，其示意图如下： 首先，执行一个reduce操作（比如在0卡上），将每张卡上的1、2、3、4收集，那么0卡上得到4、8、12、16。然后执行一个scatter操作，将数据分割到不同卡上，每张卡的数据依次为：4、8、12、16。 在大模型训练过程中，常常会执行all_reduce操作，确保每张卡的parameter相同，为了是实现并行化，一般会将all_reduce操作进行拆分，一般由一个reduce_scatter和一个all_gather组成。如下所示： ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:7","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"AllToAll(多对多) All to All 作为全交换操作，每个进程的接收缓冲区和发送缓冲区都是一个分为若干个数据块的数组。All to All 的具体操作是：将进程 i 的发送缓冲区中的第 j 块数据发送给进程 j，进程 j 将接收到的来 自进程 i 的数据块放在自身接收缓冲区的第 i 块位置。 All to All 与 All Gather 区别：All Gather 操作中，不同进程向某一进程收集到的数据是完全 相同的，而在 All to All 中，不同的进程向某一进程收集到的数据是不同的。在每个进程的发送 缓冲区中，为每个进程都单独准备了一块数据。 全体到全体操作，该操作需要一个通信组中的N个进程各自提供一个含有N个元素的列表，对N个元素分别执行一次all_gather和scatter操作，如下所示： ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:1:8","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"一些常见问题 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:2:0","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"为什么不缓存Q，而要缓存KV？ 参考文献：https://www.zhihu.com/question/653658936 因为分析attention计算可以知道，预测下一个token只用到了当前的query信息，而没有用到之前的query信息，所以不用缓存Q； 在序列的t位置，Q只有当前位置的，参与了计算，而K和V多个位置参与了计算，所以需要KV Cache，而不需要Q Cache。 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:2:1","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"为什么 DeepSeek 使用 FP8 而 Qwen 使用 BF16 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:2:2","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"为什么 CPU 需要三层 Cache 而 GPU 只需要两层； CPU三级缓存介绍：https://blog.csdn.net/weixin_43719763/article/details/128602160 CPU三级缓存主要为了用于不同级别的数据访问，L1/L2是私有的，而L3是共享的，这样可以减少核心之间的数据竞争，降低数据访问延迟。而GPU由于线程数量较多，每个线程对缓存的压力较小，无需为单个线程进行优化缓存层级。而且GPU数据重用集中在相邻线程（如卷积核滑动窗口），L1/L2 缓存+共享内存即可满足。 ","date":"2025-08-19","objectID":"/2025/7ae2a3f/:2:3","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["推理优化"],"content":"分布式并行方法 数据并行（DP）、张量并行（TP）、流水线并行（PP）、上下文并行（CP）、专家并行（EP）以及序列并行（SP）。 为什么分布式并行很重要？ 一个70B参数的FP16模型，仅模型权重就需要约140GB的存储空间，远超任何单张主流GPU的显存容量。此外，训练过程中产生的优化器状态、梯度和中间激活值，更是将内存需求推向了TB级别 训练如此大规模的模型需要巨大的计算资源，单一设备的计算能力根本无法满足需求。通过分布式并行，可以将计算任务分解到多个设备上，实现高效的并行计算。 基础并行策略（DP、TP、PP） DP (Data Parallelism, DP) 核心原理：数据并行是最直观、最常用的并行方式。其核心思想是“模型复制，数据分片”。即将完整的模型副本放置在每一个计算设备（如GPU）上，然后将一个大的训练批次（mini-batch）分割成多个更小的微批次（micro-batch），每个设备独立处理一个微批次的数据。 工作流程： 分发： 将一个全局批次的数据切分，分发给各个GPU。 前向传播： 每个GPU使用其上的模型副本独立计算其微批次数据的前向传播，得到损失。 反向传播： 每个GPU独立计算梯度。梯度同步： 这是DP的关键步骤。所有GPU上的梯度需要通过一个集合通信操作（通常是All-Reduce）进行聚合（如求和或求平均），以确保每个GPU最终都得到全局梯度 。 权重更新： 每个GPU使用同步后的全局梯度独立更新其本地的模型副本，从而保证所有副本在下一步开始前保持一致。 这里涉及到本文的第一个“分布式通信”原语，全归约（All-Reduce），它处理 GPU 实例和节点之间的同步和通信。其实很好理解：就是每一个GPU把数据发送给其余GPU，每一个GPU对于接收到的数据进行求和。 Reduce（归约）：将多个GPU的数据发送到指定的GPU然后求和。 DP方法的优缺点： 优点： 实现简单，易于集成到现有训练框架中 适用于大多数标准模型架构 扩展性较好，可轻松扩展到多GPU集群 缺点： 内存瓶颈显著，无法解决\"模型太大，单卡放不下\"的问题 随着GPU数量增加，All-Reduce操作的通信开销成为主要瓶颈 扩展效率受限于通信带宽和延迟 关键优化：ZeRO (Zero Redundancy Optimizer) 为了克服DP的内存冗余问题，ZeRO[1]技术应运而生。它通过在不同设备间分割和管理模型状态（而不仅仅是数据）来极大地优化内存使用。 ZeRO-DP使得数据并行也能用于训练单卡无法容纳的大模型，是当前大规模训练框架（如DeepSpeed[2]）的核心技术之一。 TP (Tensor Parallelism, TP) 在使用 ZeRO 对模型的参数、梯度和优化器状态进行了分片，但当激活内存超过内存预算时，遇到了瓶颈。张量并行（TP），这是一种分片权重、梯度和优化器状态以及激活的方法——而且无需在计算之前将它们全部收集起来。 张量并行利用了矩阵乘法的数学特性，A×B 。 $$ \\begin{aligned} A\\cdot B \u0026= A\\cdot [B_{1}, B_{2} \\cdots] = [AB_{1}, AB_{2} \\cdots] \\\\\\\\ A\\cdot B \u0026= [A_{1}, A_{2} \\cdots] \\begin{bmatrix} B_{1} \\\\ B_{2} \\\\ \\vdots \\end{bmatrix} = \\sum_{i=1}^{n}A_{i}B_{i} \\end{aligned} $$这意味着可以通过单独计算 B 的每一列或单独计算每一行并将结果组合起来来进行矩阵乘法。示例如下： 在张量并行中，张量会沿着特定维度被分割成 N 个片段，并分布到 N 个 GPU 上。矩阵可以在列或行上进行分割，从而实现行或列并行。 第一个选项是使用按列（也称为按列线性）分片：将完整输入矩阵复制到每个工作节点，需要一种称为广播（broadcast）的操作，并将权重矩阵按列拆分。然后输入与部分权重矩阵相乘，最后使用全聚合（all-gather）操作合并结果。 “分布式通信”原语： 广播（broadcast）：将一个GPU上的数据同时发送到所有其他GPU。 全聚合（all-gather）：每一个GPU把数据发送到其余所有GPU，每个GPU对于接受到的数据拼接为完整数据。 第二种选项称为按行（或行线性）分片：行线性意味着将权重矩阵按行分割成块。因此，需要使用散播（scatter）操作（第四种分布式通信原语！）而不是列线性分片中所使用的广播操作。每个工作节点上的结果已经处于正确的形状，但需要求和以得到最终结果，因此这种场景也需要一个全归约操作： “分布式通信”原语： 散播（scatter）：把一个GPU上的数据切分为多块，按照顺序向其他GPU发送数据块。 核心原理： 张量并行，又称模型内部并行（Intra-layer Model Parallelism），旨在解决单层网络计算量过大或参数过多无法在单个设备上处理的问题。它将模型中的单个张量（如权重矩阵）沿特定维度切分到多个GPU上，并在这些GPU上协同完成运算。 工作流程（以Transformer中的MLP层为例）： 一个标准的MLP层包含两次线性变换。假设我们将模型并行到N个GPU上： 第一个线性层的权重矩阵A可以按列切分成 [A1, A2, …, AN]。输入X与每个Ai相乘得到Xi * Ai。这是一个并行的矩阵乘法，无需通信。 第二个线性层的权重矩阵B可以按行切分成 [B1; B2; …, BN]。在进行第二次矩阵乘法前，需要将第一步的输出结果 [Y1, Y2, …, YN] 通过All-Gather操作在所有GPU间聚合，得到完整的Y。然后每个GPU计算Y * Bi。 最后，将各个GPU上的结果通过Reduce-Scatter或简单的加和操作得到最终输出。 Megatron-LM[3]框架是张量并行的杰出代表，它巧妙地设计了Transformer中注意力层和MLP层的并行计算方式，将通信操作与计算有效结合。 优缺点分析 优点 缺点 （1）直接减少了单个GPU上的内存占用（包括模型参数和激活值）（2）使得训练远超单卡内存限制的模型成为可能（3）计算粒度细，可以更好地利用GPU的并行计算能力 （1）通信开销巨大且频繁，通常仅限于在单个计算节点内部进行高效扩展（2）对设备间的通信带宽要求极高（3）实现复杂度较高，需要仔细设计通信模式 流水线并行 (Pipeline Parallelism, PP) 核心原理：流水线并行，又称层间模型并行（Inter-layer Model Parallelism），将模型的不同层（或层块）分配到不同的GPU上，形成一个计算\"流水线\"。数据在一个设备上完成部分层的计算后，其输出（即激活值）被传递到下一个设备，继续后续层的计算。 工作流程： 朴素流水线： 最简单的方式是将一个批次的数据依次通过所有GPU。但这会导致严重的“流水线气泡”（pipeline bubble），即在任何时刻，只有一个GPU在工作，其他GPU都在等待，利用率极低。 微批次流水线 (Micro-batching)： 为了解决气泡问题，GPipe[4]等框架引入了微批次的概念 。它将一个大的批次（mini-batch）再切分成多个微批次（micro-batches），并将这些微批次依次送入流水线。这样，当第一个微批次在第二个GPU上计算时，第二个微批次就可以在第一个GPU上开始计算，实现了计算的重叠，从而显著减少了GPU的空闲时间。 优缺点分析 优点 缺点 （1）同样能有效降低单个GPU的内存占用（2）通信开销相对较低，因为通信只发生在流水线中相邻的两个阶段之间（3）通信内容主要是激活值，而非整个模型参数（4）实现相对简单，易于调试 （1）仍然存在无法完全消除的\"气泡\"问题，特别是在流水线的启动和排空阶段（2）要求对模型进行合理的切分，以实现各阶段的负载均衡（3）“木桶效应\"会限制整体性能，最慢的阶段决定整体吞吐量（4）流水线深度增加会导致额外的启动和结束延迟 长序列并行策略：序列、上下文并行 随着模型对长文本处理能力的需求日益增长，输入序列长度成为新的内存瓶颈。传统的TP和PP主要解决参数存储问题，而SP和CP则专注于解决因序列过长导致的激活值内存爆炸问题。 序列并行 (Sequence Parallelism, SP) 核心原理： Transformer架构中的自注意力机制（Self-Attention）需要在序列维度上进行全局计算，这使得沿序列维度进行并行化变得困难。序列并行巧妙地绕过了这一点，它选择性地对那些在序列维度上计算独立的模块（如LayerNorm、Dropout和MLP中的逐点操作）的输入激活进行切分。[5] 工作流程： 在进入非并行化的模块（如自注意力）之前，被切分的激活需要通过All-Gather操作在序列维度上拼接回完整的张量。 在完成该模块的计算后，输出的激活可以再次被切分，或者通过Reduce-Scatter操作将计算（如梯度累加）分散到各个设备上。 价值与定位 SP是TP的一个重要补充。TP虽然切分了权重，但每个GPU上仍然需要存储完整的激活张量，这在长序列场景下会成为内存瓶颈。SP通过切分激活，进一步降低了内存占用，使得在有限的显存下能够训练更长的序列或使用更大的批次大小。截至2024年的研究表明：将SP与PP结合（如Mnemosyne 2D并行化[6]）可以在长上下文预填充处理中达到80%以上的扩展效率。 应用场景 长文本处理：对于需要长文档、代码或对话的","date":"2025-08-19","objectID":"/2025/7ae2a3f/:2:4","tags":null,"title":"分布式推理优化技术","uri":"/2025/7ae2a3f/"},{"categories":["算法"],"content":" Kruskal 算法原理及证明见提高课。 ","date":"2025-08-16","objectID":"/2025/acc2381/:0:0","tags":null,"title":"Kruskal算法求最小生成树","uri":"/2025/acc2381/"},{"categories":["算法"],"content":"Kruskal算法求最小生成树 $O(m\\log m)$ 这里重载运算符的方式也可以通过写cmp()来作为sort的第三个参数 bool cmp(Edge a, Edge b) { return a.w \u003c b.w; } ","date":"2025-08-16","objectID":"/2025/acc2381/:1:0","tags":null,"title":"Kruskal算法求最小生成树","uri":"/2025/acc2381/"},{"categories":["算法"],"content":"完整代码 #include \u003ciostream\u003e #include \u003calgorithm\u003e #include \u003ccstring\u003e using namespace std; const int N = 100010, M = 200010, INF = 0x3f3f3f3f; int p[N]; int n, m; struct Edge { int a, b, w; bool operator\u003c (const Edge \u0026W)const { return w \u003c W.w; } }edges[M]; int find(int x) { if (p[x] != x) p[x] = find(p[x]); return p[x]; } int kruskal() { sort(edges, edges + m); for (int i = 1; i \u003c= n; i ++ ) p[i] = i; int res = 0, cnt = 0; for (int i = 0; i \u003c m; i ++ ) { int a = edges[i].a, b = edges[i].b, w = edges[i].w; a = find(a), b = find(b); if (a != b) { p[a] = b; res += w; cnt ++; } } if (cnt \u003c n - 1) return INF; return res; } int main() { cin \u003e\u003e n \u003e\u003e m; for (int i = 0; i \u003c m; i ++ ) { int a, b, w; scanf(\"%d%d%d\", \u0026a, \u0026b, \u0026w); edges[i] = {a, b, w}; } int t = kruskal(); if (t == INF) puts(\"impossible\"); else cout \u003c\u003c t \u003c\u003c endl; return 0; } ","date":"2025-08-16","objectID":"/2025/acc2381/:1:1","tags":null,"title":"Kruskal算法求最小生成树","uri":"/2025/acc2381/"},{"categories":["算法"],"content":" Prim 算法原理及证明见提高课。 Prim算法求最小生成树 $O(n^{2})$，跟Dijkstra很像 ","date":"2025-08-16","objectID":"/2025/1f7d467/:0:0","tags":null,"title":"Prim算法求最小生成树","uri":"/2025/1f7d467/"},{"categories":["算法"],"content":"完整代码 #include \u003ciostream\u003e #include \u003calgorithm\u003e #include \u003ccstring\u003e using namespace std; const int N = 510, INF = 0x3f3f3f3f; int n, m; int g[N][N]; int dist[N]; // 当前点距离集合的距离 bool st[N]; int prim() { memset(dist, 0x3f, sizeof dist); int res = 0; // 最小生成树中所有边长度之和 for (int i = 0; i \u003c n; i ++ ) // 每次找到集合外的，距集合距离最小的点 { int t = -1; // t = -1 表示当前还没还有找到任何一个点 for (int j = 1; j \u003c= n; j ++ ) if (!st[j] \u0026\u0026 (t == -1 || dist[t] \u003e dist[j])) t = j; // 当前图是不连通的，不存在最小生成树 if (i \u0026\u0026 dist[t] == INF) return INF; if (i) res += dist[t]; // dist[t] 表示一条树边，加到生成树中去 for (int j = 1; j \u003c= n; j ++ ) dist[j] = min(dist[j], g[t][j]); st[t] = true; } return res; } int main() { scanf(\"%d%d\", \u0026n, \u0026m); memset(g, 0x3f, sizeof g); while (m -- ) { int a, b, c; scanf(\"%d%d%d\", \u0026a, \u0026b, \u0026c); g[a][b] = g[b][a] = min(g[a][b], c); // 无向图，处理重边 } int t = prim(); if (t == INF) puts(\"impossible\"); else printf(\"%d\\n\", t); return 0; } ","date":"2025-08-16","objectID":"/2025/1f7d467/:0:1","tags":null,"title":"Prim算法求最小生成树","uri":"/2025/1f7d467/"},{"categories":["算法"],"content":"Floyd 算法 ","date":"2025-08-16","objectID":"/2025/6278ffa/:1:0","tags":["draft"],"title":"Floyd求最短路","uri":"/2025/6278ffa/"},{"categories":["算法"],"content":"算法框架 ","date":"2025-08-16","objectID":"/2025/6278ffa/:1:1","tags":["draft"],"title":"Floyd求最短路","uri":"/2025/6278ffa/"},{"categories":["算法"],"content":"适用场景 多源汇最短路，可以有负权边，但是不能有负权回路。 ","date":"2025-08-16","objectID":"/2025/6278ffa/:1:2","tags":["draft"],"title":"Floyd求最短路","uri":"/2025/6278ffa/"},{"categories":["算法"],"content":"算法原理概述 基于动态规划。 闫氏DP分析法： 状态表示 $d(k,i,j)$：从点 $i$ 出发，只经过 $1\\sim k$ 这些中间点到达 $j$ 的最短距离。 状态计算 $d(k,i,j) = d(k-1,i,k) + d(k-1,k,j) \\Rightarrow d(i,j) = d(i,k) + d(k,j)$ ","date":"2025-08-16","objectID":"/2025/6278ffa/:1:3","tags":["draft"],"title":"Floyd求最短路","uri":"/2025/6278ffa/"},{"categories":["算法"],"content":"时间复杂度分析 三重循环，故复杂度为 $O(n^{3})$。 ","date":"2025-08-16","objectID":"/2025/6278ffa/:1:4","tags":["draft"],"title":"Floyd求最短路","uri":"/2025/6278ffa/"},{"categories":["算法"],"content":"完整代码 注：一定要先循环k，i和j的顺序可以任意颠倒。 #include \u003ciostream\u003e #include \u003ccstring\u003e #include \u003calgorithm\u003e using namespace std; const int N = 210, INF = 1e9; int n, m, Q; int d[N][N];// 邻接矩阵，也是floyd算法处理的距离 void floyd() { for (int k = 1; k \u003c= n; k ++ ) for (int i = 1; i \u003c= n; i ++ ) for (int j = 1; j \u003c= n; j ++ ) d[i][j] = min(d[i][j], d[i][k] + d[k][j]); } int main() { scanf(\"%d%d%d\", \u0026n, \u0026m, \u0026Q); for (int i = 1; i \u003c= n; i ++ ) for (int j = 1; j \u003c= n; j ++ ) if (i == j) d[i][j] = 0; // 去掉自环 else d[i][j] = INF; while (m -- ) { int a, b, w; scanf(\"%d%d%d\", \u0026a, \u0026b, \u0026w); d[a][b] = min(d[a][b], w); // 若有重边，则只保留最短的边 } floyd(); while (Q -- ) { int a, b; scanf(\"%d%d\", \u0026a, \u0026b); // 和bellman ford类似，即使终点与起点不连通，也还是可能会被负权邻边更新，所以适当放宽条件 if (d[a][b] \u003e INF / 2) puts(\"impossible\"); else printf(\"%d\\n\", d[a][b]); } return 0; } ","date":"2025-08-16","objectID":"/2025/6278ffa/:1:5","tags":["draft"],"title":"Floyd求最短路","uri":"/2025/6278ffa/"},{"categories":["算法"],"content":"堆优化 Dijkstra ","date":"2025-08-16","objectID":"/2025/652842d/:1:0","tags":null,"title":"Dijkstra求最短路II","uri":"/2025/652842d/"},{"categories":["算法"],"content":"算法框架 \u0026 步骤 堆优化版 Dijkstra 适用于稀疏图，故用邻接表存储图。 ","date":"2025-08-16","objectID":"/2025/652842d/:1:1","tags":null,"title":"Dijkstra求最短路II","uri":"/2025/652842d/"},{"categories":["算法"],"content":"优化思路 1、朴素 Dijkstra 中，最慢的一步是每轮迭代中，寻找所有未更新过其他点的点中，距离起点最近的点，我们可以用小根堆来维护这个所有未更新过其他点的点构成的集合，这样每次查找最小值，就是 $O(1)$ 的时间复杂度，一共迭代 $n$ 次，所以是 $O(n)$。 2、这样用这个最小点去更新其他所有点的距离的时间复杂度，就由 $O(m)$ 变为了 $O(m\\log n)$，因为在堆中修改一个数的时间复杂度是 $O(\\log n)$ 的。故总时间复杂度是 $O(m\\log n)$。 3、另外由于 STL 内的优先队列写法，堆中不支持修改任意元素，修改体现在往堆中添加一个新元素，这样会造成堆中元素冗余，堆中元素可能是 $m$ 个，这样时间复杂度就会退化至 $O(m\\log m)$，但由于一般 $m \\leq n^{2}$，故 $\\log m \\leq \\log(n^{2}) = \\log m \\leq 2\\log n$，时间复杂度接近，所以一般不用手写堆，直接用 STL 内的优先队列即可。 另外，Dijkstra 可以算是 BFS 的升级版，就是说如果求最短路径，当图从无权值变成有权值时，BFS 不再适用了，于是我们用 Dijkstra 方法。换句话说，对于无权值图，Dijkstra 方法跟 BFS 是一致的。你可以画个无权图，用 Dijkstra 走一遍，发现其实这就是 BFS。 此处邻接表中不需要特殊考虑重边，因为算法保证了一定能够选择最短的边。 ","date":"2025-08-16","objectID":"/2025/652842d/:1:2","tags":null,"title":"Dijkstra求最短路II","uri":"/2025/652842d/"},{"categories":["算法"],"content":"完整代码 C++ #include \u003ciostream\u003e #include \u003ccstring\u003e #include \u003calgorithm\u003e #include \u003cqueue\u003e using namespace std; typedef pair\u003cint, int\u003e PII; const int N = 200010; // 可能存在重边，需要开大一点 int n, m; int e[N], h[N], w[N], ne[N], idx; // w[i]表示当前这个结点所连的下一条边权 int dist[N]; // 当前点到初始点的最短距离 bool st[N]; // 标记当前点的最短距离是否已经确定 void add(int a, int b, int c) { e[idx] = b, w[idx] = c, ne[idx] = h[a], h[a] = idx++; } int dijkstra() { memset(dist, 0x3f, sizeof dist); dist[1] = 0; priority_queue\u003cPII, vector\u003cPII\u003e, greater\u003cPII\u003e\u003e heap; // 小根堆的写法，不用过多深究，记住即可 heap.push({0, 1}); while (heap.size()) { auto t = heap.top(); heap.pop(); int ver = t.second, distance = t.first; if (st[ver]) continue; // st[ver]为真表示当前这个点是堆中备份点，已经被处理过 st[ver] = true; // 遍历t的所有出边 for (int i = h[ver]; i != -1; i = ne[i]) { int j = e[i]; if (dist[j] \u003e distance + w[i]) // 依旧是用t更新其他所有直连点距离 { dist[j] = distance + w[i]; heap.push({dist[j], j}); } } } if (dist[n] == 0x3f3f3f3f) return -1; return dist[n]; } int main() { scanf(\"%d%d\", \u0026n, \u0026m); memset(h, -1, sizeof h); while (m -- ) { int a, b, c; scanf(\"%d%d%d\", \u0026a, \u0026b, \u0026c); add(a, b, c); // 堆优化版Dijkstra不用特殊处理重边和自环，因为算法本身会选择最短边 } int t = dijkstra(); printf(\"%d\\n\", t); return 0; } Python3 def add(a, b, c): global idx e[idx], w[idx], ne[idx] = b, c, h[a] h[a] = idx idx += 1 def dijkstra(): from queue import PriorityQueue q = PriorityQueue() q.put((0, 1)) dist = [float('inf')] * (n + 10) dist[1] = 0 while q.qsize() \u003e 0: t = q.get() # .get() 相当于 .front(), .pop() ver, distance = t[1], t[0] if st[ver]: continue st[ver] = True i = h[ver] while i != -1: j = e[i] if dist[j] \u003e distance + w[i]: dist[j] = distance + w[i] q.put((dist[j], j)) i = ne[i] if dist[n] == float('inf'): return -1 return dist[n] if __name__ == '__main__': N, M = int(2e5 + 10), int(2e5 + 10) n, m = map(int, input().split()) h, w, e, ne = [-1] * N, [0] * M, [0] * M, [0] * M idx = 0 st = [False] * (n + 10) for _ in range(m): a, b, c = map(int, input().split()) add(a, b, c) print(\"%d\" % dijkstra()) ","date":"2025-08-16","objectID":"/2025/652842d/:1:3","tags":null,"title":"Dijkstra求最短路II","uri":"/2025/652842d/"},{"categories":["算法"],"content":"朴素 Dijkstra 算法 ","date":"2025-08-16","objectID":"/2025/c2f4d7e/:1:0","tags":null,"title":"Dijkstra求最短路I","uri":"/2025/c2f4d7e/"},{"categories":["算法"],"content":"算法框架 ","date":"2025-08-16","objectID":"/2025/c2f4d7e/:1:1","tags":null,"title":"Dijkstra求最短路I","uri":"/2025/c2f4d7e/"},{"categories":["算法"],"content":"算法步骤 适用于稠密图，故用邻接矩阵存储图 st[]存储所有当前已经更新过其他点的点。 1、遍历 $n$ 次，找到当前不在st集合中的，距离起点最近的点，赋给t 2、用t去更新其他所有能够直连的点的距离：dist[x] \u003e dist[t] + w若真，就更新dist[x]。 ","date":"2025-08-16","objectID":"/2025/c2f4d7e/:1:2","tags":null,"title":"Dijkstra求最短路I","uri":"/2025/c2f4d7e/"},{"categories":["算法"],"content":"证明 基于贪心，无需掌握，过程略。 提示 每一轮迭代，在还没有更新过其他点的所有点中，找到距离起点最近的点t，然后用t去更新其他所有点到起点的距离。 int t = -1; // 为了方便找这样一个距离起点最近的点，先让 t = -1，以便加入起点循环查找 for (int j = 1; j \u003c= n; j ++ ) if (!st[j] \u0026\u0026 (t == -1 || dist[t] \u003e dist[j])) t = j; ","date":"2025-08-16","objectID":"/2025/c2f4d7e/:1:3","tags":null,"title":"Dijkstra求最短路I","uri":"/2025/c2f4d7e/"},{"categories":["算法"],"content":"完整代码 #include \u003ciostream\u003e #include \u003ccstring\u003e #include \u003calgorithm\u003e using namespace std; const int N = 510; int n, m; int g[N][N]; int dist[N]; // 当前点到初始点的最短距离 bool st[N]; // st数组更确切的含义是某个点是否已经更新过其他点，而不是它的最短距离是否已经确定 int dijkstra() { memset(dist, 0x3f, sizeof dist); dist[1] = 0; for (int i = 0; i \u003c n; i ++ ) { int t = -1; for (int j = 1; j \u003c= n; j ++ ) if (!st[j] \u0026\u0026 (t == -1 || dist[t] \u003e dist[j])) t = j; st[t] = true; // 实际上，这里也只用t去更新了其他与t相邻的点的距离，只更新了相邻节点，因为不是相邻的话 // g[t][j] 为 INF，相当于没有更新。 for (int j = 1; j \u003c= n; j ++ ) dist[j] = min(dist[j], dist[t] + g[t][j]); // 用t去更新其他点的距离 } if (dist[n] == 0x3f3f3f3f) return -1; return dist[n]; } int main() { cin \u003e\u003e n \u003e\u003e m; memset(g, 0x3f, sizeof g); while (m -- ) { int a, b, c; scanf(\"%d%d%d\", \u0026a, \u0026b, \u0026c); g[a][b] = min(g[a][b], c); // 处理自环和重边 } int t = dijkstra(); cout \u003c\u003c t \u003c\u003c endl; return 0; } Python3 N, M = 510, int(1e5 + 10) dist = [0 for _ in range(N)] st = [False for _ in range(N)] g = [[0] * N for _ in range(N)] def dijkstra(): dist = [int(1e9) for _ in range(N)] dist[1] = 0 for i in range(n): t = -1 for j in range(1, n + 1): if not st[j] and (t == -1 or dist[t] \u003e dist[j]): t = j st[t] = True for j in range(1, n + 1): dist[j] = min(dist[j], dist[t] + g[t][j]) if dist[n] == int(1e9): return -1 return dist[n] if __name__ == '__main__': n, m = map(int, input().split()) g = [[int(1e9)] * N for _ in range(N)] while m: m -= 1 a, b, c = map(int, input().split()) g[a][b] = min(g[a][b], c) print(dijkstra()) ","date":"2025-08-16","objectID":"/2025/c2f4d7e/:1:4","tags":null,"title":"Dijkstra求最短路I","uri":"/2025/c2f4d7e/"},{"categories":["算法"],"content":"第一种写法，按行枚举 #include \u003ciostream\u003e using namespace std; const int N = 20; char g[N][N]; bool col[N], dg[N], udg[N]; int n; void dfs(int u) { if(u == n) { for(int i = 0; i \u003c n; i ++ ) puts(g[i]); puts(\"\"); return; } for(int i = 0; i \u003c n; i ++ ) { if(!col[i] \u0026\u0026 !dg[u + i] \u0026\u0026 !udg[n - u + i]) { g[u][i] = 'Q'; col[i] = dg[u + i] = udg[n - u + i] = true; dfs(u + 1); col[i] = dg[u + i] = udg[n - u + i] = false; g[u][i]= '.'; } } } int main() { cin \u003e\u003e n; for(int i = 0; i \u003c n; i ++ ) for(int j = 0; j \u003c n; j ++ ) g[i][j] = '.'; dfs(0); return 0; } ","date":"2025-08-16","objectID":"/2025/f5a0f88/:0:1","tags":null,"title":"N皇后问题","uri":"/2025/f5a0f88/"},{"categories":["算法"],"content":"第二种写法，一格一格枚举 #include \u003ciostream\u003e using namespace std; const int N = 20; bool row[N], col[N], dg[N], udg[N]; char g[N][N]; int n; void dfs(int x, int y, int s) { if(y == n) y = 0, x ++; if(x == n) { if(s == n) { for(int i = 0; i \u003c n; i ++) puts(g[i]); puts(\"\"); } return; } dfs(x, y + 1, s); if(!row[x] \u0026\u0026 !col[y] \u0026\u0026 !dg[x + y] \u0026\u0026 !udg[x - y + n]) { g[x][y] = 'Q'; row[x] = col[y] = dg[x + y] = udg[x - y + n] = true; dfs(x, y + 1, s + 1); row[x] = col[y] = dg[x + y] = udg[x - y + n] = false; g[x][y] = '.'; } } int main() { cin \u003e\u003e n; for(int i = 0; i \u003c n; i ++ ) for(int j = 0; j \u003c n; j ++ ) g[i][j] = '.'; dfs(0, 0, 0); return 0; } ","date":"2025-08-16","objectID":"/2025/f5a0f88/:0:2","tags":null,"title":"N皇后问题","uri":"/2025/f5a0f88/"},{"categories":["算法"],"content":"求逆元 ","date":"2025-08-16","objectID":"/2025/c322c46/:1:0","tags":null,"title":"快速幂求逆元","uri":"/2025/c322c46/"},{"categories":["算法"],"content":"前置知识 1、设 $m$ 为正整数，若 $a$ 和 $b$ 是整数，且 $m\\mid (a - b)$ ，则称 $a$ 和 $b$ 模 $m$ 同余，记为 $a\\equiv b\\pmod m$。 2、若 $a$ 和 $b$ 是整数，则 $a\\equiv b\\pmod m$ 当且仅当存在整数 $k$，使得 $a = b + km$。 3、设 $m$ 为正整数，模 $m$ 的同余满足以下性质： 自反性：若 $a$ 是整数，则 $a\\equiv a\\pmod m$ 对称性：若 $a$ 和 $b$ 是整数，且 $a\\equiv b\\pmod m$，则 $b\\equiv a\\pmod m$ 传递性：若 $a, b$ 和 $c$ 是整数，且 $a\\equiv b\\pmod m$ 和 $b\\equiv c\\pmod m$，则 $a\\equiv c\\pmod m$ 4、若 $a, b, c$ 和 $m$ 是整数，$m \u003e 0$，且 $a\\equiv b\\pmod m$，则有： $$ \\begin{align} a + c \u0026\\equiv b + c\\pmod m \\\\\\ a - c \u0026\\equiv b - c\\pmod m \\\\\\ ac \u0026\\equiv bc\\pmod m \\end{align} $$5、乘法逆元：若整数 $b$ 与 $m$ 互质，并且 $b\\mid a$，则存在一个整数 $x$，使得 $\\dfrac{a}{b} \\equiv a\\cdot x\\pmod m$，则称 $x$ 为 $b$ 的模 $m$ 乘法逆元，记为 $b^{-1}\\pmod m$ $\\Rightarrow$ $\\dfrac{a}{b} \\equiv a\\cdot b^{-1}\\pmod m \\Rightarrow bb^{-1}\\equiv 1\\pmod m$，其中 $b^{-1}$ 是 $b$ 的逆元。 6、费马小定理：如果 $p$ 是一个质数，并且 $a$ 与 $p$ 互质，则有 $a^{p-1}\\equiv 1\\pmod p$ 7、快速幂 8、扩展欧几里得算法 9、一个质数和比它小的每一个非零自然数都互质 ","date":"2025-08-16","objectID":"/2025/c322c46/:1:1","tags":null,"title":"快速幂求逆元","uri":"/2025/c322c46/"},{"categories":["算法"],"content":"适用场景 一般在做除法时，有余数存在是一件很麻烦的事，两个整数相除不一定是整数，两个整数相乘一定整数，因此我们希望将除法转化成乘法（模上一个数余数相同）。 ","date":"2025-08-16","objectID":"/2025/c322c46/:1:2","tags":null,"title":"快速幂求逆元","uri":"/2025/c322c46/"},{"categories":["算法"],"content":"算法分析 根据前置知识中的内容，当模数 $p$ 为质数时： 1、快速幂求逆元： 若 $b$ 与 $p$ 互质，根据费马小定理，有： $$b^{p-1}\\equiv 1\\pmod p \\iff b\\cdot b^{p - 2}\\equiv 1\\pmod p$$此时 $b^{p - 2}$ 为 $b$ 的逆元，这时即可用快速幂来求解逆元。 若 $b$ 与 $p$ 不互质，则 $b$ 的乘法逆元不存在。 2、扩展欧几里得求逆元： 上面已经证明了，对于一个整数 $a$ 来说，若存在乘法逆元 $x = a^{-1}$，则有 $a\\cdot a^{-1}\\equiv 1\\pmod p$，即存在整数 $y$ 使得 $ax = 1 + yp$，整理得 $ax + py = 1$。 而存在乘法逆元 $x = a^{-1}$ 的充要条件为 $a$ 与模数 $p$ 互质，即 $\\gcd(a, p) = 1$，故原式转化为： $$ ax + py = 1 = \\gcd(a, p)$$符合裴蜀定理的表达式，因此我们可以用扩展欧几里得算法求解逆元 $x = a^{-1}$。 ","date":"2025-08-16","objectID":"/2025/c322c46/:1:3","tags":null,"title":"快速幂求逆元","uri":"/2025/c322c46/"},{"categories":["算法"],"content":"完整代码 快速幂求逆元 #include \u003ccstdio\u003e using namespace std; typedef long long LL; int qmi(int a, int k, int p) { int res = 1; while (k) { if (k \u0026 1) res = (LL)res * a % p; k \u003e\u003e= 1; a = (LL)a * a % p; } return res; } int main() { int n; scanf(\"%d\", \u0026n); while (n -- ) { int a, p; scanf(\"%d%d\", \u0026a, \u0026p); if (a % p) printf(\"%d\\n\", qmi(a, p - 2, p)); else puts(\"impossible\"); } return 0; } 扩展欧几里得求逆元 #include \u003ccstdio\u003e using namespace std; typedef long long LL; int n; int exgcd(int a, int b, int \u0026x, int \u0026y) { if (!b) { x = 1, y = 0; return a; } int d = exgcd(b, a % b, y, x); y -= a / b * x; return d; } int main() { scanf(\"%d\", \u0026n); while (n -- ) { int a, p, x, y; scanf(\"%d%d\", \u0026a, \u0026p); int d = exgcd(a, p, x, y); if (d == 1) printf(\"%d\\n\", ((LL)x + p) % p); // 保证 x 是正数 else puts(\"impossible\"); } return 0; } ","date":"2025-08-16","objectID":"/2025/c322c46/:1:4","tags":null,"title":"快速幂求逆元","uri":"/2025/c322c46/"},{"categories":["算法"],"content":"快速幂 ","date":"2025-08-16","objectID":"/2025/302d3a9/:1:0","tags":null,"title":"快速幂","uri":"/2025/302d3a9/"},{"categories":["算法"],"content":"适用场景 快速求出 $a^k \\bmod p$ 的值。 ","date":"2025-08-16","objectID":"/2025/302d3a9/:1:1","tags":null,"title":"快速幂","uri":"/2025/302d3a9/"},{"categories":["算法"],"content":"算法原理 \u0026 步骤分析 快速幂原理是基于二进制枚举的思想。 对于整数 $a$ 的任意幂数 $k$，$k$ 都可以用二进制进行拆分，并且只会由 $\\left [2^0, 2^{\\lfloor \\log k \\rfloor} \\right]$ 中的部分数组合而成。 例如 $k_1 = 4 = (100)_2, k_2 = 7 = (111)_2$，均为三位二进制数，只由 $2^0、2^1、2^2$ 中的部分数构成。这样原式就可转化为： $$a^{k_1} = a^{4} = a^{0\\times 2^{0} + 0\\times 2^{1} + 1\\times 2^{2}}$$$$a^{k_2} = a^{7} = a^{1\\times 2^{0} + 1\\times 2^{1} + 1\\times 2^{2}} = a^{2^{0}}\\cdot a^{2^{1}}\\cdot a^{2^{2}}$$因此，对于给定的幂数 $k$，我们可以先预处理出 $\\left[ a^{2^{0}},\\ a^{2^{\\lfloor \\log k \\rfloor}} \\right]$ 中的所有数。时间复杂度为 $O(\\log k)$ 然后依次枚举 $k$ 的二进制表示的每一位，每次枚举时 $a$ 都需要自我累乘一次： a = (LL)a * a % p；如果该位为 $1$，就将当前累乘的 $a$ 计入答案：res = (LL)res * a % p，如果该位为 $0$，就不予计入，枚举下一位。 注：这里 $a, k, p$ 满足 $1\\leq a, k, p\\leq 2\\times 10^9$，所以在自我累乘和答案计入时可能会溢出整数范围，需要开 long long。 枚举完 $k$ 的二进制表示数的所有位后，累乘的 res 就是答案。 完整代码 #include \u003ciostream\u003e using namespace std; const int N = 1e5 + 10; typedef long long LL; int quick_mi(int a, int k, int p) { int res = 1; while (k) { if (k \u0026 1) res = (LL)res * a % p; k \u003e\u003e= 1; a = (LL)a * a % p; } return res; } int main() { int n; cin \u003e\u003e n; while (n -- ) { int a, k, p; scanf(\"%d%d%d\", \u0026a, \u0026k, \u0026p); printf(\"%d\\n\", quick_mi(a, k, p)); } return 0; } ","date":"2025-08-16","objectID":"/2025/302d3a9/:1:2","tags":null,"title":"快速幂","uri":"/2025/302d3a9/"},{"categories":["算法"],"content":"并查集 ","date":"2025-08-16","objectID":"/2025/55e4c93/:1:0","tags":null,"title":"并查集","uri":"/2025/55e4c93/"},{"categories":["算法"],"content":"适用场景 1、将两个集合合并 2、询问两个元素是否在一个集合当中 3、维护一个集合中的点的个数：siz[root]，见 AcWing 837. 连通块中点的数量 4、维护一个集合中各个点到根节点的距离：d[]，见： AcWing 240. 食物链 核心操作：路径压缩和按秩合并（按秩合并不常用，这里省略） 上述两个操作最坏情况下为 $O(\\log n)$ int find(int x) { if (p[x] != x) p[x] = find(p[x]); return p[x]; } ","date":"2025-08-16","objectID":"/2025/55e4c93/:1:1","tags":null,"title":"并查集","uri":"/2025/55e4c93/"},{"categories":["算法"],"content":"算法原理 \u0026 实现 每个集合用一棵树来表示，数根的编号就是整个集合的编号，每个节点存储它的父节点，p[x]表示x的父节点。 问题1：如何判断树根？ if (p[x] == x) 问题2：如何求x的集合编号？ while (p[x] != x) x = p[x]; 问题3：如何合并两个集合？ p[x] 是 x 的集合编号，p[y]是 y的集合编号。 p[x] = y 初始化 初始时，每个点都是一个集合，故每个点的树根就是它本身。 for (int i = 1; i \u003c= m; i ++ ) p[i] = i; ","date":"2025-08-16","objectID":"/2025/55e4c93/:1:2","tags":null,"title":"并查集","uri":"/2025/55e4c93/"},{"categories":["算法"],"content":"完整代码 #include \u003ciostream\u003e using namespace std; const int N = 1e5 + 10; int p[N], n, m; int find(int x) { if(p[x] != x) p[x] = find(p[x]); return p[x]; } int main() { scanf(\"%d%d\", \u0026n, \u0026m); for (int i = 1; i \u003c=m ; i ++ ) p[i] = i; while (m -- ) { char op[2]; int a, b; scanf(\"%s%d%d\", op, \u0026a, \u0026b); if (op[0] == 'M') p[find(a)] = find(b); else { if (find(a) == find(b)) puts(\"Yes\"); else puts(\"No\"); } } return 0; } ","date":"2025-08-16","objectID":"/2025/55e4c93/:1:3","tags":null,"title":"并查集","uri":"/2025/55e4c93/"},{"categories":["算法"],"content":"算法思路 稳定 $O(n\\log n)$ ","date":"2025-08-16","objectID":"/2025/f751a18/:0:1","tags":null,"title":"归并排序","uri":"/2025/f751a18/"},{"categories":["算法"],"content":"完整代码 C++ #include \u003ciostream\u003e using namespace std; const int N = 1e5 + 10; int q[N], tmp[N]; void merge_sort(int q[], int l, int r) { if (l \u003e= r) return; int mid = l + r \u003e\u003e 1; merge_sort(q, l, mid), merge_sort(q, mid + 1, r); int k = 0, i = l, j = mid + 1; while (i \u003c= mid \u0026\u0026 j \u003c= r) if (q[i] \u003c= q[j]) tmp[k ++ ] = q[i ++ ]; else tmp[k ++ ] = q[j ++ ]; while (i \u003c= mid) tmp[k ++ ] = q[i ++ ]; while (j \u003c= r) tmp[k ++ ] = q[j ++ ]; for (i = l, j = 0; i \u003c= r; i ++ , j ++ ) q[i] = tmp[j]; } int main() { int n; cin \u003e\u003e n; for (int i = 0; i \u003c n; i ++ ) scanf(\"%d\", \u0026q[i]); merge_sort(q, 0, n - 1); for (int i = 0; i \u003c n; i ++ ) printf(\"%d \", q[i]); return 0; } Python3 def merge_sort(q, l, r): if l \u003e= r: return mid = l + r \u003e\u003e 1 merge_sort(q, l, mid) merge_sort(q, mid + 1, r) i, j, k = l, mid + 1, 0 while i \u003c= mid and j \u003c= r: if q[i] \u003c q[j]: tmp[k] = q[i] i += 1 else: tmp[k] = q[j] j += 1 k += 1 while i \u003c= mid: tmp[k] = q[i] i += 1 k += 1 while j \u003c= r: tmp[k] = q[j] j += 1 k += 1 j = 0 for i in range(l, r + 1): q[i] = tmp[j] j += 1 if __name__ == \"__main__\": n = int(input()) q = list(map(int, input().split())) tmp = [0] * n merge_sort(q, 0, n - 1) for i in range(len(q)): print(q[i], end = ' ') ","date":"2025-08-16","objectID":"/2025/f751a18/:0:2","tags":null,"title":"归并排序","uri":"/2025/f751a18/"},{"categories":["推理优化"],"content":"safe-softmax 推导 原始的 softmax 公式： $$ softmax(x_i) = \\dfrac{exp(x_i)}{exp(x_0) + exp(x_1) + ... + exp(x_n)}$$为了防止数值溢出，超过一定范围精度下降，需要减去 $x$ 中最大值： $$ safe-softmax(x_i) = \\dfrac{exp(x_i - \\max_x)}{exp(x_0 - \\max_x) + exp(x_1 - \\max_x) + ... + exp(x_n - \\max_x)}$$该式与原始版本完全相同，因为： $$ \\begin{aligned} safe-softmax(x_i) \u0026= \\dfrac{exp(x_i - \\max_x)}{exp(x_0 - \\max_x) + exp(x_1 - \\max_x) + ... + exp(x_n - \\max_x)} \\\\\\\\ \u0026= \\dfrac{exp(x_i) / exp(\\max_x)}{exp(x_0) / exp(\\max_x) + exp(x_1) / exp(\\max_x) + ... + exp(x_n) / exp(\\max_x)} \\\\\\\\ \u0026= \\dfrac{exp(x_i)}{exp(x_0) + exp(x_1) + ... + exp(x_n)} \\\\\\\\ \u0026= softmax(x_i) \\end{aligned} $$","date":"2025-08-15","objectID":"/2025/39b641b/:1:0","tags":null,"title":"softmax与Flash-attention","uri":"/2025/39b641b/"},{"categories":["推理优化"],"content":"online-softmax 推导 核心：动态更新最大值，以及指数和 $m_j = \\max(m_{j-1}, x_j)$【最大值的更新】 $d_j = e^{x_1 - m_j} + e^{x_2 - m_j} + ... + e^{x_j - m_j}$【指数和的更新】 以上，$m_j$ 为 前 j 项的最大值，$d_j$ 则代表前 j 个元素的指数和，之所以减去 $m_j$ 是因为 safe-softmax 的缘故 第一项比较好理解，第二项需要我们拆开分析： 将 $d_j = e^{x_1 - m_j} + e^{x_2 - m_j} + ... + e^{x_j - m_j}$ 的贡献拆分为： 前 j-1 项贡献： $$e^{x_1 - m_j} + e^{x_2 - m_j} + ... + e^{x_{j-1} - m_j}$$第j项的贡献： $$ e^{x_j - m_j} $$其中对于前j-1项的贡献，这部分指数和本来应该是基于 $m_{j-1}$ 来计算的： $$ d_{j-1} = e^{x_1 - m_{j-1}} + e^{x_2 - m_{j-1}} + ... + e^{x_{j-1} - m_{j-1}} $$但是这跟上面前j-1项的贡献表示不同，所以我们要将 $d_{j-1}$ 转换为以 $m_j$ 为基准： $$ \\begin{aligned} e^{x_1 - m_j} + e^{x_2 - m_j} + ... + e^{x_{j-1} - m_j} \u0026= e^{x_1 - m_{j-1} + m_{j-1} - m_j} + e^{x_2 - m_{j-1} + m_{j-1} - m_j} + ... + e^{x_{j-1} - m_{j-1} + m_{j-1} - m_j} \\\\\\\\ \u0026=(e^{x_1 - m_{j-1}} + e^{x_2 - m_{j-1}} + ... + e^{x_{j-1} - m_{j-1}}) * e^{m_{j-1} - m_j} \\\\\\\\ \u0026= d_{j-1} * e^{m_{j-1} - m_j} \\end{aligned} $$所以： $$ d_j = d_{j-1} * e^{m_{j-1} - m_j} + e^{x_j - m_j} $$","date":"2025-08-15","objectID":"/2025/39b641b/:2:0","tags":null,"title":"softmax与Flash-attention","uri":"/2025/39b641b/"},{"categories":["推理优化"],"content":"FlashAttention-v1 ","date":"2025-08-15","objectID":"/2025/39b641b/:3:0","tags":null,"title":"softmax与Flash-attention","uri":"/2025/39b641b/"},{"categories":["推理优化"],"content":"背景动机 参考文献：https://zhuanlan.zhihu.com/p/669926191 FlashAttention主要解决Transformer计算速度慢和存储占用高的问题。但与绝大多数Efficient Transformer把改进方法集中在降低模型的FLOPS（floating point operations per second）不同，FlashAttention将优化重点放在了降低存储访问开销（Memory Access Cost，MAC）上。 Transformer 复杂度可以理解为 $O(dN^2)$，这是因为 Self-Attention 的计算占据了 Transformer 的主要部分，而 Self-Attention 的复杂度为 $O(dN^2)$，主要为 $S = QK^T$ 和 $O = PV$ 的计算。 正因为Transformer的复杂度随序列长度的增长呈二次方增长，所以通常基于Transformer的大语言模型的上下文长度都不会特别长（如早期的大模型普遍在2k、4k左右）。 为了减少对HBM的读写，FlashAttention将参与计算的矩阵进行分块送进SRAM，来提高整体读写速度（减少了HBM读写）。 ","date":"2025-08-15","objectID":"/2025/39b641b/:3:1","tags":null,"title":"softmax与Flash-attention","uri":"/2025/39b641b/"},{"categories":["推理优化"],"content":"核心思想 前置知识：softmax计算的逐步更新（指数和逐步更新；最大值逐步更新） 分块累加见推导：From Online Softmax to FlashAttention j=0，遍历 i: j=1，遍历i: 分块计算输出O： 这里 $O$ 的更新可以看Ye推导公式，更新方式： 总体来看，不需要将 $S = QK^T$ 写入 HBM 再取回做 $Softmax$ 得到 $P$，然后再取回 $P$ 与 $V$ 做 $PV$ 得到结果。 内循环中，只需要计算分块后的 $S = QK^T$ 然后写入 SRAM，然后利用 $S$ 更新局部最大值 $m$ 和 局部指数和 $d$，然后与 $V$ 相乘后得到 $O$（必须乘以 $V$ 后这样才具有局部累加性） ","date":"2025-08-15","objectID":"/2025/39b641b/:3:2","tags":null,"title":"softmax与Flash-attention","uri":"/2025/39b641b/"},{"categories":["推理优化"],"content":"MLA 参考图解视频 ","date":"2025-08-14","objectID":"/2025/d01413e/:1:0","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"朴素实现 ","date":"2025-08-14","objectID":"/2025/d01413e/:1:1","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"矩阵吸收 上面图解版本不包括旋转位置编码，但我认为用来理解MLA基本原理还是非常好的 ","date":"2025-08-14","objectID":"/2025/d01413e/:1:2","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"朴素实现（包含rope） 参考文献：https://zhuanlan.zhihu.com/p/1901704483446187870 $t$ 代表 t-th token 首先输入 $h_t$ 经过 $W_{DQ}$ 得到 $c_t^Q$ 再分别经过 $W^{QR}$, $W^{UQ}$ 得到用于 rope计算的 $q_t^R$ 和 nope 的 $q_t^C$： $$ \\begin{aligned} h_{t}W_{DQ} \u0026= c_{t}^{Q} \\\\ c_{t}^{Q}W^{QR} \u0026= q_{t}^{R} \\\\ c_{t}^{Q}W^{UQ} \u0026= q_{t}^{C} \\\\ \\end{aligned} $$其次来看 KV 部分，输入 $h_t$ 经过 $W_{DKV}$ 得到 $c_t^{KV}$，得到联合低秩压缩的 KV，$c_t^{KV}$ 与历史的 c cache拼接后，得到完整的 $c^{KV}$， 再分别经过 $W_{UK}$，$W_{UV}$ 进行升维，得到 $k^C$，$v^C$： $$ \\begin{aligned} XW_{DKV} \u0026= c_t^{KV} \\\\ c^{KV}W_{UK} \u0026= k^C\\\\ c^{KV}W_{UV} \u0026= v^C \\\\ \\end{aligned} $$再回到输入部分，现在来计算 qk 的rope部分，输入 $h_t$ 经过 $W_{KR}$ 得到 $k_t^R$，同样与历史的 $k_pe$ cache拼接后，得到完整的 $k^R$，然后与 $q_t^R$ 进行 rope计算： $$ \\begin{aligned} h_tW^{KR} \u0026= k_t^R \\\\ q_t^R(k^R)^T \u0026= \\text{attn}^R \\end{aligned} $$得到 $\\text{attn}^R$ 后，我们再回到之前的计算过程，我们之前计算得到了 $q_t^C$ 和完整的 $k^C$，因此可以计算出 attn 的nope部分： $$ \\begin{aligned} q_t^C(k^C)^T \u0026= \\text{attn}^C \\end{aligned} $$故： $$ Softmax(\\dfrac{QK^T}{\\sqrt{d}}) = Softmax(\\dfrac{\\text{attn}^C + \\text{attn}^R}{\\sqrt{d}}) $$然后再与v进行相乘得到最终的注意力输出： $$ \\begin{aligned} PV \u0026= Softmax(\\dfrac{\\text{attn}^C + \\text{attn}^R}{\\sqrt{d}})v^C \\\\ O \u0026= Softmax(\\dfrac{\\text{attn}^C + \\text{attn}^R}{\\sqrt{d}})v^C W_o \\end{aligned} $$","date":"2025-08-14","objectID":"/2025/d01413e/:1:3","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"矩阵吸收（包含rope） 主要是将 $c_{t}^{Q}W^{UQ} = q_{t}^{C}$ 和 $c^{KV}W_{UK} = k^C$ 这两步中的 $W^{UQ}$ 和 $W^{UK}$ 吸收到了 $W^{UQK}$，使得 $\\text{attn}^C$ 的计算可以直接由 $q_t^C(c^{KV})^T = \\text{attn}^C$ 得到，而不需要先对低秩联合压缩的 $c^{KV}$ 先做升维得到 $k^C$，即解压操作。 另外是将 $c^{KV}W^{UV} = v^C$ 和 $O = Softmax(\\dfrac{\\text{attn}^C + \\text{attn}^R}{\\sqrt{d}})v^C W_o$ 这两步中的 $W^{UV}$ 和 $W_o$ 吸收到了 $W^{UVO}$，使得注意力输出的 $O$ 的计算可以直接由 $Softmax(\\dfrac{\\text{attn}^C + \\text{attn}^R}{\\sqrt{d}})W^{UVO}$ 得到，而不需要先对低秩联合压缩的 $c^{KV}$ 先做升维得到 $v^C$，即解压操作。 总结： 这样做可以直接在潜在空间（即压缩后的维度）中进行注意力计算，而不需要先解压 KV-Cache。并且能够减少计算量：通过将两个矩阵相乘的操作合并成一个，减少了所需的计算步骤，从而提高了推理速度。简单来说，矩阵吸收就像是把一个两步走的任务（先解压，再计算）变成了一步走的任务（直接用“吸收”后的矩阵进行计算），从而实现了内存和计算效率的双重优化。 ","date":"2025-08-14","objectID":"/2025/d01413e/:1:4","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"MoE class Expert(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super().__init__() self.net = nn.Sequential( nn.Linear(input_dim, hidden_dim), nn.GELU(), nn.Linear(hidden_dim, output_dim)) def forward(self, x): return self.net(x) class MoE(nn.Module): def __init__(self, input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim): super().__init__() self.num_experts = num_experts self.top_k = top_k self.expert_capacity = expert_capacity # 路由网络 self.gate = nn.Linear(input_dim, num_experts) # 专家集合 self.experts = nn.ModuleList( [Expert(input_dim, hidden_dim, output_dim) for _ in range(num_experts)]) def forward(self, x): batch_size, input_dim = x.shape device = x.device # 路由计算 logits = self.gate(x) probs = torch.softmax(logits, dim=-1) print(\"probs: \", probs) topk_probs, topk_indices = torch.topk(probs, self.top_k, dim=-1) print(\"topk_probs: \", topk_probs) print(\"topk_indices: \", topk_indices) # 辅助损失计算 if self.training: # 重要性损失（专家利用率均衡） importance = probs.sum(0) importance_loss = torch.var(importance) / (self.num_experts ** 2) # 负载均衡损失（样本分配均衡） mask = torch.zeros_like(probs, dtype=torch.bool) mask.scatter_(1, topk_indices, True) routing_probs = probs * mask expert_usage = mask.float().mean(0) routing_weights = routing_probs.mean(0) load_balance_loss = self.num_experts * (expert_usage * routing_weights).sum() aux_loss = importance_loss + load_balance_loss else: aux_loss = 0.0 # 专家分配逻辑 flat_indices = topk_indices.view(-1) flat_probs = topk_probs.view(-1) sample_indices = torch.arange(batch_size, device=device)[:, None]\\ .expand(-1, self.top_k).flatten() print(\"sample_indices: \", sample_indices) # 初始化输出 outputs = torch.zeros(batch_size, self.experts[0].net[-1].out_features, device=device) # 处理每个专家 for expert_idx in range(self.num_experts): print(\"expert_idx: \", expert_idx) # 获取分配给当前专家的样本 expert_mask = flat_indices == expert_idx print(\"expert_mask: \", expert_mask) expert_samples = sample_indices[expert_mask] print(\"expert_samples: \", expert_samples) expert_weights = flat_probs[expert_mask] print(\"expert_weights: \", expert_weights) # 容量控制 if len(expert_samples) \u003e self.expert_capacity: expert_samples = expert_samples[:self.expert_capacity] expert_weights = expert_weights[:self.expert_capacity] if len(expert_samples) == 0: continue # 处理专家计算 expert_input = x[expert_samples] print(\"expert_input: \", expert_input) expert_output = self.experts[expert_idx](expert_input) weighted_output = expert_output * expert_weights.unsqueeze(-1) # 累加输出 outputs.index_add_(0, expert_samples, weighted_output) return outputs, aux_loss # 测试示例 if __name__ == \"__main__\": input_dim = 5 output_dim = 10 num_experts = 8 top_k = 3 expert_capacity = 32 hidden_dim = 512 batch_size = 10 # add device = torch.device(\"npu:4\" if torch.npu.is_available() else \"cpu\") moe = MoE(input_dim, num_experts, top_k, expert_capacity, hidden_dim, output_dim).to(device) x = torch.randn(batch_size, input_dim).to(device) moe.eval() output, _ = moe(x) print(f\"Eval output shape: {output.shape}\") # torch.Size([64, 256]) gate 就是一个线性层，形状为 (hidden_state, n_experts) 输入 x (num_tokens, hidden_state) 经过 gate 得到 router_logits (num_tokens, n_experts) 然后会经过 topk 来将每个token对应的topk个激活专家选出来，这里可以用python代码简单介绍这一过程： logits = self.gate(x) probs = torch.softmax(logits, dim=-1) 假设这里的 num_tokens=10, num_experts=8，故 probs 是一个10行8列的矩阵 probs: tensor([[0.1710, 0.1348, 0.0746, 0.1714, 0.0594, 0.2695, 0.0251, 0.0940], [0.1556, 0.0776, 0.1658, 0.1489, 0.1152, 0.1679, 0.0565, 0.1124], [0.1077, 0.1154, 0.1564, 0.1317, 0.0630, 0.2026, 0.0518, 0.1715], [0.0681, 0.0680, 0.1236, 0.1030, 0.1707, 0.2827, 0.0627, 0.1211], [0.0453, 0.0648, 0.2313, 0.0781, 0.1026, 0.1304, 0.1326, 0.2149], [0.1394, 0.2278, 0.0625, 0.1832, 0.0395, 0.1512, 0.0691, 0.1274], [0.1096, 0.1462, 0.1302, 0.1397, 0.0607, 0.1898, 0.0639, 0.1598], [0.1200, 0.1952, 0.0970, 0.1648, 0.0360, 0.1072, 0.1018, 0.1779], [0.0650, 0.0501, 0.1463, 0.1025, 0.2219, 0.1446, 0.1439, 0.1257], [0.0641, 0.0813, 0.0579, 0.1348, 0.1170, 0.0631, 0.3554","date":"2025-08-14","objectID":"/2025/d01413e/:2:0","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"MTP ","date":"2025-08-14","objectID":"/2025/d01413e/:3:0","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"DeepSeek开源周 ","date":"2025-08-14","objectID":"/2025/d01413e/:4:0","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"FlashMLA ","date":"2025-08-14","objectID":"/2025/d01413e/:4:1","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"DeepEP ","date":"2025-08-14","objectID":"/2025/d01413e/:4:2","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"DeepGEMM ","date":"2025-08-14","objectID":"/2025/d01413e/:4:3","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"EPLB 背景动机 专家并行时，如何决定将那个专家放到哪张卡上。 考虑DeepSeek的EP，总共256个路由专家，1个共享专家. prefill时,EP32, 256/32 = 8，每张卡放8个路由专家， 共享专家在所有卡上都复制一份，单卡总共9个专家。 decode时，EP144, 每张卡只放2个路由专家和一个共享专家，总共有 144 * 2-256 = 32个来放冗余路由专家。 需要解决的问题： 怎么决定对哪些专家进行冗余？ 冗余多少份？ 对于任意一个专家，应该放在哪张卡上？ EPLB 就是在解决上述问题。 逻辑专家：指模型中的256路由专家 + 1共享专家 物理专家：指经过冗余后实际部署到GPU上的专家, 数量大于 256 + 1 DeepSeek 推理系统概览 Prefill：路由专家 EP32、MLA 和共享专家 DP32，一个部署单元是 4 节点，32 个冗余路由专家，每张卡 9 个路由专家和 1 个共享专家 Decode：路由专家 EP144、MLA 和共享专家 DP144，一个部署单元是 18 节点，32 个冗余路由专家，每张卡 2 个路由专家和 1 个共享专家 官网例子： import torch import eplb # 这里的weight是记录每一个专家历史工作负载，来评估每个专家的“热门”程度 weight = torch.tensor([[ 90, 132, 40, 61, 104, 165, 39, 4, 73, 56, 183, 86], [ 20, 107, 104, 64, 19, 197, 187, 157, 172, 86, 16, 27]]) num_replicas = 16 # 实际可以放置的总物理专家数量 num_groups = 4 # 对总卡数进行分组 num_nodes = 2 # 节点总数 num_gpus = 8 # 卡总数 phy2log, log2phy, logcnt = eplb.rebalance_experts(weight, num_replicas, num_groups, num_nodes, num_gpus) print(phy2log) # 最后输出负载均衡后的推荐放置方案 # Output: # tensor([[ 5, 6, 5, 7, 8, 4, 3, 4, 10, 9, 10, 2, 0, 1, 11, 1], # [ 7, 10, 6, 8, 6, 11, 8, 9, 2, 4, 5, 1, 5, 0, 3, 1]]) 该示例展示了一个两层的 MoE 模型，每层包含 12 个专家。每层引入 4 个冗余专家，总共 16 个副本被放置在 2 个节点上，每个节点包含4个 GPU。输出结果展示了专家复制和放置的计划。 EPLB核心函数 def balanced_packing(weight: torch.Tensor, num_packs: int) -\u003e Tuple[torch.Tensor, torch.Tensor]: \"\"\" Pack n weighted objects to m packs, such that each bin contains exactly n/m objects and the weights of all packs are as balanced as possible. Parameters: weight: [X, n], the weight of each item num_packs: number of packs Returns: pack_index: [X, n], the pack index of each item rank_in_pack: [X, n], the rank of the item in the pack \"\"\" num_layers, num_groups = weight.shape assert num_groups % num_packs == 0 groups_per_pack = num_groups // num_packs if groups_per_pack == 1: pack_index = torch.arange(weight.size(-1), dtype=torch.int64, device=weight.device).expand(weight.shape) rank_in_pack = torch.zeros_like(weight, dtype=torch.int64) return pack_index, rank_in_pack indices = weight.float().sort(-1, descending=True).indices.cpu() pack_index = torch.full_like(weight, fill_value=-1, dtype=torch.int64, device='cpu') rank_in_pack = torch.full_like(pack_index, fill_value=-1) for i in range(num_layers): pack_weights = [0] * num_packs pack_items = [0] * num_packs for group in indices[i]: pack = min((i for i in range(num_packs) if pack_items[i] \u003c groups_per_pack), key=pack_weights.__getitem__) assert pack_items[pack] \u003c groups_per_pack pack_index[i, group] = pack rank_in_pack[i, group] = pack_items[pack] pack_weights[pack] += weight[i, group] pack_items[pack] += 1 return pack_index, rank_in_pack def replicate_experts(weight: torch.Tensor, num_phy: int) -\u003e Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: \"\"\" Replicate `num_log` experts to `num_phy` replicas, such that the maximum load of all replicas is minimized. Parameters: weight: [X, num_log] num_phy: total number of experts after replication Returns: phy2log: [X, num_phy], logical expert id of each physical expert rank: [X, num_phy], the replica rank logcnt: [X, num_log], number of replicas for each logical expert \"\"\" n, num_log = weight.shape num_redundant = num_phy - num_log assert num_redundant \u003e= 0 device = weight.device phy2log = torch.arange(num_phy, dtype=torch.int64, device=device).repeat(n, 1) rank = torch.zeros(n, num_phy, dtype=torch.int64, device=device) logcnt = torch.ones(n, num_log, dtype=torch.int64, device=device) arangen = torch.arange(n, dtype=torch.int64, device=device) for i in range(num_log, num_phy): redundant_indices = (weight / logcnt).max(dim=-1).indices phy2log[:, i] = redundant_indices rank[:, i] = logcnt[arangen, redundant_indices] logcnt[arangen, redundant_indices] += 1 return phy2log, rank, logcnt 核心API # 分层均衡 def rebalance_experts_hierarchical(weight: torch.Tensor, num_physical_experts: int, num_groups: int, num_nodes: int, num_gpus: int): \"\"\" Parameters: weight: [num_moe_layers, num_logical_experts] num_physical_experts: number of physical experts after replication num_groups: number of expert ","date":"2025-08-14","objectID":"/2025/d01413e/:4:4","tags":null,"title":"DeepSeek相关优化技术","uri":"/2025/d01413e/"},{"categories":["推理优化"],"content":"基本概念 参考链接：https://zhuanlan.zhihu.com/p/1912965417447686461 缩放系数：$s= \\dfrac{r_{max} - r_{min}}{q_{max} - q_{min}} = \\dfrac{\\max(|r_{min}|, |r_{max}|)}{2^{N-1}}$ 零点：$z=round(q_{min}-\\dfrac{r_{min}}{s})=round(-2^{N-1}-\\dfrac{r_{min}}{s})$ 这里 $r_{min}$ 指的是输入值的最大和最小值。$q_{min}, q_{max}$ 是指量化后的整数范围。 quantized: x_float = (x_int8 - zero_point) * scale dequantized: x_int8 = round(x_float/scale) + zero_point 这里有个前置知识就是，在神经网络中一般权重的形状 都是 $W\\in R^{O\\times I}$，其中 $O$ 表示输出通道(output channel), $I$ 表示输入通道(input channel)，对于矩阵乘法，即 $y = Wx$, $x\\in R^{I}, y\\in R^{O}$ ","date":"2025-08-07","objectID":"/2025/a793bc7/:1:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"量化粒度 - 三种形式 per-tensor 这是最粗粒度的量化方式。对整个 tensor 计算出一个统一的缩放因子（scale factor），然后将张量中的所有元素都使用这个缩放因子量化到低精度格式，例如 INT8。例如，一个 INT8 的 per-tensor 动态量化器会找到整个张量的最大绝对值，以此计算出一个缩放因子，然后将所有元素缩放到 INT8 的表示范围内 [-127, +127] 并进行取整1 。 per-token 这种量化方式的粒度比 per-tensor 更细。对于张量的每一个 token（通常指的是矩阵的每一行），都单独计算并应用一个缩放因子。 per-block 这种量化方式的粒度比 per-token 更细。它将张量在 token 维度上划分为若干个块（block）。对于每一个块内的所有 token（行），计算出一个统一的缩放因子并应用，。 per-channel 这种量化方式和 per-token 类似，但是量化维度不一样，对于张量的每一个通道（通常指 hidden dim 维度的一列），都单独计算并应用一个缩放因子。 ","date":"2025-08-07","objectID":"/2025/a793bc7/:2:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"离群值 (Outliers) 离群值的存在是模型量化时精度下降的主要原因。这是因为量化将模型中的连续数值映射到有限的离散数值范围（例如 INT4 的范围是 [-7, +7]）。如果数据中存在少数数值远超其他数据的离群值，为了表示这些极端的数值，量化的步长（resolution）就需要增大。 这样做的直接后果是： 大多数正常的、幅度较小的数值在量化后会变得非常接近甚至等于零。例如，如果一个数值比组内的最大值小很多倍，它可能会被量化为零，导致大量信息的丢失。 有限的量化比特无法精确表示这些大部分的正常数值，从而降低了整体的量化精度。 为了解决这个问题，需要采用平滑（smoothing）技术来减小激活或权重中离群值的影响，使得数值的幅度分布更加均匀。量化方法通过观察任务的 outliner 特点，来针对性地设计量化方法。 比较有代表性的是SmoothQuant ，它观察到在LLM的推理过程中，激活值（activations）中往往比权重值（weights）更容易出现显著的离群值。SmoothQuant 通过一种数学上等价的Per-channel缩放（channel-wise scaling）操作，将模型量化的难度从激活转移到权重。具体来说，它降低了激活中异常大的数值，使得激活值更容易被量化到低比特（例如 INT8），从而在保持模型精度的前提下，实现更高效的量化推理。 ","date":"2025-08-07","objectID":"/2025/a793bc7/:3:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"对称 / 非对称量化区别 项目 对称量化 非对称量化 是否有 zero point ❌ 固定为 0 ✅ 存在 zero_point 适用范围 权重量化（特别是 centered） 激活量化（值范围变化大） 运算简化 ✅ 快速，适合矩阵乘法 ❌ 多了减法，加快复杂度 表现力 ❌ 表达负偏移有限 ✅ 支持偏移，精度更高 效果稳定性 ✅ 稳定 ✅ 更灵活，适应动态变化 ","date":"2025-08-07","objectID":"/2025/a793bc7/:4:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"量化对象 参考链接：https://zhuanlan.zhihu.com/p/1895945361824122028 ","date":"2025-08-07","objectID":"/2025/a793bc7/:5:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"Linear(Dense) 量化 指对模型的 Linear 层进行量化，Linear 层主要分布于： Attention 中的 Q/K/V/O projection MLP（FFN) 中的 gate/up/down Embedding ([vocab_size, hidden_size]) 和 LM Head ([hidden_size, vocab_size]) MoE 中的 expert ","date":"2025-08-07","objectID":"/2025/a793bc7/:5:1","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"Attention量化，以 SageAttention 为例 参考文献：https://zhuanlan.zhihu.com/p/28866643551 首先 SageAttention 是基于 FlashAttention2并采用动态量化。 SageAttention 基于 FlashAttention 的分块方法，对分块应用per-block的方式进行量化。具体来说，在Q，K，P，V的分块上进行 INT8 量化，然后对乘积进行反量化，主要是为了加速 QK^T 和 PV 的矩阵乘法计算。online softmax 保持全精度。 per-block 量化 “块”（block）是指将输入张量按序列维度划分的连续数据段。每个块包含固定数量的 token，对整个块使用同一个量化缩放因子。用一个具体的例子说明： 假设我们有一个 Query 张量，形状为 [1, 8, 512, 128]（batch=1, heads=8, seq_len=512, head_dim=128），使用 BLKQ=128 的块大小： 序列长度 512 被划分为 512 ÷ 128 = 4 个块 第1块：token 0-127 第2块：token 128-255 第3块：token 256-383 第4块：token 384-511 对于上述例子，Q 的缩放因子张量形状为 [1, 8, 4]，即每个 head 的每个块都有一个缩放因子。 Smooth-K K表现出明显的 Channel 方向异常值，但是由于矩阵乘法 $QK^T$ 中，量化只能在token维度上进行，因此无法对K应用per-channel量化。 某个Channel方向异常值，应用per-channel量化，可以把异常值影响范围缩小到该方向上的S和Z 但是K的通道异常值表现出一个规律，即每个token的key实际上是所有tokens共享的一个大bias，再加上一个小的token-wise的信号。 即在最终量化之前，先从全精度 K 中减去平均值。 Quantization for Q, K, P, V Q, K 的量化粒度：Q和K的量化粒度可以设置为 per-token，per-block或per-tensor粒度。但是不可以设置为 per-channel，原因上面说了，内轴在相乘时会约掉，没办法进行反量化 Q, K的数据类型：之所以对Q和K进行 INT8 量化，原因有两个：其一是因为测试了很多模型，对Q，K，P，V使用INT8 量化比 FP8量化具有更高的准确率；其二是因为在许多常用的GPU上进行INT8矩阵乘法比使用FP8快两倍 P, V的量化粒度：对P进行per-block量化，对V进行per-channel量化，原因有三个：其一是因为对P进行per-channel量化和对V进行per-token量化不可行，因为反量化需要外部轴的scale factor；其二是P的每一行最大值为1，因此可以为一个块分配一个固定的scale factor $s=\\frac{1}{127}$；其三是per-channel量化可以解决V的通道方向异常值问题 FP16 累加 对 P和V进行量化时，在某些模型层使用INT8的准确率会非常低。 论文中建议在矩阵乘法PV中使用FP16作为数据类型，并且使用FP16累加器 使用 FP16 累加器的 FP16 矩阵乘法速度比使用 FP32 累加器快 2 倍。此外，使用 FP16 累加器比使用 FP32 累加器可以节省更多的寄存器资源，从而加快计算速度。其次，表 3 表明，对 P, V 使用 FP16 比使用所有其他 8 位数据类型要精确得多。而且，使用 FP16 累加器与使用 FP32 累加器相比不会导致精度损失。 关于内部轴不可进行反量化的分析 矩阵乘法中，对于每个矩阵你只能沿着公共维度进行量化（下图右边）。根据这个简单的原则，Attention 中四个矩阵可以量化的组合如下。注意能做 per-token，就能做 per-block 量化。其中 P 代表 $softmax(QK^T/\\sqrt{d})$ 这是因为在进行矩阵乘法 $QK^{T}$ 后，得到的结果矩阵的维度是 N × N（Q 和 K 的维度都是 N × d）。如果我们对 K 进行了per-channel 量化（下图左边，总共 d 个channel，每个 channel 包含 N 个元素），每个通道都有一个独立的scale factor，总共是 d 个 scale factor。在反量化（dequantization）时，我们需要将量化后的结果乘以对应的scale factor，而QK^T 的结果矩阵的维度是 NxN，根本没有 d 的通道维度不直接对应，因此无法使用 K 的通道维度的缩放因子进行正确的反量化。 Q K P V per-channel ❌ ❌ ❌ ✅ per-token ✅ ✅ ✅ ❌ per-block ✅ ✅ ✅ ❌ ","date":"2025-08-07","objectID":"/2025/a793bc7/:5:2","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"量化类型 INT 量化范式： FP32作为基准，提供了最大的数值范围和零精度损失，但存储开销最大。 如果用户不太关心效率，那么INT16格式是最佳选择。INT16格式是最精确的，如果是转换FP32，INT16甚至比FP16更精确。 对于对实时性要求高的服务，建议采用INT8量化方案，可以在保持较高精度的同时获得显著的性能提升。如果你的网络中某些层需要更高的精度，可以使用W8A16来解决这个问题。 在资源受限但对精度要求相对较低的场景，则可以采用INT4方案以获得最大的资源效益。 ","date":"2025-08-07","objectID":"/2025/a793bc7/:6:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"weight-only GPTQ $\\text{Frobenius}$ 范数，简称 F-范数，$||\\cdot||^{2}_{2}$ 表示对矩阵的 Frobenius 范数的平方（即所有元素平方和） Frobenius 范数可以用来衡量矩阵整体的大小，比如在误差分析中，可以用来评估两个矩阵之间的差异程度 GPTQ是一种训练后权重量化方法，使用基于二阶信息的逐层量化，成功将每个权重量化至 3-4 位，几乎没有精度损失。GPTQ 对某个 block 内的所有参数逐个量化，每个参数量化后，需要适当调整这个 block 内其他未量化的参数，以弥补量化造成的精度损失。 GPTQ 量化需要准备校准数据集。 OBS/OBQ/GPTQ等一系列工作的核心就是： 不直接最小化权重误差，而是：$\\min_{q(w)}||(q(w)-w)X||^2_{2}$ 给定权重矩阵W，有以下步骤： 收集激活样本（校准数据集） 计算输入协方差矩阵 $H = X^{T}X$ 逐列量化 W 的每一列 每列权重找最优 int4 表示 误差反馈，用 Hessian 更新下列的残差 保存量化结果，然后在推理阶段使用 int4 ✖️ float16 / int8的高效矩阵乘法 AWQ 参考：https://www.zhihu.com/search?type=content\u0026q=AWQ%20%E9%87%8F%E5%8C%96 AWQ观察到权重通道对性能的重要性各不相同，通过保留1%的显著权重可以大大减少量化误差。基于此观察，AWQ采用了激活感知权重量化来量化LLM，具体会专注于激活值较大的权重通道，并通过每通道缩放实现最佳量化效果。 误区：AWQ量化=W4A16量化 AWQ是一种对模型权重进行低比特量化的方法，使用该方法可以将模型权重(Weight)量化为4bit，并在计算激活值(Activation)时反量化为FP16，即W4A16。也可以基于AWQ方法将权重量化为3bit/8bit，并在计算时是使用4bit/8bit/16bit，由此衍生出W4A4、W4A8等一系列方法。 作者在原文中指出，W4A16可以在精度损失较小的情况下，大幅降低内存占用，且提升模型推理速度，是最常用的方法，因此AWQ和W4A16同镜率较高。 显著权重（权重并不同等重要，仅有部分显著权重对结果影响较大） 权重矩阵中显著权重位于哪个通道，找到这个通道，将这个通道内的部分保留原来的精度(fp16)，然后其他部分量化为低 bit。 步骤：在计算时，首先将激活值对每一列求绝对值的平均值，然后把平均值较大的一列对应的通道视作显著通道，保留FP16精度。对其他通道进行低比特量化，如下图： 这里详细解释一下“将激活值对每一列求绝对值的平均值，然后把平均值较大的一列对应的通道视作显著通道” 对于 $y=xW^{T}$，其中 $y\\in R^{B\\times O}, W\\in R^{O\\times I}, x\\in R^{B\\times I}$ （这里 chatgpt 的解释是，在数学上 $y = Wx$，实现中记为 $y=xW^{T}$） 这张图中的 $X\\in R^{I\\times B}, W\\in R^{O\\times I}$，因此 $y=Wx \\in R^{O\\times B}$ 所以不管数学上的表达还是实际上的代码实现，y 的输出通道始终由权重的输出通道决定，即 y 的输出通道是否是显著值，就看对应的激活值那一列，所以这里是求的每一列的绝对值的平均值，把平均值较大的一列视为显著通道。 但是如果这样做，权重矩阵中有的元素需要用 FP16，而其他元素需要用 INT8，不好写 kernel。因此就引入了 Scaling 方法 Scaling（量化时对显著权重进行放大可以降低量化误差） 量化误差主要来源于对权重的量化，AWQ的目标是通过缩放显著权重，减少量化误差 核心思想：对显著权重按比例放大，然后在计算时相应地缩小输入，这样在量化过程中显著权重的相对误差被降低。 量化函数（这里量化函数不是表示量化后的整数值，而是指 反量化之后的近似权重值，是直接给出最终用于推理的值）： 其中 $N$ 是量化后的比特数，$\\Delta$ 是量化因子(scaler)，$\\Delta= \\dfrac{max(|w|)}{2^{N-1}}$ $w'=Round(\\frac{w}{\\Delta})$ 是量化过程， $\\Delta\\cdot w'$ 是反量化过程 $w, \\Delta, x$ 都是 fp16 格式，不会带来精度损失，精度损失全部来源于 round 函数 对于权重 $\\text{w}$ 中的单个元素 $w$，引入一个缩放因子 $s\u003e1$，量化过程将 $w$ 与该因子相乘，写作：$\\text{w}'=Round(\\dfrac{ws}{\\Delta'})$，相应将反量化过程写作 $\\dfrac{\\Delta' \\cdot w'}{s}$，对 x 进行逆缩放，则： 其原始量化误差为： RoundErr：四舍五入误差，为±0.5 $\\Delta$：量化比例因子，决定误差绝对值大小 缩放后的量化误差： 所以误差比值可以描述为 $\\dfrac{\\Delta'}{\\Delta}\\cdot \\dfrac{1}{s}$，我们认为 $\\Delta'\\approx \\Delta$，加上 $s\u003e1$，所以作者认为量化时对显著权重进行放大，可以降低量化误差 从量化函数来看，AWQ 属于对称量化。这里量化因子 q_scale: $\\Delta' = \\dfrac{max(|w|)}{2^{N-1}}$ 自动计算缩放系数 按照上文的分析，我们需要找到权重矩阵每个通道的缩放系数，使得量化误差最小，即最小化公式4： 按照作者的观点，激活值越大，对应通道越显著，就应该分配更大的缩放系数降低其量化误差。这里为了简单记忆，作者统计了各通道的平均激活值（计算输入矩阵各列绝对值的平均值），并直接将此作为各通道的缩放系数。 SmoothQuant 是一个 W8A8 算法，本质还是跟 AWQ 有点像，主要是将 激活量化 的难度转移到权重上，简单来说就是 除以一个值，然后权重乘以一个值： $$ Y=(X*\\text{diag}(s)^{-1})*(\\text{diag}(s)*W) $$也就是对activate (也就是X)进行缩放，并把相反的缩放系数应用到对应的weight(也就是W)上，得到数学上等价的结果。它的核心观察在于： 由于activation outlier的存在，activation的分布非常不规则； weight分布均匀 这样，通过上面的操作，试图把 activation 变得更均匀，而把 weight 的均匀分布变得没有那么均匀，也就是把activation 量化de 难度部分平摊到weight 上 并且该论文主要实现了三种量化方法：per-tensor、per-token、per-channel。同时也进行了static 和 dynamic量化的区分 ","date":"2025-08-07","objectID":"/2025/a793bc7/:6:1","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"weight-activation LLM.int8() 发现激活中的异常值集中在一小部分通道中。基于这一点，LLM.int8() 根据输入通道内的离群值分布将激活和权重分成两个不同的部分。包含激活值和权重的异常数据的通道以FP16格式存储，其他通道则以INT8格式存储。 ","date":"2025-08-07","objectID":"/2025/a793bc7/:6:2","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"KV cache 待补充 ","date":"2025-08-07","objectID":"/2025/a793bc7/:6:3","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"动态量化与静态量化 dynamic：在线运行的时候统计缩放系数 动态离线量化仅将模型中特定算子的权重从FP32类型映射成 INT8/16 类型，bias和激活函数 在推理过程中动态量化。但是对于不同的输入值来说，其缩放因子是动态计算的（“动态”的由来）。动态量化是几种量化方法中性能最差的。 static：离线使用标定数据计算好缩放系数 静态离线量化使用少量无标签校准数据，采用 KL 散度等方法计算量化比例因子。静态量化（Static quantization）与动态量化的区别在于其输入的缩放因子计算方法不同，静态量化的模型在使用前有“calibrate”的过程（校准缩放因子）：准备部分输入（对于图像分类模型就是准备一些图片，其他任务类似），使用静态量化后的模型进行预测，在此过程中量化模型的缩放因子会根据输入数据的分布进行调整。一旦校准完成后，权重和输入的缩放因子都固定（“静态”的由来）。静态量化的性能一般比动态量化好，常用于中等模型和大模型。因此实际中基本都是在用静态量化。 网址静态离线量化的目标是求取量化比例因子，主要通过对称量化、非对称量化方式来求，而找最大值或者阈值的方法又有MinMax、KLD、ADMM、EQ等方法 ","date":"2025-08-07","objectID":"/2025/a793bc7/:7:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"支撑量化的一些算子和库 ","date":"2025-08-07","objectID":"/2025/a793bc7/:8:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"Marlin 支持混合精度运算，例如 FP16 * INT4 运算，FP8 * INT4运算 一种支持 W4A16的 GEMM kernel（一定程度上kernel 实现和量化算法是独立的），因此 marlin kernel 也支持 AWQ 量化模型执行。原始的Marlin Kernel只支持W4A16计算模式，而 QQQ 在 Marlin kernel 的基础上，支持了 W4A8 的计算模式。 ","date":"2025-08-07","objectID":"/2025/a793bc7/:8:1","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["推理优化"],"content":"MoE量化 以实习期间做过的 MoE wna16marlin kernel 为例（sgl PR 7683） MoE 专用路由: 支持动态专家选择和 token 路由 多量化格式: 支持 INT4/INT8/FP8 等多种量化类型 Tensor Core 优化: 使用 CUDA tensor core 指令加速矩阵运算 内存布局优化: 针对 MoE 访问模式优化的内存布局 原子操作: 支持原子加法减少全局归约开销 ","date":"2025-08-07","objectID":"/2025/a793bc7/:9:0","tags":null,"title":"量化","uri":"/2025/a793bc7/"},{"categories":["找工作"],"content":"记录一些常用的 CUDA 算子写法 ","date":"2025-08-07","objectID":"/2025/ebaa040/:0:0","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"Reduce ","date":"2025-08-07","objectID":"/2025/ebaa040/:1:0","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"Reduce Sum template \u003cconst int kWarpSize = 256\u003e __device__ __forceinline__ float warp_reduce_sum_f32(float val) { #pragma unroll for (int mask = kWarpSize; mask \u003e= 1; mask \u003e\u003e= 1) { val += __shfl_xor_sync(0xffffffff, val, mask); } return val; } ","date":"2025-08-07","objectID":"/2025/ebaa040/:1:1","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"Block reduce sum template \u003cconst int NUM_THREADS = 256\u003e __device__ float block_reduce_sum_f32(float val) { int tid = threadIdx.x; int idx = blockIdx.x * blockDim.x + tid; constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE; static __shared__ reduce_sum[NUM_WARPS]; float sum = warp_reduce_sum_f32\u003cWARP_SIZE\u003e(val); if (lane == 0) reduce_sum[warp] = sum; __syncthreads(); sum = (lane \u003c NUM_WARPS) ? reduce_sum[lane] : 0.0f; sum = warp_reduce_sum_f32\u003cNUM_WARPS\u003e(sum); sum = __shfl_sync(0xffffffff, sum, 0, 32); return sum; } ","date":"2025-08-07","objectID":"/2025/ebaa040/:1:2","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"Reduce Max template \u003cconst int kWarpSize = 256\u003e __device__ __forceinline__ float warp_reduce_max_f32(float val) { #pragma unroll for (int mask = kWarpSize; mask \u003e= 1; mask \u003e\u003e= 1) { val = fmaxf(val, __shfl_xor_sync(0xffffffff, val, mask)); } return val; } ","date":"2025-08-07","objectID":"/2025/ebaa040/:1:3","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"block reduce max // grid 1D block 1D, grid(N/256), block(256) template \u003cconst int NUM_THREADS = 256\u003e __device__ float block_reduce_max_f32(float val) { constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE; int warp = threadIdx.x / WARP_SIZE; int lane = threadIdx.x % WARP_SIZE; static __shared__ float reduce_max[NUM_WARPS]; float value = warp_reduce_sum_f32\u003cWARP_SIZE\u003e(val); if (lane == 0) { shared[warp] = value; } __syncthreads(); value = (lane \u003c NUM_WARPS) ? shared[lane] : -FLT_MAX; value = warp_reduce_max_f32\u003cNUM_WARPS\u003e(value); value = __shfl_sync(0xffffffff, value, 0, 32); return value; } ","date":"2025-08-07","objectID":"/2025/ebaa040/:1:4","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"Softmax ","date":"2025-08-07","objectID":"/2025/ebaa040/:2:0","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"naive版 template \u003cconst int NUM_THREADS = 256\u003e __global__ void softmax_f32_per_token_kernel(float *x, float *y, int N) { const int tid = threadIdx.x; const int idx = blockIdx.x * blockDim.x + tid; float exp_val = (idx \u003c N) ? expf[x[idx]] : 0.0f; float exp_sum = block_reduce_sum_f32\u003cNUM_THREADS\u003e(exp_val); if (idx \u003c N) { y[idx] = exp_val / exp_sum; } } ","date":"2025-08-07","objectID":"/2025/ebaa040/:2:1","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"向量化存取 #define FLOAT4(value) (reinterpret_cast\u003cint4 *\u003e(\u0026(value))[0]) template \u003cconst int NUM_THREADS = 256\u003e __global__ void softmax_f32x4_per_token_kernel(float *x, float *y, int N) { const int tid = threadIdx.x; const int idx = (blockIdx.x * blockDim.x + tid) * 4; // 向量化 取 float4 reg_x = FLOAT4(x[idx]); float4 reg_exp; reg_exp.x = (idx + 0 \u003c N) ? expf(reg_x.x) : 0.0f; reg_exp.y = (idx + 1 \u003c N) ? expf(reg_x.y) : 0.0f; reg_exp.z = (idx + 2 \u003c N) ? expf(reg_x.z) : 0.0f; reg_exp.w = (idx + 3 \u003c N) ? expf(reg_x.w) : 0.0f; float exp_val = (reg_exp.x + reg_exp.y + reg_exp.z + reg_exp.w); float exp_sum = block_reduce_sum_f32\u003cNUM_THREADS\u003e(exp_val); // 向量化 存 if (idx + 3 \u003c N) { float4 reg_y; reg_y.x = reg_exp.x / exp_sum; reg_y.y = reg_exp.y / exp_sum; reg_y.z = reg_exp.z / exp_sum; reg_y.w = reg_exp.w / exp_sum; FLOAT4[y[idx]] = reg_y; } } ","date":"2025-08-07","objectID":"/2025/ebaa040/:2:2","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"safe-softmax template \u003cconst int NUM_THREADS = 256\u003e __global__ void safe_softmax_f32_per_token_kernel(float *x, float *y, int N) { const int tid = threadIdx.x; const int idx = blockIdx.x * blockDim.x + tid; float val = (idx \u003c N) ? x[idx] : -FLT_MAX; float max_val = block_reduce_max_f32\u003cNUM_THREADS\u003e(val); float exp_val = (idx \u003c N) ? expf[x[idx] - max_val] : 0.0f; float exp_sum = block_reduce_sum_f32\u003cNUM_THREADS\u003e(exp_val); if (idx \u003c N) y[idx] = exp_val / exp_sum; } ","date":"2025-08-07","objectID":"/2025/ebaa040/:2:3","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"online-softmax struct __align__(8) MD { float M; float D; }; template \u003cconst int NUM_THREADS = 256\u003e __global__ void online_softmax_f32_per_token_kernel(float *x, float *y, int N) { // } __global__ void sgemm_naive_f32_kernel(float *a, float *b, float *c, int M, int N, int K) { int n = blockIdx.x * blockDim.x + threadIdx.x; int m = blockIdx.y * blockDim.y + threadIdx.y; if (m \u003c M \u0026\u0026 n \u003c N) { float psum = 0.0f; #pragma unroll for (int k = 0; k \u003c K; ++ k) { psum += a[m * K + k] * [k * N + n]; } c[m * M + n] = psum; } } template \u003cconst int BM = 32, const int BN = 32, const int BK = 32\u003e __global__ void sgemm_sliced_k_f32_kernel(float *a, float *b, float *c, int M, int N, int K) { // block tile ： 32x32 的 block 处理 c 上一块 32x32 的元素计算 // K tile： 使用共享内存，将 K 分块成 BK 大小的块 __shared__ float s_a[BM][BK], s_b[BK][BN]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int tid = threadIdx.y * blockDim.x + tx; int load_smem_a_m = tid / 32; int load_smem_a_n = tid % 32; int load_smem_b_n = tid / 32; int load_smem_b_k = tid % 32; int load_gmem_a_m = by * BM + load_smem_a_m; // global row of a and c int load_gmem_b_n = bx * BN + load_smem_b_n; // global col of b and c if (load_gmem_a_m \u003e= M || load_gmem_b_n \u003e= N) return; float sum = 0.0f; for (int bk = 0; bk \u003c (K + BK - 1) / BK; ++ bk) { // 加载 a 的全局内容到共享内存 int load_gmem_a_k = bk * BK + load_smem_a_k; int load_gmem_a_addr = load_gmem_a_m * K + load_gmem_a_k; s_a[load_gmem_a_m][load_gmem_a_k] = a[load_gmem_a_addr]; // 加载 b 的全局内容到共享内存 int load_gmem_b_k = bk * BK + load_smem_b_k; int load_gmem_b_addr = load_gmem_b_k * N + load_gmem_b_n; s_b[load_gmem_b_k][load_gmem_b_n] = b[load_gmem_b_addr]; __syncthreads(); } #pragma unroll for (int k = 0; k \u003c BK; ++ k) { // 共享内存内进行 block gemm sum += s_a[load_smem_a_m][k] * s_b[k][load_smem_b_n]; } __syncthreads(); // 存 int store_gmem_c_addr = load_gmem_a_m * N + load_gmem_b_n; c[store_gmem_c_addr] = sum; } ","date":"2025-08-07","objectID":"/2025/ebaa040/:2:4","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"Matmul ","date":"2025-08-07","objectID":"/2025/ebaa040/:3:0","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"naive 版 __global__ void sgemm_naive_f32_kernel(float *a, float *b, float *c, int M, int N, int K) { int n = blockIdx.x * blockDim.x + threadIdx.x; int m = blockIdx.y * blockDim.y + threadIdx.y; if (m \u003c M \u0026\u0026 n \u003c N) { float psum = 0.0f; #pragma unroll for (int k = 0; k \u003c K; ++ k) { psum += a[m * K + k] * [k * N + n]; } c[m * M + n] = psum; } } ","date":"2025-08-07","objectID":"/2025/ebaa040/:3:1","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"shared_mem 优化 template \u003cconst int BM = 32, const int BN = 32, const int BK = 32\u003e __global__ void sgemm_sliced_k_f32_kernel(float *a, float *b, float *c, int M, int N, int K) { // block tile ： 32x32 的 block 处理 c 上一块 32x32 的元素计算 // K tile： 使用共享内存，将 K 分块成 BK 大小的块 __shared__ float s_a[BM][BK], s_b[BK][BN]; int bx = blockIdx.x; int by = blockIdx.y; int tx = threadIdx.x; int ty = threadIdx.y; int tid = threadIdx.y * blockDim.x + tx; int load_smem_a_m = tid / 32; int load_smem_a_n = tid % 32; int load_smem_b_n = tid / 32; int load_smem_b_k = tid % 32; int load_gmem_a_m = by * BM + load_smem_a_m; // global row of a and c int load_gmem_b_n = bx * BN + load_smem_b_n; // global col of b and c if (load_gmem_a_m \u003e= M || load_gmem_b_n \u003e= N) return; float sum = 0.0f; for (int bk = 0; bk \u003c (K + BK - 1) / BK; ++ bk) { // 加载 a 的全局内容到共享内存 int load_gmem_a_k = bk * BK + load_smem_a_k; int load_gmem_a_addr = load_gmem_a_m * K + load_gmem_a_k; s_a[load_gmem_a_m][load_gmem_a_k] = a[load_gmem_a_addr]; // 加载 b 的全局内容到共享内存 int load_gmem_b_k = bk * BK + load_smem_b_k; int load_gmem_b_addr = load_gmem_b_k * N + load_gmem_b_n; s_b[load_gmem_b_k][load_gmem_b_n] = b[load_gmem_b_addr]; __syncthreads(); } #pragma unroll for (int k = 0; k \u003c BK; ++ k) { // 共享内存内进行 block gemm sum += s_a[load_smem_a_m][k] * s_b[k][load_smem_b_n]; } __syncthreads(); // 存 int store_gmem_c_addr = load_gmem_a_m * N + load_gmem_b_n; c[store_gmem_c_addr] = sum; } ","date":"2025-08-07","objectID":"/2025/ebaa040/:3:2","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"Transpose ","date":"2025-08-07","objectID":"/2025/ebaa040/:4:0","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"naive __global__ void transpose_naive(float *input, float *output, int M, int N) { int n = blockIdx.x * blockDim.x + threadIdx.x; int m = blockIdx.y * blockDim.y + threadIdx.y; if (m \u003c M \u0026\u0026 n \u003c N) { output[n * M + m] = input[m * N + n]; } } ","date":"2025-08-07","objectID":"/2025/ebaa040/:4:1","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"合并写入 __global__ void transpose(float* input, float* output, int M, int N) { // output的row和col int row = blockDim.y * blockIdx.y + threadIdx.y; int col = blockDim.x * blockIdx.x + threadIdx.x; ​ if (row \u003c N \u0026\u0026 col \u003c M) { output[row * M + col] = __ldg(\u0026input[col * N + row]); // 合并写入，读取使用__ldg进行缓存 } } ","date":"2025-08-07","objectID":"/2025/ebaa040/:4:2","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"使用共享内存中转，同时合并读取和写入 // 输入矩阵是M行N列，输出矩阵是N行M列 dim3 block(32, 32); dim3 grid(CEIL(N,32), CEIL(M,32)); // 根据input的形状(M行N列)进行切块 transpose\u003c32\u003e\u003c\u003c\u003cgrid, block\u003e\u003e\u003e(input, output, M, N); ​ template \u003cconst int BLOCK_SIZE\u003e __global__ void transpose(float* input, float* output, int M, int N) { __shared__ float s_mem[BLOCK_SIZE][BLOCK_SIZE + 1]; // 避免bank conflict int bx = blockIdx.x * BLOCK_SIZE; int by = blockIdx.y * BLOCK_SIZE; int x1 = bx + threadIdx.x; int y1 = by + threadIdx.y; if (x1 \u003c N \u0026\u0026 y1 \u003c M) { s_mem[threadIdx.y][threadIdx.x] = input[y1 * N + x1]; } __syncthreads(); ​ int x2 = by + threadIdx.x; int y2 = bx + threadIdx.y; if (x2 \u003c M \u0026\u0026 y2 \u003c N) { output[y2 * M + x2] = s_mem[threadIdx.x][threadIdx.y]; // padding后，不存在bank conflict } } ","date":"2025-08-07","objectID":"/2025/ebaa040/:4:3","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"cuda example 补充 https://zhuanlan.zhihu.com/p/12661298743 主要是 transpose 优化、gemv、softmax_matrix，cuda前缀和，topk ","date":"2025-08-07","objectID":"/2025/ebaa040/:4:4","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"Memory Coalescing 和 Bank Conflict 内存合并访问： 参考文献：https://zhuanlan.zhihu.com/p/300785893 Bank Conflict： 参考文献： https://zhuanlan.zhihu.com/p/4746910252 https://zhuanlan.zhihu.com/p/659142274 ","date":"2025-08-07","objectID":"/2025/ebaa040/:4:5","tags":["八股"],"title":"CUDA 常用算子案例","uri":"/2025/ebaa040/"},{"categories":["找工作"],"content":"计算机网络基础复习 ","date":"2025-08-06","objectID":"/2025/3dc98a3/:0:0","tags":["八股"],"title":"计算机网络","uri":"/2025/3dc98a3/"},{"categories":["找工作"],"content":"TCP 协议 ","date":"2025-08-06","objectID":"/2025/3dc98a3/:1:0","tags":["八股"],"title":"计算机网络","uri":"/2025/3dc98a3/"},{"categories":["找工作"],"content":"三次握手 为什么三次握手可以建立\"可靠\"传输？ 第一次握手：如果成功，Server 则确认 Client的发送信道可靠，Server 也确认 自己的接收信道可靠。 第二次握手：如果成功，Client 则确认 Server的发送信道和接收信道都可靠，Client 也确认 自己的发送信道和接收信道都可靠。 第三次握手：如果成功，Server 则确认 Client的接收信道可靠，Server 也确认 自己的发送信道可靠。 此时，Server和Client双方都确认 自己和对方的发送和接收通道已经可以正常运行。 为什么不进行两次握手？ Server 无法确认 Client 的接收信道和自己的发送信道 是否可靠。即 Server 无法知道客户端是否收到了 SYN-ACK 响应 为什么不需要超过三次握手？ 四次或更多次握手将带来不必要的开销，且不会带来额外的可靠性优势，三次足以。 ","date":"2025-08-06","objectID":"/2025/3dc98a3/:1:1","tags":["八股"],"title":"计算机网络","uri":"/2025/3dc98a3/"},{"categories":["找工作"],"content":"四次挥手 为什么需要四次挥手？ 服务器收到客户端的 FIN 报文时，内核会马上回一个 ACK 应答报文，但是服务端应用程序可能还有数据要发送，所以不能马上发送 FIN 报文，而是将发送 FIN 报文的控制权交给服务端应用程序： 如果服务端应用程序有数据要发送的话，就发完数据后，才调用关闭连接的函数 如果服务端应用程序没有数据要发送的话，可以直接调用关闭连接的函数 什么时候变成三次挥手？ 当被动关闭方（上图的服务端）在 TCP 挥手过程中，「没有数据要发送」并且「开启了 TCP 延迟确认机制」，那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。 ","date":"2025-08-06","objectID":"/2025/3dc98a3/:1:2","tags":["八股"],"title":"计算机网络","uri":"/2025/3dc98a3/"},{"categories":["找工作"],"content":"可靠传输 TCP 是通过序列号、确认应答、校验和、重发控制、连接管理以及窗口控制等机制实现可靠性传输的。 重传机制 常见的重传机制 超时重传 快速重传 SACK D-SACK 为什么需要重传机制？ 因为会多种网络问题会导致丢包（网络拥塞、信号干扰导致数据包损坏、路由器故障或链路中断、接收方处理能力不足，来不及处理…） 为了应对各种情况，TCP 需要能够检测到丢包，并重发丢失的数据包。那么 TCP 发送方如何判断丢包了呢？答案是确认应答机制（ACK）TCP 通过确认应答（ACK）和超时重传这两种基本机制来实现可靠传输。 确认应答 发送方发送数据时，为每个数据段分配一个序列号（Sequence Number） 接收方收到数据后，返回一个确认号（Acknowledgement Number），表示期望收到的下一个字节序号 发送方通过接收到的确认号，可以判断数据是否已经成功到达接收方 如上图所示， 发送方先发送第一个数据包： 序列号 SEQ = 100，长度50字节，也就是发送了100 到 149 这段数据 接收方收到后回复： ACK = 150，意思是\"我收到了100-149 的数据，请发送150 开始的数据\" 发送方继续发送第二个数据包： 序列号 SEQ = 150，长度50字节，也就是发送了150 到 199 这段数据 接收方再次回复： ACK = 200，意思是\"我收到了150-199 的数据，请发送 200 开始的数据\" 当然，上图简化为一方发送，另一方接收数据，实际上 TCP 是全双工协议，双方可以同时发送/接收，同时发送时，ACK 就携带在数据包中。 超时重传 上面的例子展示的是理想情况，但网络是不可靠的，数据包在传输过程中可能会出现两种问题： 数据包丢失 ACK 确认包丢失 如何解决这个问题呢？对，就是超时重传：在发送数据包之后启动一个定时器，如果在指定重传超时时间（RTO, Retransmission Timeout）没有收到 ACK，就认为数据包丢失了，需要重新发送 看到这，我们很自然的想到，超时重传机制有一个关键的参数：RTO（Retransmission Timeout，超时重传时间），RTO 的选择是个非常难的问题，过长或者过短都有问题： RTO 太短：可能导致不必要的重传，浪费带宽 RTO 太长：丢包后等待时间过长，影响传输效率 理论上来说，RTO 应该要略微大于**网络往返时间（RTT）**，既要避免过早重传，也要保证丢包时能及时恢复 滑动窗口 TCP 利用滑动窗口实现流量控制和拥塞控制 发送窗口图示，可以分为四个部分： 已经发送并且确认的 TCP 段（已经发送并确认） 已经发送但是没有确认的 TCP 段（已经发送未确认） 未发送但是接收方准备接收的 TCP 段（可以发送） 未发送并且接收方也并未准备接受的 TCP 段（不可发送） 接收窗口图示，可以分为三个部分： 已经接收并且已经确认的 TCP 段（已经接收并确认） 等待接收且允许发送方发送 TCP 段（可以接收未确认） 不可接收且不允许发送方发送 TCP 段（不可接收） 流量控制 流量控制是为了控制发送方发送速率，保证接收方来得及接收 在上面图片中这个案例，发送方初始时拥有一个400字节的可用窗口，这个窗口代表了在不等待对方确认的情况下，发送方可以连续发送的数据量。就像是一个在传送带上滑动的窗口，窗口内的数据可以被发送，随着 ACK 的到来，窗口会向前滑动 整个过程的数据传输流程如下： 第一阶段，正常传输 发送方首先发送1-100字节的数据，接着发送101-200字节，然后发送201-300字节，最后发送301-400字节。 发送方每次发送完数据后，窗口就会相应地向前移动，直到全部可用窗口用完，不能再继续发送数据，必须等待接收方确认。 第二阶段，丢包处理 在发送301-400字节的数据包时发生了丢包。这时系统的处理过程如下： 接收方发现收到了乱序数据，发送ACK=301，表示期望收到301字节开始的数据，接收方同时将窗口调整为200字节，这是进行流量控制。 发送方收到ACK=301后，知道前300字节已经被确认，发送窗口向右移动 300 字节，此时还有 301-400 未被确认，并且窗口被调整为 200 字节，所以还可以继续发送 401-500 字节的数据。 第三阶段，超时重传 当发送方重传计时器超时后发现301-400字节的数据包可能丢失： 触发超时重传，重新发送301-400字节的数据 接收方成功收到重传的数据和之前的401-500字节数据，发送ACK=501，并将接收窗口调整为100字节，进行流量控制 发送方收到 ACK 501 后，知道前面 500 字节数据已经被确认，发送窗口向右移动 200 字节，但由于此时窗口大小已经被调整为 100 字节，所以只能继续发送501-600字节的数据。 第四阶段，暂停发送 最后接收方收到 501-600 的数据，回复一个 ACK 601，同时将接收方窗口调整为 0，此时： 接收方通过ACK=601确认所有数据 窗口调整为 0，迫使发送方暂停发送，直到接收方重新分配窗口空间 整个过程为了方便理解，我们只画了 Client 到 Server 的数据发送和 Server 对 Client 进行的流量控制，但实际上 TCP 是全双工协议，数据发送是双向的，Client 也会对 Server 进行流量控制，道理都是一样的。 这个例子中，接收方可能由于缓冲区满，应用层来不及接收，在不断的减缓接收方窗口，以此降低发送方发送速率，并在最后将接收窗口调整为 0，这将暂停发送方的数据发送。 窗口减小到 0，我们称之为窗口关闭 拥塞控制 真实的网络是有传输瓶颈的，因为网络传输过程中会经过网线、路由器等设备，网线具有带宽限制，路由器等中转设备有缓存限制，一旦网络中传输的数据超过其承载能力，就会导致：路由器缓冲区溢出，丢弃数据包传输延迟增加网络吞吐量下降重传次数增加，进一步加重网络负担最后会导致大家都没法传输数据，所以我们需要拥塞控制。没有合适的拥塞控制机制，就容易出现网络拥塞的情况，就像在没有红绿灯和限速的高速公路上，车辆可能会堵塞一样。 拥塞控制与流量控制的区别？ 流量控制的目的是为了让发送方速率和接收方匹配，而拥塞控制是从整个网络全局出发，检测拥塞是否发生，如果发生则自发调整发送速度，以恢复网络。 流量控制确保接收方不会过载，拥塞控制避免整个网络过载。 都是通过控制发送方窗口来实现的。 拥塞控制核心概念 TCP 拥塞控制主要通过以下两个关键参数来实现： 拥塞窗口（Congestion Window，cwnd）：发送方维护的一个状态变量，用于限制可以发送但未收到确认的数据量 慢开始门限（Slow Start Threshold，ssthresh）：用于决定是使用慢开始算法还是拥塞避免算法 当 cwnd \u003c= ssthresh 使用慢启动算法 当 cwnd \u003e ssthresh 使用拥塞避免算法 实际发送窗口 = min(拥塞窗口cwnd, 接收窗口rwnd) 为了进行拥塞控制，TCP 发送方要维持一个 拥塞窗口(cwnd) 的状态变量。拥塞控制窗口的大小取决于网络的拥塞程度，并且动态变化。发送方让自己的发送窗口取为拥塞窗口和接收方的接受窗口中较小的一个。 TCP 的拥塞控制采用了四种算法，即 慢开始、 拥塞避免、快重传 和 快恢复。在网络层也可以使路由器采用适当的分组丢弃策略（如主动队列管理 AQM），以减少网络拥塞的发生。 慢开始 慢开始算法的思路是当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么可能会引起网络阻塞，因为现在还不知道网络的符合情况。经验表明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是由小到大逐渐增大拥塞窗口数值。cwnd 初始值为 1，每经过一个传播轮次，cwnd 加倍。 拥塞避免 拥塞避免算法的思路是让拥塞窗口 cwnd 缓慢增大，即每经过一个往返时间 RTT 就把发送方的 cwnd 加 1. 拥塞时间 这种拥塞避免的线性增长什么时候会结束呢？当出现数据包丢失，也就是 RTO 超时的时候，TCP 认为网络可能出现了拥塞，于是重传超时的数据包，同时： 更新 ssthresh 为当前 cwnd 一半，即 sshthresh = cwnd/2 同时将 cwnd 更新为 1 进入慢启动过程 可以看到，当 TCP 发生了 RTO 超时重传的时候，cwnd 就被打回了原型，重新从慢启动开始探测。显然这个拥塞处理太粗暴了，毕竟偶尔丢个包也不一定是网络拥塞了，完全犯不着 会导致网络传输速度剧烈抖动！ 快重传 丢包事件也能由三个冗余 ACK 事件触发，TCP 认为这种“丢包事件”，相比于 RTO 超时指示的丢包，反应应该不那么剧烈。 这种情况下就是触发快速重传，发送方的行为: 收到3个连续重复ACK后立即重传对应报文段 不必等到RTO超时，可以更快响应丢包情况 接收方行为： 收到数据立即发送ACK确认，不能等待发送数据进行捎带确认 对失序数据也要发送重复ACK，而不是等待数据恢复 而后面 TCP Reno（rfc5681） 的处理则会温和很多： cwnd 更新为原来一半，cwnd = cwnd /2 ssthresh 更新为最新的 cwnd ，ssthresh = cwnd 然后进入快速恢复阶段 快恢复 快速恢复算法（Fast Recovery）是 TCP Reno 中用来处理丢包时的一种机制，它通过减少数据包丢失后对窗口大小的调整，避免了像传统的超时重传（RTO）那样的剧烈退避。 引入了快速恢复算法，是为了在恢复丢失数","date":"2025-08-06","objectID":"/2025/3dc98a3/:1:3","tags":["八股"],"title":"计算机网络","uri":"/2025/3dc98a3/"},{"categories":["找工作"],"content":"UDP 协议 ","date":"2025-08-06","objectID":"/2025/3dc98a3/:2:0","tags":["八股"],"title":"计算机网络","uri":"/2025/3dc98a3/"},{"categories":["找工作"],"content":"协议特点 无连接：与 TCP 不同，UDP 在发送数据之前，不需要与目标主机建立连接，也不需要经过“三次握手”这些复杂的步骤。它就像是投掷一封信到远方，根本不管信是否能够成功送达。 无状态：UDP 不维护任何的连接状态，也不记录数据的发送情况。每次发送的数据包都是独立的，UDP不关心数据包的顺序和丢失。每个数据包（Datagram）独立存在，和其他的数据包没有任何关系。 尽力而为：UDP 会尽力传输数据，但并不保证数据一定能够成功到达目标主机，也不保证数据的顺序。它没有流量控制、拥塞控制，也不提供重传机制。如果一个数据包丢失了，发送方也不会自动重新发送。它的目标就是尽可能快速地把数据送到目标。 较小的开销：由于没有TCP那种复杂的控制机制，UDP报文头的大小也较小（只有8字节），这使得它在带宽受限或要求低延迟的场景反而表现得更好。 从不做什么角度，可能更能清晰的记住 UDP 提供的服务： UDP 在发送数据之前不建立连接，它只是将其打包然后发送。 UDP 不提供确认来表明数据已收到。 UDP 不保证其消息一定会到达。 UDP 不会检测丢失的消息并重新传输它们。 UDP 不确保数据的接收顺序与发送顺序相同。 UDP 不提供任何机制来管理设备之间的数据流或处理拥塞。 ","date":"2025-08-06","objectID":"/2025/3dc98a3/:2:1","tags":["八股"],"title":"计算机网络","uri":"/2025/3dc98a3/"},{"categories":["找工作"],"content":"协议报文格式 ","date":"2025-08-06","objectID":"/2025/3dc98a3/:2:2","tags":["八股"],"title":"计算机网络","uri":"/2025/3dc98a3/"},{"categories":["找工作"],"content":"《HOT100》做题记录 ","date":"2025-03-26","objectID":"/2025/a4da344/:0:0","tags":null,"title":"《HOT100》","uri":"/2025/a4da344/"},{"categories":["找工作"],"content":"5.最长回文子串 🔗 给你一个字符串 s，找到 s 中最长的 回文 子串。 class Solution { static const int N = 1e3 + 10; static const int p[N], b[N]; public: string longestPalindrome(string s) { // 用 manacher 算法求解 int k = 0, n = 0; auto init = [\u0026]() { b[k ++ ] = '$', b[k ++ ] = '#'; for (auto c : s) { b[k ++ ] = c; b[k ++ ] = '#'; } b[k ++ ] = '^'; n = k; }; init(); auto manacher = [$]() { int mr = 0, mid = 0; for (int i = 1; i \u003c n; i ++ ) { if (i \u003c mr) p[i] = max(2 * mid - i, mr - i); else p[i] = 1; while (b[i - p[i]] == b[i + p[i]]) p[i] ++ ; if (i + p[i] \u003e mr) { mr = i + p[i]; mid = i; } } }; manacher(); // 如果只是求最长回文子串的长度，返回res即可 int res = 0, max_i = 0; for (int i = 0; i \u003c n; i ++ ) { if (p[i] \u003e res) { res = p[i]; max_i = i; } } // 如果是求具体的回文子串，需要根据最长长度截取 int start = max_i - (res - 1); int end = max_i + (res - 1); for (int i = start; i \u003c= end; i ++ ) { if (i == 0 || i == n - 1) continue; if (i % 2 == 0) ans.push_back(b[i]); } return ans; } }; ","date":"2025-03-26","objectID":"/2025/a4da344/:0:1","tags":null,"title":"《HOT100》","uri":"/2025/a4da344/"},{"categories":["找工作"],"content":"记录准备面试过程中看到的一些 C++ 的常用语法及特性 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:0","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"const 和 constexpr 区别 const 并未区分编译期常量和运行期常量，并且 const 只保证了运行时不直接被修改，而 constexpr 是限定在了编译器常量。所以 constexpr 相当于是把 const 的职责拆出来一部分，const 只做只读语义的保证，而常量语义交给了 constexpr 负责。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:1","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"左值、右值与左值引用和右值引用 左值指既能够出现在等号左边，也能出现在等号右边的变量（可以取地址）；右值则是只能出现在等号右边的变量（不可取地址）。 左值是可寻址的变量，有持久性； 右值一般是不可寻址的常量，或在表达式求值过程中创建的无名临时对象，短暂性的。 左值引用就是对左值的引用，给左值取别名，避免对象拷贝 右值引用就是对右值的引用，给右值取别名。主要作用是把延长对象的生命周期，一般是延长到作用域之外 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:2","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"字节序—大端序与小端序 字节序是指在多字节数据类型（如整数、浮点数等）中，字节在内存中的存储顺序。主要有两种字节序：大端字节序（Big-endian）和小端字节序（Little-endian）。 大端序：高位字节存储在低地址处，低位字节存储在高地址处。例如，一个4字节的整数0x12345678，在大端字节序的系统中，内存布局如下（从左侧的低地址到右侧的高地址）：0x12 | 0x34 | 0x56 | 0x78 小端序：低位字节存储在低地址处，高位字节存储在高地址处。例如，一个4字节的整数0x12345678，在小端字节序的系统中，内存布局如下（从左侧的低地址到右侧的高地址）：0x78 | 0x56 | 0x34 | 0x12 常见大小端字节序应用场景： 网络传输，通常用大端序，也称网络字节序 操作系统一般主要是小端序 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:3","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"字节对齐 理论上，任何类型的变量都可以从任意地址开始存放。然而实际上，访问特定类型的变量通常需要从特定对齐的内存地址开始。如果不对数据存储进行适当的对齐，可能会导致存取效率降低。 例如，有些平台每次读取都是从偶数地址开始。如果一个 int 类型（假设为 32 位系统）存储在偶数地址开始的位置，那么一个读周期就可以读取这 32 位。但如果存储在奇数地址开始的位置，则需要两个读周期，并将两次读取的结果的高低字节拼凑才能得到这 32 位数据。显然这会显著降低读取效率。 总结： 字节对齐有助于提高内存访问速度，因为许多处理器都优化了对齐数据的访问。但是，这可能会导致内存中的一些空间浪费。 字节对齐的一些规则： 自然规则对齐：按照数据本身的数据类型进行对齐，例如，char 类型的自然对齐边界为 1 字节，short 为 2 字节，int 和 float 为 4 字节，double 和 64 位指针为 8 字节。具体数值可能因编译器和平台而异。 结构体对齐：结构体内部的每个成员都根据其自然对齐边界进行对齐。也就是可能在成员之间插入填充字节。结构体本身的总大小也会根据其最大对齐边界的成员进行对齐（比如结构体成员包含的最长类型为int类型，那么整个结构体要按照4的倍数对齐），以便在数组中正确对齐。 可以使用编译器指令（如 #pragma pack）更改默认的对齐规则。这个命令是全局生效的。这可以用于减小数据结构的大小，但可能会降低访问性能。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:4","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"C++中class和struct区别 C++ 中为了兼容 C 语言而保留了 C 语言的 struct 关键字，并且加以扩充了含义。 在 C 语言中，struct 只能包含成员变量，不能包含成员函数。 而在 C++ 中，struct 类似于 class，既可以包含成员变量，又可以包含成员函数。 不同点 class 中类中的成员默认都是 private 属性的。 而在 struct 中结构体中的成员默认都是 public 属性的。 class 继承默认是 private 继承，而 struct 继承默认是 public 继承。 class 可以用于定义模板参数，struct 不能用于定义模板参数。 这样写是正确的： template \u003cclass T\u003e struct Person { public: T age; }; 而这样写是错误的： template \u003cstruct T\u003e struct Person { public: T age; }; 使用习惯 实际使用中，struct 我们通常用来定义一些 POD(plain old data) 在 C++11 及之后的标准中，POD 类型需要同时满足两个独立条件： ​​平凡（Trivial）​​：类型具有默认的构造/拷贝/移动/析构函数（可自动生成且非虚） ​​标准布局（Standard Layout）​​：内存布局与 C 兼容，成员排列顺序符合特定规则 同时满足平凡性和标准布局的类型称为 POD 类型，这类数据可以安全使用 memcpy 等底层内存操作，因为它们的内存布局与 C 完全兼容且没有特殊处理需求。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:5","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"C++四种强制类型转换 B站一面面试题 static_cast 用法：static_cast\u003cnew_type\u003e(expression) 其实static_cast 和 C 语言 () 做强制类型转换基本是等价的。 主要用于以下场景: 基本类型之间的转换 将一个基本类型转换为另一个基本类型，例如将整数转换为浮点数或将字符转换为整数。 int a = 42; double b = static_cast\u003cdouble\u003e(a); // 将整数a转换为双精度浮点数b 指针类型之间的转换 将一个指针类型转换为另一个指针类型，尤其是在类层次结构中从基类指针转换为派生类指针。这种转换不执行运行时类型检查，可能不安全，要自己保证指针确实可以互相转换。 class Base {}; class Derived : public Base {}; Base* base_ptr = new Derived(); Derived* derived_ptr = static_cast\u003cDerived*\u003e(base_ptr); // 将基类指针base_ptr转换为派生类指针derived_ptr 引用类型之间的转换 类似于指针类型之间的转换，可以将一个引用类型转换为另一个引用类型。在这种情况下，也应注意安全性。 Derived derived_obj; Base\u0026 base_ref = derived_obj; Derived\u0026 derived_ref = static_cast\u003cDerived\u0026\u003e(base_ref); // 将基类引用base_ref转换为派生类引用derived_ref static_cast在编译时执行类型转换，在进行指针或引用类型转换时，需要自己保证合法性。 如果想要运行时类型检查，可以使用dynamic_cast进行安全的向下类型转换。 dynamic_cast 用法: dynamic_cast\u003cnew_type\u003e(expression) dynamic_cast在C++中主要应用于父子类层次结构中的安全类型转换。它在运行时执行类型检查，因此相比于static_cast，它更加安全。dynamic_cast的主要应用场景： 向下类型转换 当需要将基类指针或引用转换为派生类指针或引用时，dynamic_cast可以确保类型兼容性。 如果转换失败，dynamic_cast将返回空指针（对于指针类型）或抛出异常（对于引用类型）。 class Base { virtual void dummy() {} }; class Derived : public Base { int a; }; Base* base_ptr = new Derived(); Derived* derived_ptr = dynamic_cast\u003cDerived*\u003e(base_ptr); // 将基类指针base_ptr转换为派生类指针derived_ptr，如果类型兼容，则成功 用于多态类型检查 处理多态对象时，dynamic_cast可以用来确定对象的实际类型，例如： class Animal { public: virtual ~Animal() {} }; class Dog : public Animal { public: void bark() { /* ... */ } }; class Cat : public Animal { public: void meow() { /* ... */ } }; Animal* animal_ptr = /* ... */; // 尝试将Animal指针转换为Dog指针 Dog* dog_ptr = dynamic_cast\u003cDog*\u003e(animal_ptr); if (dog_ptr) { dog_ptr-\u003ebark(); } // 尝试将Animal指针转换为Cat指针 Cat* cat_ptr = dynamic_cast\u003cCat*\u003e(animal_ptr); if (cat_ptr) { cat_ptr-\u003emeow(); } 另外，要使用dynamic_cast有效，基类至少需要一个虚拟函数。 因为，dynamic_cast只有在基类存在虚函数(虚函数表)的情况下才有可能将基类指针转化为子类。 dynamic_cast 底层原理 dynamic_cast的底层原理依赖于运行时类型信息（RTTI, Runtime Type Information）。 C++编译器在编译时为支持多态的类生成RTTI，它包含了类的类型信息和类层次结构。 我们都知道当使用虚函数时，编译器会为每个类生成一个虚函数表（vtable），并在其中存储指向虚函数的指针。 伴随虚函数表的还有 RTTI(运行时类型信息)，这些辅助的信息可以用来帮助我们运行时识别对象的类型信息。 《深度探索C++对象模型》中有个例子： class Point { public: Point(float xval); virtual ~Point(); float x() const; static int PointCount(); protected: virtual ostream\u0026 print(ostream\u0026 os) const; float _x; static int _point_count; }; 首先，每个多态对象都有一个指向其vtable的指针，称为vptr。 RTTI（就是上面图中的 type_info 结构)通常与vtable关联。 dynamic_cast就是利用RTTI来执行运行时类型检查和安全类型转换。 以下是dynamic_cast的工作原理的简化描述： 首先，dynamic_cast通过查询对象的 vptr 来获取其RTTI（这也是为什么 dynamic_cast 要求对象有虚函数） 然后，dynamic_cast比较请求的目标类型与从RTTI获得的实际类型。如果目标类型是实际类型或其基类，则转换成功。 如果目标类型是派生类，dynamic_cast会检查类层次结构，以确定转换是否合法。如果在类层次结构中找到了目标类型，则转换成功；否则，转换失败。 当转换成功时，dynamic_cast返回转换后的指针或引用。 如果转换失败，对于指针类型，dynamic_cast返回空指针；对于引用类型，它会抛出一个std::bad_cast异常。 因为dynamic_cast依赖于运行时类型信息，它的性能可能低于其他类型转换操作（如static_cast），static 是编译器静态转换，编译时期就完成了。 const_cast 用法: const_cast\u003cnew_type\u003e(expression) new_type 必须是一个指针、引用或者指向对象类型成员的指针。 修改const对象 当需要修改const对象时，可以使用const_cast来删除const属性。 const int a = 42; int* mutable_ptr = const_cast\u003cint*\u003e(\u0026a); // 删除const属性，使得可以修改a的值 *mutable_ptr = 43; // 修改a的值 const对象调用非const成员函数 当需要使用const对象调用非const成员函数时，可以使用const_cast删除对象的const属性。 class MyClass { public: void non_const_function() { /* ... */ } }; const MyClass my_const_obj; MyClass* mutable_obj_ptr = const_cast\u003cMyClass*\u003e(\u0026my_const_obj); // 删除const属性，使得可以调用非const成员函数 mutable_obj_ptr-\u003enon_const_function(); // 调用非const成员函数 不过上述行为都不是很安全，可能导致未定义的行为，因此应谨慎使用。 reinterpret_cast 用法: reinterpret_cast\u003cnew_type\u003e(expression) reinterpret_cast用于在不同类型之间进行低级别的转换。 首先从英文字面的意思理解，interpret是“解释，诠释”的意思，加上前缀“re”，就是“重新诠释”的意思； cast 在这里可以翻译成“转型”（在侯捷大大翻译的《深度探索C对象模型》、《Effective C（第三版）》中，cast都被翻译成了转型），这样整个词顺下来就是“重新诠释的转型”。 它仅仅是重新解释底层比特（也就是对指针所指针的那片比特位换个类型做解释），而不进行任何类型检查。 因此，reinterpret_cast可能导致未定义的行为，应谨慎使用。 reinterpret_cast的一些典型应用场景： 指针类型之间的转换 在某些情况下，需要在不同指针类型之间进行转换，如将一个int指针转换为char指针。 这在 C 语言中用的非常多，C语言中就是直接使用 () 进行强制类型转换 int a = 42; int* int_ptr = \u0026a; char* char_ptr = reinterpret_cast\u003cchar*\u003e(int_ptr); // 将int指针转换为char指针 其实在 CUDA 中能经常见到 reinterpret_cast 的使用，例如这里的向量化加载： #define INT4(value) (reinterpret_cast\u003cint4 *\u003e(\u0026(value)","date":"2025-03-26","objectID":"/2025/978bdd1/:0:6","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"面向对象特性 封装 实现一个class，将数据属性，方法等集成到一个类中的过程，隐藏内部实现细节，仅暴露接口给外部 继承 一个类从另外一个类中获得属性与方法的过程。通过创建具有共享代码的类层次结构，减少重复代码，提高代码复用性和可维护性。 多态 多态是允许不同类的对象使用相同的接口名字，但具有不同实现的特性。在 C++ 中，多态主要通过虚函数（Virtual Function）和抽象基类（Abstract Base Class）来实现。虚函数允许在派生类中重写基类的方法，而抽象基类包含至少一个纯虚函数（Pure Virtual Function），不能被实例化，只能作为其他派生类的基类。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:7","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"重载、重写和隐藏 重载 重载是指相同作用域(比如命名空间或者同一个类)内拥有相同的方法名，但具有不同的参数类型和/或参数数量的方法。 重载允许根据所提供的参数不同来调用不同的函数。它主要在以下情况下使用： 方法具有相同的名称。 方法具有不同的参数类型或参数数量。 返回类型可以相同或不同。 同一作用域，比如都是一个类的成员函数，或者都是全局函数 重写 重写是指在派生类中重新定义基类中的方法。当派生类需要改变或扩展基类方法的功能时，就需要用到重写。重写的条件包括： 方法具有相同的名称。 方法具有相同的参数类型和数量。 方法具有相同的返回类型。 重写的基类中被重写的函数必须有virtual修饰。重 写主要在继承关系的类之间发生。 隐藏 隐藏是指派生类的函数屏蔽了与其同名的基类函数。注意只要同名函数，不管参数列表是否相同，基类函数都会被隐藏。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:8","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"类初始化顺序 基类初始化顺序 如果当前类继承自一个或多个基类，它们将按照声明顺序进行初始化，但是在有虚继承和一般继承存在的情况下，优先虚继承。比如虚继承：class MyClass : public Base1, public virtual Base2，此时应当先调用 Base2 的构造函数，再调用 Base1 的构造函数。 成员变量初始化顺序 类的成员变量按照它们在类定义中的声明顺序进行初始化 执行构造函数 在基类和成员变量初始化完成后，执行类的构造函数。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:9","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"类的析构顺序 记住一点即可，类的析构顺序和构造顺序完全相反 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:10","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"析构函数中可以抛出异常吗？ 可以但不建议 由于析构函数常常被自动调用，在析构函数中抛出的异常往往会难以捕获，引发程序非正常退出或未定义行为。另外，我们都知道在容器析构时，会逐个调用容器中的对象析构函数，而某个对象析构时抛出异常还会引起后续的对象无法被析构，导致资源泄漏。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:11","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"深拷贝和浅拷贝 C++中的深拷贝和浅拷贝涉及到对象的复制。 当对象包含指针成员时，这两种拷贝方式的区别变得尤为重要。 浅拷贝 浅拷贝是一种简单的拷贝方式，它仅复制对象的基本类型成员和指针成员的值，而不复制指针所指向的内存。 这可能导致两个对象共享相同的资源，从而引发潜在的问题，如内存泄漏、意外修改共享资源等。 深拷贝 深拷贝不仅复制对象的基本类型成员和指针成员的值，还复制指针所指向的内存。 因此，两个对象不会共享相同的资源，避免了潜在问题。 深拷贝通常需要显式实现拷贝构造函数和赋值运算符重载。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:12","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"C++ 多态实现方式 C++实现多态的方法主要包括虚函数、纯虚函数和模板函数，其中虚函数、纯虚函数实现的多态叫动态多态，模板函数、重载等实现的叫静态多态。 区分静态多态和动态多态的一个方法就是看决定所调用的具体方法是在编译期还是运行时，运行时就叫动态多态。 虚函数、纯虚函数实现多态 虚函数是指在基类中声明的函数，它在派生类中可以被重写。当我们使用基类指针或引用指向派生类对象时，通过虚函数的机制，可以调用到派生类中重写的函数，从而实现多态。 C++ 的多态必须满足两个条件： 必须通过基类的指针或者引用调用虚函数 被调用的函数是虚函数，且必须完成对基类虚函数的重写 class Shape { public: virtual int area() = 0; }; class Rectangle: public Shape { public: int area () { cout \u003c\u003c \"Rectangle class area :\"; return (width * height); } }; class Triangle: public Shape{ public: int area () { cout \u003c\u003c \"Triangle class area :\"; return (width * height / 2); } }; int main() { Shape *shape; Rectangle rec(10,7); Triangle tri(10,5); shape = \u0026rec; shape-\u003earea(); shape = \u0026tri; shape-\u003earea(); return 0; } 模板函数多态 模板函数可以根据传递参数的不同类型，自动生成相应类型的函数代码。模板函数可以用来实现多态。 template \u003cclass T\u003e T GetMax (T a, T b) { return (a\u003eb?a:b); } int main () { int i=5, j=6, k; long l=10, m=5, n; k=GetMax\u003cint\u003e(i,j); n=GetMax\u003clong\u003e(l,m); cout \u003c\u003c k \u003c\u003c endl; cout \u003c\u003c n \u003c\u003c endl; return 0; } 函数重载多态 见函数重载 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:13","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"this 指针 关于 this 是一个指向当前对象的指针。 其实在面向对象的编程语言中，都存在this指针这种机制， Java、C++ 叫 this，而 Python 中叫 self。 在类的成员函数中访问类的成员变量或调用成员函数时，编译器会隐式地将当前对象的地址作为 this 指针传递给成员函数。 因此，this 指针可以用来访问类的成员变量和成员函数，以及在成员函数中引用当前对象。 static 函数不能访问成员变量，因此不可使用 this 指针 static 函数是一种静态成员函数，它与类本身相关，而不是与类的对象相关。因为静态函数没有 this 指针，所以它不能访问任何非静态成员变量。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:14","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"虚函数表 虚函数（Virtual Function）是通过一张虚函数表（Virtual Table）来实现的，简称为V-Table。在这个表中，存放的是一个类的虚函数的地址表，这张表解决了继承、覆盖的问题，保证其真实反应实际的函数。 C++ 对象模型 class Point { public: Point(float xval ); virtual ~Point(); float x() const, static int PointCount(); protected: virtual ostream\u0026 print( ostream \u0026os ) const; float _x; static int _point_count; }; 比如上面这个类，它的对象模型如下： 在上面的示例中，意思就是一个对象在内存中一般由成员变量（非静态）、虚函数表指针(vptr)构成。虚函数表指针指向一个数组，数组的元素就是各个虚函数的地址，通过函数的索引，我们就能直接访问对应的虚函数。 动态多态底层原理 当基类指针或引用指向一个派生类对象时，调用虚函数时，实际上会调用派生类中的虚函数，而不是基类中的虚函数。 在底层，当一个类声明一个虚函数时，编译器会为该类创建一个虚函数表（Virtual Table）。 这个表存储着该类的虚函数指针，这些指针指向实际实现该虚函数的代码地址。 每个对象都包含一个指向该类的虚函数表的指针，这个指针在对象创建时被初始化，通常是作为对象的第一个成员变量。 当调用一个虚函数时，编译器会通过对象的虚函数指针查找到该对象所属的类的虚函数表，并根据函数的索引值（通常是函数在表中的位置，编译时就能确定）来找到对应的虚函数地址。 然后将控制转移到该地址，实际执行该函数的代码。 对于派生类，其虚函数表通常是在基类的虚函数表的基础上扩展而来的。 在派生类中，如果重写了基类的虚函数，那么该函数在派生类的虚函数表中的地址会被更新为指向派生类中实际实现该函数的代码地址。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:15","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"C++ 纯虚函数 纯虚函数是一种在基类中声明但没有实现的虚函数。 它的作用是定义了一种接口，这个接口需要由派生类来实现。（PS: C++ 中没有接口，纯虚函数可以提供类似的功能 包含纯虚函数的类称为抽象类（Abstract Class）。 抽象类仅仅提供了一些接口，但是没有实现具体的功能。作用就是制定各种接口，通过派生类来实现不同的功能，从而实现代码的复用和可扩展性。 另外，抽象类无法实例化，也就是无法创建对象。原因很简单，纯虚函数没有函数体，不是完整的函数，无法调用，也无法为其分配内存空间。 #include \u003ciostream\u003e using namespace std; class Shape { public: // 纯虚函数 virtual void draw() = 0; }; class Circle : public Shape { public: void draw() { cout \u003c\u003c \"画一个圆形\" \u003c\u003c endl; } }; class Square : public Shape { public: void draw() { cout \u003c\u003c \"画一个正方形\" \u003c\u003c endl; } }; int main() { Circle circle; Square square; Shape *pShape1 = \u0026circle; Shape *pShape2 = \u0026square; pShape1-\u003edraw(); pShape2-\u003edraw(); return 0; } /* 在上面的代码中，定义了一个抽象类 Shape，它包含了一个纯虚函数 draw()。 Circle 和 Square 是 Shape 的两个派生类，它们必须实现 draw() 函数，否则它们也会是一个抽象类。 在 main() 函数中，创建了 Circle 和 Square 的实例，并且使用指向基类 Shape 的指针来调用 draw() 函数。 由于 Shape 是一个抽象类，不能创建 Shape 的实例，但是可以使用 Shape 类型指针来指向派生类，从而实现多态。 */ ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:16","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"为什么 C++ 构造函数不能是虚函数？ 从语法层面来说 虚函数的主要目的是实现多态，即允许在派生类中覆盖基类的成员函数。 但是，构造函数负责初始化类的对象，每个类都应该有自己的构造函数。 在派生类中，基类的构造函数会被自动调用，用于初始化基类的成员。因此，构造函数没有被覆盖的必要，不需要使用虚函数来实现多态。 从虚函数表机制来回答 虚函数使用了一种称为虚函数表（vtable）的机制。然而，在调用构造函数时，对象还没有完全创建和初始化，所以虚函数表可能尚未设置。 这意味着在构造函数中使用虚函数表会导致未定义的行为。只有执行完了对象的构造，虚函数表才会被正确的初始化。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:17","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"为什么 C++ 基类析构函数需要是虚函数？ 首先我们需要知道析构函数的作用是什么。 析构函数是进行类的清理工作，比如释放内存、关闭DB链接、关闭Socket等等， 前面我们在介绍虚函数的时候就说到，为实现多态性（C++多态），可以通过基类的指针或引用访问派生类的成员。 也就是说，声明一个基类指针，这个基类指针可以指向派生类对象。 若基类Base的析构函数没有定义为虚函数，当创建一个派生类Derived的对象，并通过基类指针ptr删除它时，只有基类Base的析构函数被调用（因为这里没有多态，构造多态的必要条件就是虚函数）。派生类Derived的析构函数不会被调用，导致资源（这里是resource）没有被释放，从而产生资源泄漏。 若基类Base的析构函数是虚函数，所以当删除ptr时，会首先调用派生类Derived的析构函数，然后调用基类Base的析构函数，这样可以确保对象被正确销毁。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:18","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"为什么默认的析构函数不是虚函数？ 原因是虚函数不同于普通成员函数，当类中有虚成员函数时，类会自动进行一些额外工作。 这些额外的工作包括生成虚函数表和虚表指针，虚表指针指向虚函数表。 每个类都有自己的虚函数表，虚函数表的作用就是保存本类中虚函数的地址，我们可以把虚函数表形象地看成一个数组，这个数组的每个元素存放的就是各个虚函数的地址。 这样一来，就会占用额外的内存，当们定义的类不被其他类继承时，这种内存开销无疑是浪费的。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:19","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"为什么C++的成员模板函数不能是virtual的？ 为什么在C++里面，一个类的成员函数不能既是 template 又是 virtual 的。比如，下面的代码编译会报错： class Animal{ public: template\u003ctypename T\u003e virtual void make_sound(){ //... } }; 因为C的编译与链接模型是\"分离\"的(至少是部分原因吧)。从Unix/C开始，一个C/C程序就可以被分开编译，然后用一个linker链接起来。这种模型有一个问题，就是各个编译单元可能对另一个编译单元一无所知。一个 function template最后到底会被 instantiate 为多少个函数，要等整个程序(所有的编译单元)全部被编译完成才知道。同时，virtual function的实现大多利用了一个\"虚函数表\"（参考: 虚函数机制）的东西，这种实现中，一个类的内存布局(或者说虚函数表的内存布局)需要在这个类编译完成的时候就被完全确定。所以当一个虚拟函数是模板函数时，编译器在编译时无法为其生成一个确定的虚函数表条目，因为模板函数可以有无数个实例。但是编译时无法确定需要调用哪个特定的模板实例。因此，C++标准规定member function 不能既是 template 又是 virtual 的。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:20","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"C++中 sizeof 一个空类大小是多大 class Empty {}; 大多数情况下 sizeof(Empty) = 1，这是因为C++标准要求每个对象都必须具有独一无二的内存地址。 为了满足这一要求，编译器会给每个空类分配一定的空间，通常是1字节。 这样，即使是空类，也能保证每个实例都有不同的地址。 ","date":"2025-03-26","objectID":"/2025/978bdd1/:0:21","tags":null,"title":"C++语法及面向对象特性","uri":"/2025/978bdd1/"},{"categories":["找工作"],"content":"《面试经典150题》做题记录，原题单链接 ","date":"2025-03-24","objectID":"/2025/be012dd/:0:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"数组/字符串 ","date":"2025-03-24","objectID":"/2025/be012dd/:1:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"88.合并两个有序数组 给你两个按 非递减顺序 排列的整数数组 nums1 和 nums2，另有两个整数 m 和 n ，分别表示 nums1 和 nums2 中的元素数目。 请你 合并 nums2 到 nums1 中，使合并后的数组同样按 非递减顺序 排列。 注意：最终，合并后数组不应由函数返回，而是存储在数组 nums1 中。为了应对这种情况，nums1 的初始长度为 m + n，其中前 m 个元素表示应合并的元素，后 n 个元素为 0 ，应忽略。nums2 的长度为 n 。 腾讯PCG-青云 一、二面手撕 class Solution { public: void merge(vector\u003cint\u003e\u0026 nums1, int m, vector\u003cint\u003e\u0026 nums2, int n) { int p1 = m - 1, p2 = n - 1, p = m + n - 1; while (p2 \u003e= 0) { if (p1 \u003e= 0 \u0026\u0026 nums1[p1] \u003e nums2[p2]) { nums1[p -- ] = nums1[p1 -- ]; } else { nums1[p -- ] = nums2[p2 -- ]; } } } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"27.移除元素 给你一个数组 nums 和一个值 val，你需要 原地 移除所有数值等于 val 的元素。元素的顺序可能发生改变。然后返回 nums 中与 val 不同的元素的数量。 假设 nums 中不等于 val 的元素数量为 k，要通过此题，您需要执行以下操作： 更改 nums 数组，使 nums 的前 k 个元素包含不等于 val 的元素。nums 的其余元素和 nums 的大小并不重要。 返回 k。 // 用栈存储去除后的元素 class Solution { public: int removeElement(vector\u003cint\u003e\u0026 nums, int val) { int top = 0; for (int x : nums) { if (x != val) { nums[top ++ ] = x; } } return top; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:2","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"26.删除有序数组中的重复项 给你一个 非严格递增排列 的数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。元素的 相对顺序 应该保持 一致 。然后返回 nums 中唯一元素的个数。 考虑 nums 的唯一元素的数量为 k ，你需要做以下事情确保你的题解可以被通过： 更改数组 nums ，使 nums 的前 k 个元素包含唯一元素，并按照它们最初在 nums 中出现的顺序排列。nums 的其余元素与 nums 的大小不重要。 返回 k 。 class Solution { public: int removeDuplicates(vector\u003cint\u003e\u0026 nums) { int siz = nums.size(); int top = 0; for (int i = 0; i \u003c siz; i ++ ) { int x = nums[i]; if (i \u0026\u0026 x == nums[i - 1]) continue; // 跳过重复数字 nums[top ++ ] = x; } return top; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:3","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"80.删除有序数组中的重复项 II 给你一个有序数组 nums ，请你 原地 删除重复出现的元素，使得出现次数超过两次的元素只出现两次 ，返回删除后数组的新长度。 不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 $O(1)$ 额外空间的条件下完成。 // 用栈模拟, top表示栈顶, 栈存储全部不重复元素 class Solution { public: int removeDuplicates(vector\u003cint\u003e\u0026 nums) { int top = 2; int siz = nums.size(); for (int i = 2; i \u003c siz; i ++ ) { if (nums[i] != nums[top - 2]) nums[top ++ ] = nums[i]; } return min(top, siz); } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:4","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"169.多数元素 给定一个大小为 n 的数组 nums ，返回其中的多数元素。多数元素是指在数组中出现次数 大于 `⌊ n/2 ⌋`` 的元素。 你可以假设数组是非空的，并且给定的数组总是存在多数元素。 // 摩尔投票法 class Solution { public: int majorityElement(vector\u003cint\u003e\u0026 nums) { int x = 0, votes = 0; for (int num: nums) { if (votes == 0) x = num; votes += num == x ? 1 : -1; } return x; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:5","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"189.轮转数组 给定一个整数数组 nums，将数组中的元素向右轮转 k 个位置，其中 k 是非负数。 示例 1: 输入: nums = [1,2,3,4,5,6,7], k = 3 输出: [5,6,7,1,2,3,4] 解释: 向右轮转 1 步: [7,1,2,3,4,5,6] 向右轮转 2 步: [6,7,1,2,3,4,5] 向右轮转 3 步: [5,6,7,1,2,3,4] 示例 2: 输入：nums = [-1,-100,3,99], k = 2 输出：[3,99,-1,-100] 解释: 向右轮转 1 步: [99,-1,-100,3] 向右轮转 2 步: [3,99,-1,-100] class Solution { public: void rotate(vector\u003cint\u003e\u0026 nums, int k) { // 根据 k 计算出每个元素轮转后的位置，然后填入新的 vector 中 int n = nums.size(); k %= n; std::reverse(nums.begin(), nums.end()); std::reverse(nums.begin(), nums.begin() + k); std::reverse(nums.begin() + k, nums.end()); } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:6","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"121.买卖股票的最佳时机 给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。 返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。 状态机模型 class Solution { public: int maxProfit(vector\u003cint\u003e\u0026 prices) { // 买入-\u003e卖出，求区间端点差值的最大值 int ans = 0; int min_price = prices[0]; for (int p : prices) { ans = max(ans, p - min_price); min_price = min(min_price, p); } return ans; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:7","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"122.买卖股票的最佳时机II 给你一个整数数组 prices ，其中 prices[i] 表示某支股票第 i 天的价格。 在每一天，你可以决定是否购买和/或出售股票。你在任何时候 最多 只能持有 一股 股票。你也可以先购买，然后在 同一天 出售。 返回 你能获得的 最大 利润 。 状态划分参考 class Solution { public: int maxProfit(vector\u003cint\u003e\u0026 prices) { // 状态机模型 // f[i][j]表示所有考虑前 i 个步骤，且第 i 个状态是 j(0未持股, 1持股)的集合，属性是最大值 // 对于f[i][j] // 如果i-1步是0，0-\u003e0（未持股且不买入）；0-\u003e1（未持股且买入）； // 如果i-1步是1，1-\u003e0（持股且卖出）；1-\u003e1（持股且不卖出） int n = prices.size(); int INF = 0x3f3f3f3f; vector\u003cvector\u003cint\u003e\u003e f(n + 1, vector\u003cint\u003e(2, 0)); // f[n][2] prices.insert(prices.begin(), 0); f[0][0] = 0, f[0][1] = -INF; for (int i = 1; i \u003c= n; i ++ ) { f[i][0] = max(f[i - 1][0], f[i - 1][1] + prices[i]); f[i][1] = max(f[i - 1][1], f[i - 1][0] - prices[i]); } return max(f[n][0], f[n][1]); } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:8","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"55.跳跃游戏 给你一个非负整数数组 nums ，你最初位于数组的 第一个下标 。数组中的每个元素代表你在该位置可以跳跃的最大长度。 判断你是否能够到达最后一个下标，如果可以，返回 true ；否则，返回 false 。 class Solution { public: bool canJump(vector\u003cint\u003e\u0026 nums) { // 只要跳到了不为 0 的格子上，就一直可以往后跳 // 转为合并区间问题 int mx = 0; for (int i = 0; i \u003c nums.size(); i ++ ) { if (i \u003e mx) return false; mx = max(mx, i + nums[i]); } return true; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:9","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"45.跳跃游戏II 给定一个长度为 n 的 0 索引整数数组 nums。初始位置为 nums[0]。 每个元素 nums[i] 表示从索引 i 向后跳转的最大长度。换句话说，如果你在 nums[i] 处，你可以跳转到任意 nums[i + j] 处: 0 \u003c= j \u003c= nums[i] i + j \u003c n 返回到达 nums[n - 1] 的最小跳跃次数。生成的测试用例可以到达 nums[n - 1]。 class Solution { public: int jump(vector\u003cint\u003e\u0026 nums) { int ans = 0; int cur_right = 0; // 已建造的桥的右端点 int next_right = 0; // 下一座桥的右端点的最大值 for (int i = 0; i + 1 \u003c nums.size(); i ++ ) { // 遍历的过程中，记录下一座桥的最远点 next_right = max(next_right, i + nums[i]); if (i == cur_right) { // 无路可走，必须建桥 cur_right = next_right; // 建桥后，最远可以到达 next_right ans ++ ; } } return ans; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:10","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"274.H指数 给你一个整数数组 citations ，其中 citations[i] 表示研究者的第 i 篇论文被引用的次数。计算并返回该研究者的 h 指数。 根据维基百科上 h 指数的定义：h 代表“高引用次数” ，一名科研人员的 h 指数 是指他（她）至少发表了 h 篇论文，并且 至少 有 h 篇论文被引用次数大于等于 h 。如果 h 有多种可能的值，h 指数 是其中最大的那个。 即：给你一个数组，求一个最大的 $h$，使得数组中有至少 $h$ 个数都大于等于 $h$。 class Solution { public: int hIndex(vector\u003cint\u003e\u0026 citations) { int n = citations.size(); auto check = [\u0026](int mid) -\u003e bool { int n = citations.size(); int res = 0; for (int i = 0; i \u003c n; i ++ ) { if (citations[i] \u003c mid) res ++ ; } return (n - res) \u003e= mid; }; int l = -1, r = n + 1; while (l + 1 \u003c r) { int mid = l + (r - l) / 2; if (check(mid)) l = mid; else r = mid; } return l; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:11","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"380. O(1) 时间插入、删除和获取随机元素 实现RandomizedSet 类： RandomizedSet() 初始化 RandomizedSet 对象 bool insert(int val) 当元素 val 不存在时，向集合中插入该项，并返回 true ；否则，返回 false 。 bool remove(int val) 当元素 val 存在时，从集合中移除该项，并返回 true ；否则，返回 false 。 int getRandom() 随机返回现有集合中的一项（测试用例保证调用此方法时集合中至少存在一个元素）。每个元素应该有 相同的概率 被返回。 你必须实现类的所有函数，并满足每个函数的 平均 时间复杂度为 O(1) 。 // 哈希表 + 变长数组 class RandomizedSet { public: RandomizedSet() { srand((unsigned)time(NULL)); } bool insert(int val) { if (indices.count(val)) { return false; } int index = nums.size(); nums.emplace_back(val); indices[val] = index; return true; } // 主要是这里，删除默认让尾部的值覆盖要删除的元素，然后erase掉指定的值 bool remove(int val) { if (!indices.count(val)) return false; int index = indices[val]; int last = nums.back(); nums[index] = last; indices[last] = index; nums.pop_back(); indices.erase(val); return true; } int getRandom() { int randomIndex = rand() % nums.size(); return nums[randomIndex]; } private: vector\u003cint\u003e nums; unordered_map\u003cint, int\u003e indices; }; /** * Your RandomizedSet object will be instantiated and called as such: * RandomizedSet* obj = new RandomizedSet(); * bool param_1 = obj-\u003einsert(val); * bool param_2 = obj-\u003eremove(val); * int param_3 = obj-\u003egetRandom(); */ ","date":"2025-03-24","objectID":"/2025/be012dd/:1:12","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"238.除自身以外数组的乘积 给你一个整数数组 nums，返回 数组 answer ，其中 answer[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积 。题目数据 保证 数组 nums 之中任意元素的全部前缀元素和后缀的乘积都在 32位 整数范围内。请不要使用除法，且在 O(n) 时间复杂度内完成此题。 class Solution { public: vector\u003cint\u003e productExceptSelf(vector\u003cint\u003e\u0026 nums) { // 用“前缀数组”和“后缀数组”完成 int n = nums.size(); vector\u003cint\u003e pre(n + 1, 1); vector\u003cint\u003e suf(n + 1, 1); vector\u003cint\u003e ans(n); for (int i = 1; i \u003c= n; i ++ ) { pre[i] = pre[i - 1] * nums[i - 1]; } for (int i = n - 1; i \u003e= 1; i -- ) { suf[i] = suf[i + 1] * nums[i]; } for (int i = 0; i \u003c n; i ++ ) { ans[i] = pre[i] * suf[i + 1]; } return ans; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:13","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"134.加油站 在一条环路上有 n 个加油站，其中第 i 个加油站有汽油 gas[i] 升。 你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。 给定两个整数数组 gas 和 cost ，如果你可以按顺序绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1 。如果存在解，则 保证 它是 唯一 的。 本题是特殊做法，通用做法是单调队列，详情见：AcWing.1088 class Solution { public: int canCompleteCircuit(vector\u003cint\u003e\u0026 gas, vector\u003cint\u003e\u0026 cost) { int n = gas.size(); for (int i = 0, j = 0; i \u003c n;) { // 枚举起点 int left = 0; for (j = 0; j \u003c n; j ++ ) { // 枚举走了几步 int k = (i + j) % n; left += gas[k] - cost[k]; if (left \u003c 0) break; // 如果剩余油量不够，则退出枚举，这里有个贪心思想，i~j 之间不用枚举 } if (j == n) return i; i = i + j + 1; } return -1; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:14","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"135.分发糖果 n 个孩子站成一排。给你一个整数数组 ratings 表示每个孩子的评分。 你需要按照以下要求，给这些孩子分发糖果： 每个孩子至少分配到 1 个糖果。 相邻两个孩子中，评分更高的那个会获得更多的糖果。 请你给每个孩子分发糖果，计算并返回需要准备的 最少糖果数目 。 class Solution { public: int candy(vector\u003cint\u003e\u0026 ratings) { int n = ratings.size(); int ans = n; // 每个孩子至少1个糖果 for (int i = 0; i \u003c n; i ++ ) { // 找起始点，满足递增才可以作为起始点 int start = i \u003e 0 \u0026\u0026 ratings[i - 1] \u003c ratings[i] ? i - 1 : i; // 找严格递增段 while (i + 1 \u003c n \u0026\u0026 ratings[i] \u003c ratings[i + 1]) { i ++ ; } // 循环结束时，i 为峰顶 int top = i; // 找严格递减段 while (i + 1 \u003c n \u0026\u0026 ratings[i] \u003e ratings[i + 1]) { i ++ ; } // 循环结束时，i 为谷底 int inc = top - start; // start 到 top 严格递增 int dec = i - top; // top 到 i 严格递减 ans += (inc * (inc - 1) + dec * (dec - 1)) / 2 + max(inc, dec); // 等差数列公式，由于求最少糖果数，所以公差为1 } return ans; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:15","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"42.接雨水 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 class Solution { public: int trap(vector\u003cint\u003e\u0026 height) { int n = height.size(), pre_max = 0, suf_max = 0; // pre_max之前最高的柱子高度，suf_max之后最高的柱子高度 // 注意到下标 i 处能接的雨水量由 pre_max[i] 和 suf_max[i] 中的最小值决定。 int left = 0, right = n - 1, res = 0; while (left \u003c right) { pre_max = max(pre_max, height[left]); // 维护pre_max suf_max = max(suf_max, height[right]); // 维护suf_max if (pre_max \u003c suf_max) { res += pre_max - height[left]; left ++ ; } else { res += suf_max - height[right]; right -- ; } } return res; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:16","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"13.罗马数字转整数 给你一个罗马数字，将其转换为整数 unordered_map\u003cchar, int\u003e ROMAN = { {'I', 1}, {'V', 5}, {'X', 10}, {'L', 50}, {'C', 100}, {'D', 500}, {'M', 1000}, }; class Solution { public: int romanToInt(string s) { int ans = 0; for (int i = 0; i + 1 \u003c s.size(); i ++ ) { int x = ROMAN[s[i]], y = ROMAN[s[i + 1]]; ans += x \u003c y ? -x : x; } return ans + ROMAN[s.back()]; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:17","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"12.整数转罗马数字 给你一个整数，将其转为罗马数字 class Solution { static constexpr string R[4][10] = { {\"\", \"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\", \"VIII\", \"IX\"}, // 个位 {\"\", \"X\", \"XX\", \"XXX\", \"XL\", \"L\", \"LX\", \"LXX\", \"LXXX\", \"XC\"}, // 十位 {\"\", \"C\", \"CC\", \"CCC\", \"CD\", \"D\", \"DC\", \"DCC\", \"DCCC\", \"CM\"}, // 百位 {\"\", \"M\", \"MM\", \"MMM\"}, // 千位 }; public: string intToRoman(int num) { return R[3][num / 1000] + R[2][num / 100 % 10] + R[1][num / 10 % 10] + R[0][num % 10]; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:18","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"58.最后一个单词的长度 给你一个字符串 s，由若干单词组成，单词前后用一些空格字符隔开。返回字符串中 最后一个 单词的长度。 单词 是指仅由字母组成、不包含任何空格字符的最大子字符串。 class Solution { public: int lengthOfLastWord(string s) { int i = s.length() - 1; while (s[i] == ' ' \u0026\u0026 i \u003e 0) i -- ; int j = i - 1; while (j \u003e= 0 \u0026\u0026 s[j] != ' ') j -- ; return i - j; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:19","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"14.最长公共前缀 编写一个函数来查找字符串数组中的最长公共前缀。 如果不存在公共前缀，返回空字符串 \"\"。 class Solution { public: string longestCommonPrefix(vector\u003cstring\u003e\u0026 strs) { string\u0026 s0 = strs[0]; for (int j = 0; j \u003c s0.size(); j ++ ) { for (string\u0026 s : strs) { if (j == s.size() || s[j] != s0[j]) { return s0.substr(0, j); } } } return s0; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:20","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"151.反转字符串中的单词 给你一个字符串 s ，请你反转字符串中 单词 的顺序。 单词 是由非空格字符组成的字符串。s 中使用至少一个空格将字符串中的 单词 分隔开。 返回 单词 顺序颠倒且 单词 之间用单个空格连接的结果字符串。 注意：输入字符串 s 中可能会存在前导空格、尾随空格或者单词间的多个空格。返回的结果字符串中，单词间应当仅用单个空格分隔，且不包含任何额外的空格。 class Solution { public String reverseWords(String s) { s = s.trim(); // 删除首尾空格 int j = s.length() - 1, i = j; StringBuilder res = new StringBuilder(); while (i \u003e= 0) { while (i \u003e= 0 \u0026\u0026 s.charAt(i) != ' ') i--; // 搜索首个空格 res.append(s.substring(i + 1, j + 1) + \" \"); // 添加单词 while (i \u003e= 0 \u0026\u0026 s.charAt(i) == ' ') i--; // 跳过单词间空格 j = i; // j 指向下个单词的尾字符 } return res.toString().trim(); // 转化为字符串并返回 } } ","date":"2025-03-24","objectID":"/2025/be012dd/:1:21","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"28.找出字符串中第一个匹配项的下标 给你两个字符串 haystack 和 needle ，请你在 haystack 字符串中找出 needle 字符串的第一个匹配项的下标（下标从 0 开始）。如果 needle 不是 haystack 的一部分，则返回 -1 。 class Solution { public: int strStr(string haystack, string needle) { int m = haystack.size(), n = needle.size(); for (int i = 0; i \u003c haystack.size(); i ++ ) { if (i + n \u003e m) return -1; // 判断两个区间的值是否相同 if (haystack.substr(i, n) == needle) return i; } return -1; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:1:22","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"双指针 ","date":"2025-03-24","objectID":"/2025/be012dd/:2:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"11.盛水最多的容器 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。 找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 返回容器可以储存的最大水量。 说明：你不能倾斜容器。 // 思路和接雨水类似 class Solution { public: int maxArea(vector\u003cint\u003e\u0026 height) { int n = height.size(); int left = 0, right = n - 1; int ans = 0; while (left \u003c right) { ans = max(ans, min(height[left], height[right]) * (right - left)); if (height[left] \u003c height[right]) { left ++ ; } else { right -- ; } } return ans; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:2:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"15.三数之和 🔗 给你一个整数数组 nums ，判断是否存在三元组 [nums[i], nums[j], nums[k]] 满足 i != j、i != k 且 j != k ，同时还满足 nums[i] + nums[j] + nums[k] == 0 。请你返回所有和为 0 且不重复的三元组。 注意：答案中不可以包含重复的三元组。 class Solution { public: vector\u003cvector\u003cint\u003e\u003e threeSum(vector\u003cint\u003e\u0026 nums) { int n = nums.size(); vector\u003cvector\u003cint\u003e\u003e ans; sort(nums.begin(), nums.end()); // 本质上是过滤所有不可能的情况 for (int i = 0; i \u003c n; i ++ ) { if (i \u0026\u0026 nums[i] == nums[i - 1]) continue; for (int j = i + 1, k = n - 1; j \u003c k; j ++ ) { if (j \u003e i + 1 \u0026\u0026 nums[j] == nums[j - 1]) continue; while (j \u003c k - 1 \u0026\u0026 nums[i] + nums[j] + nums[k - 1] \u003e= 0) k -- ; if (nums[i] + nums[j] + nums[k] == 0) ans.push_back({nums[i], nums[j], nums[k]}); } } return ans; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:2:2","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"滑动窗口 ","date":"2025-03-24","objectID":"/2025/be012dd/:3:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"209.长度最小的子数组 给定一个含有 n 个正整数的数组和一个正整数 target 。 找出该数组中满足其总和大于等于 target 的长度最小的 子数组 [nums_l, nums_l+1, ..., nums_r-1, nums_r] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。 class Solution { public: int minSubArrayLen(int target, vector\u003cint\u003e\u0026 nums) { // 预处理前缀和 int n = nums.size(); vector\u003cint\u003e s(n + 1, 0); for (int i = 1; i \u003c= n; i ++ ) s[i] = s[i - 1] + nums[i - 1]; // 枚举右指针，然后移动左指针 int l = 1; // l 不用回头 int ans = n + 1; for (int r = 1; r \u003c= n; r ++ ) { while ((s[r] - s[l - 1]) \u003e= target) { ans = min(ans, r - l + 1); l ++ ; } } return ans \u003c= n ? ans : 0; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:3:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"3.无重复字符的最长子串 给定一个字符串 s ，请你找出其中不含有重复字符的 最长 子串 的长度。 class Solution { public: int lengthOfLongestSubstring(string s) { // 滑动窗口, unordered_map\u003cchar, int\u003e heap; // 记录每个字符出现过多少次 int res = 0; int left = 0; for (int i = 0; i \u003c s.size(); i ++ ) { heap[s[i]] ++ ; while (heap[s[i]] \u003e 1) { heap[s[left]] -- ; left ++ ; } res = max(res, i - left + 1); } return res; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:3:2","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"76.最小覆盖子串 给你一个字符串 s 、一个字符串 t 。返回 s 中涵盖 t 所有字符的最小子串。如果 s 中不存在涵盖 t 所有字符的子串，则返回空字符串 \"\" 。 class Solution { bool is_covered(int cnt_s[], int cnt_t[]) { for (int i = 'A'; i \u003c= 'Z'; i ++ ) { if (cnt_s[i] \u003c cnt_t[i]) { return false; } } for (int i = 'a'; i \u003c= 'z'; i ++ ) { if (cnt_s[i] \u003c cnt_t[i]) { return false; } } return true; } public: string minWindow(string s, string t) { // 不定长滑动窗口 int cnt_s[128]{}; int cnt_t[128]{}; int min_left = -1; int min_right = s.size(); int ans = s.size(); for (int i = 0; i \u003c t.size(); i ++ ) cnt_t[t[i]] ++ ; for (int i = 0, left = 0; i \u003c s.size(); i ++ ) { cnt_s[s[i]] ++ ; // 已经全覆盖了，右移左端点 while (is_covered(cnt_s, cnt_t)) { if (i - left \u003c min_right - min_left) { min_left = left; min_right = i; } cnt_s[s[left]] -- ; left ++ ; } } if (min_left \u003e= 0) { return s.substr(min_left, min_right - min_left + 1); } return \"\"; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:3:3","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"矩阵 ","date":"2025-03-24","objectID":"/2025/be012dd/:4:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"54.螺旋矩阵 给你一个 m 行 n 列的矩阵 matrix ，请按照 顺时针螺旋顺序 ，返回矩阵中的所有元素。 class Solution { public: vector\u003cint\u003e spiralOrder(vector\u003cvector\u003cint\u003e\u003e\u0026 matrix) { vector\u003cint\u003e res; int m = matrix.size(); int n = matrix[0].size(); vector\u003cvector\u003cint\u003e\u003e st(m + 10, vector\u003cint\u003e(n + 10, 0)); // 用坐标偏移法模拟 int dx[] = {0, 1, 0, -1}, dy[] = {1, 0, -1, 0}; // 右，下，左，上 int step = 0; // 0, 1, 2, 3 // m 行，n 列 st[0][0] = 1; res.push_back(matrix[0][0]); int cnt = m * n - 1; int i = 0, j = 0; while (cnt -- ) { int x = i + dx[step]; int y = j + dy[step]; // 判断将要走的点有没有越过边界 if (x \u003e= m || x \u003c 0 || y \u003c 0 || y \u003e= n || st[x][y] == 1) { step = (step + 1) % 4; x = i + dx[step]; y = j + dy[step]; } res.push_back(matrix[x][y]); i = x; j = y; } return res; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:4:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"48.旋转图像 给定一个 n × n 的二维矩阵 matrix 表示一个图像。请你将图像顺时针旋转 90 度。 你必须在 原地 旋转图像，这意味着你需要直接修改输入的二维矩阵。请不要 使用另一个矩阵来旋转图像。 class Solution { public: void rotate(vector\u003cvector\u003cint\u003e\u003e\u0026 matrix) { // 先上下颠倒，再矩阵转置 int m = matrix.size(); int n = matrix[0].size(); for (int i = 0; i \u003c m / 2; i ++ ) { for (int j = 0; j \u003c n; j ++ ) { swap(matrix[i][j], matrix[m - 1 - i][j]); } } // 矩阵转置通用公式 for (int i = 0; i \u003c m; i ++ ) { for (int j = i + 1; j \u003c n; j ++ ) { swap(matrix[i][j], matrix[j][i]); } } } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:4:2","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"哈希表 ","date":"2025-03-24","objectID":"/2025/be012dd/:5:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"128.最长连续序列 给定一个未排序的整数数组 nums ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。请你设计并实现时间复杂度为 O(n) 的算法解决此问题。 class Solution { public: int longestConsecutive(vector\u003cint\u003e\u0026 nums) { int ans = 0; unordered_set\u003cint\u003e st(nums.begin(), nums.end()); // 把 nums 转为哈希集合 for (int x : st) { // 遍历哈希集合 if (st.contains(x - 1)) { continue; // 如果 x-1 在哈希集合中，则不以 x 为起点，因为 x-1 为起点计算出来的连续序列一定更长 } // x 是序列的起点 int y = x + 1; while (st.contains(y)) { // 不断查找下一个数是否在哈希集合中 y ++ ; } ans = max(ans, y - x); // 从 x 到 y - 1 一共 y - x 个数 } return ans; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:5:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"链表 ","date":"2025-03-24","objectID":"/2025/be012dd/:6:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"92.反转链表 II 给你单链表的头指针 head 和两个整数 left 和 right ，其中 left \u003c= right 。请你反转从位置 left 到位置 right 的链表节点，返回 反转后的链表 。 /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) {} * ListNode(int x) : val(x), next(nullptr) {} * ListNode(int x, ListNode *next) : val(x), next(next) {} * }; */ class Solution { public: ListNode* reverseBetween(ListNode* head, int left, int right) { ListNode dummy{0, head}; ListNode* p0 = \u0026dummy; // 首先维护p0指针，p0指针是待处理段的前一个指针（哨兵节点） for (int i = 0; i \u003c left - 1; i ++ ) { p0 = p0-\u003enext; } ListNode* pre = nullptr; ListNode* cur = p0-\u003enext; for (int i = 0; i \u003c right - left + 1; i ++ ) { ListNode* nxt = cur-\u003enext; cur-\u003enext = pre; pre = cur; cur = nxt; } p0-\u003enext-\u003enext = cur; p0-\u003enext = pre; return dummy.next; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:6:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"25.K 个一组翻转链表 🔗 给你链表的头节点 head ，每 k 个节点一组进行翻转，请你返回修改后的链表。 k 是一个正整数，它的值小于或等于链表的长度。如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。 你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。 /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) {} * ListNode(int x) : val(x), next(nullptr) {} * ListNode(int x, ListNode *next) : val(x), next(next) {} * }; */ class Solution { public: ListNode* reverseKGroup(ListNode* head, int k) { // 统计节点个数 int n = 0; for (ListNode* cur = head; cur; cur = cur-\u003enext) n ++ ; ListNode dummy(0, head); // 哨兵节点，哨兵的下一个节点是head ListNode* p0 = \u0026dummy; ListNode* pre = nullptr; ListNode* cur = head; // k 个一组进行处理 for (; n \u003e= k; n -= k) { // 每组内部就是 反转链表II for (int i = 0; i \u003c k; i ++ ) { ListNode* nxt = cur-\u003enext; // cur不断往右走的同时，维护pre指针和nxt指针 cur-\u003enext = pre; pre = cur; cur = nxt; } // 处理p0指针，p0指针主要是指向每一段被处理链表的哨兵节点（前一个节点） ListNode* nxt = p0-\u003enext; p0-\u003enext-\u003enext = cur; p0-\u003enext = pre; p0 = nxt; } return dummy.next; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:6:2","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"19.删除链表的倒数第 N 个结点 🔗 给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。 /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) {} * ListNode(int x) : val(x), next(nullptr) {} * ListNode(int x, ListNode *next) : val(x), next(next) {} * }; */ class Solution { public: ListNode* removeNthFromEnd(ListNode* head, int n) { ListNode dummy{0, head}; ListNode* left = \u0026dummy; ListNode* right = \u0026dummy; // 左右指针都先往右走n步 while (n -- ) { left = left-\u003enext; right = right-\u003enext; } // 再同时走一段距离，让右指针指向最后一个节点 while (right-\u003enext) { left = left-\u003enext; right = right-\u003enext; } // 此时 left 下一个节点就是倒数第 n 个节点 ListNode* nxt = left-\u003enext; left-\u003enext = left-\u003enext-\u003enext; delete nxt; return dummy.next; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:6:3","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"82.删除排序链表中的重复元素II 🔗 给定一个已排序的链表的头 head ， 删除原始链表中所有重复数字的节点，只留下不同的数字 。返回 已排序的链表 /** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) {} * ListNode(int x) : val(x), next(nullptr) {} * ListNode(int x, ListNode *next) : val(x), next(next) {} * }; */ class Solution { public: ListNode* deleteDuplicates(ListNode* head) { if (head == nullptr) return nullptr; ListNode dummy{0, head}; ListNode* cur = \u0026dummy; while (cur-\u003enext \u0026\u0026 cur-\u003enext-\u003enext) { if (cur-\u003enext-\u003eval == cur-\u003enext-\u003enext-\u003eval) { int x = cur-\u003enext-\u003eval; while (cur-\u003enext \u0026\u0026 cur-\u003enext-\u003eval == x) { cur-\u003enext = cur-\u003enext-\u003enext; } } else { cur = cur-\u003enext; } } return dummy.next; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:6:4","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"146.LRU 缓存 🔗 请你设计并实现一个满足 LRU (最近最少使用) 缓存 约束的数据结构。 实现 LRUCache 类： LRUCache(int capacity) 以 正整数 作为容量 capacity 初始化 LRU 缓存 int get(int key) 如果关键字 key 存在于缓存中，则返回关键字的值，否则返回 -1 。 void put(int key, int value) 如果关键字 key 已经存在，则变更其数据值 value ；如果不存在，则向缓存中插入该组 key-value 。如果插入操作导致关键字数量超过 capacity ，则应该 逐出 最久未使用的关键字。 函数 get 和 put 必须以 O(1) 的平均时间复杂度运行。 用图巧记 struct Node { int key; int value; Node* prev; Node* next; Node(int k = 0, int v = 0) : key(k), value(v) {} }; class LRUCache { private: int capacity; Node* dummy; unordered_map\u003cint, Node*\u003e key_to_node; void remove(Node* x) { x-\u003eprev-\u003enext = x-\u003enext; x-\u003enext-\u003eprev = x-\u003eprev; } void push_front(Node* x) { x-\u003enext = dummy-\u003enext; x-\u003eprev = dummy; x-\u003eprev-\u003enext = x; x-\u003enext-\u003eprev = x; } Node* get_node(int key) { auto it = key_to_node.find(key); if (it == key_to_node.end()) return nullptr; Node* node = key_to_node[key]; remove(node); push_front(node); return node; } public: LRUCache(int capacity) : capacity(capacity), dummy(new Node()) { dummy-\u003eprev = dummy; dummy-\u003enext = dummy; } int get(int key) { Node* node = get_node(key); return node ? node-\u003evalue : -1; } void put(int key, int value) { Node* node = get_node(key); if (node) { node-\u003evalue = value; return; } node = new Node(key, value); key_to_node[key] = node; push_front(node); if (key_to_node.size() \u003e capacity) { Node* back_node = dummy-\u003eprev; key_to_node.erase(back_node-\u003ekey); remove(back_node); delete back_node; } } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:6:5","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"堆 ","date":"2025-03-24","objectID":"/2025/be012dd/:7:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"215.数组中的第K个最大元素 🔗 给定整数数组 nums 和整数 k，请返回数组中第 k 个最大的元素。 请注意，你需要找的是数组排序后的第 k 个最大的元素，而不是第 k 个不同的元素。 你必须设计并实现时间复杂度为 O(n) 的算法解决此问题。 class Solution { public: int findKthLargest(vector\u003cint\u003e\u0026 nums, int target) { // 第 K 个最大元素 // 1 2 3 4，n=4, K=2，则 auto quick_select = [\u0026](this auto\u0026\u0026quick_select, int l, int r, int k) { if (l \u003e= r) return nums[l]; int x = nums[(l + r) / 2], i = l - 1, j = r + 1; while (i \u003c j) { do i ++ ; while (nums[i] \u003c x); do j -- ; while (nums[j] \u003e x); if (i \u003c j) swap(nums[i], nums[j]); } int sl = j - l + 1; if (k \u003c= sl) return quick_select(l, j, k); return quick_select(j + 1, r, k - sl); }; int n = nums.size(); return quick_select(0, n - 1, n - target + 1); } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:7:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"295.数据流的中位数 🔗 中位数是有序整数列表中的中间值。如果列表的大小是偶数，则没有中间值，中位数是两个中间值的平均值。 例如 arr = [2,3,4] 的中位数是 3 。 例如 arr = [2,3] 的中位数是 (2 + 3) / 2 = 2.5 。 实现 MedianFinder 类: MedianFinder() 初始化 MedianFinder 对象。 void addNum(int num) 将数据流中的整数 num 添加到数据结构中。 double findMedian() 返回到目前为止所有元素的中位数。与实际答案相差 10^-5 以内的答案将被接受。 class MedianFinder { priority_queue\u003cint, vector\u003cint\u003e, greater\u003cint\u003e\u003e up; // 小根堆 priority_queue\u003cint\u003e down; // 大根堆 int siz; public: MedianFinder() { siz = 0; // 记录对顶堆中元素大小 } // 对顶堆维护动态中位数 void addNum(int num) { if (down.empty() || num \u003c= down.top()) down.push(num); else up.push(num); siz ++ ; if (down.size() \u003e up.size() + 1) up.push(down.top()), down.pop(); if (up.size() \u003e down.size()) down.push(up.top()), up.pop(); } double findMedian() { if (siz % 2) return down.top(); return (up.top() + down.top()) / 2.0; } }; /** * Your MedianFinder object will be instantiated and called as such: * MedianFinder* obj = new MedianFinder(); * obj-\u003eaddNum(num); * double param_2 = obj-\u003efindMedian(); */ ","date":"2025-03-24","objectID":"/2025/be012dd/:7:2","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"一维动态规划 ","date":"2025-03-24","objectID":"/2025/be012dd/:8:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"139.单词拆分 🔗 给你一个字符串 s 和一个字符串列表 wordDict 作为字典。如果可以利用字典中出现的一个或多个单词拼接出 s 则返回 true。 注意：不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。 这里用到了字符串哈希来优化 这里有一个隐藏的细节是 “秦九韶算法”，即哈希值的维护。如果不这样写则需要维护一个 p 数组，来进行 a*P^3 + b*P^2 class Solution { public: bool wordBreak(string s, vector\u003cstring\u003e\u0026 wordDict) { typedef unsigned long long ULL; // 用ULL表示是因为为了对 2^64 取模 unordered_set\u003cULL\u003e hash; const int P = 131; // P进制的经验值，也可以取 13331，可以认为99%的概率不会哈希冲突 for (auto\u0026 s : wordDict) { ULL h = 0; for (auto c : s) { h = h * P + c; // 将词表中的每个词映射至 P 进制，秦九韶算法写法； } hash.insert(h); } int n = s.size(); vector\u003cbool\u003e f(n + 1); s = \" \" + s; f[0] = true; // f[i]表示单词 s 的前 i 个字符能否由 wordDict 中的单词组成，其中边界 f[0] = true for (int i = 0; i \u003c n; i ++ ) { if (f[i]) { // 如果 f[i] = true 并且 s[i + 1:j] 也在 wordDict 中，则 f[j] = true ULL h = 0; for (int j = i + 1; j \u003c= n; j ++ ) { // 查询 s[i + 1:j] 中所有的字符串是否在 wordDict 中出现过 h = h * P + s[j]; if (hash.count(h)) f[j] = true; } } } return f[n]; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:8:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"多维动态规划 ","date":"2025-03-24","objectID":"/2025/be012dd/:9:0","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"120. 三角形最小路径和 🔗 给定一个三角形 triangle ，找出自顶向下的最小路径和。 每一步只能移动到下一行中相邻的结点上。相邻的结点 在这里指的是 下标 与 上一层结点下标 相同或者等于 上一层结点下标 + 1 的两个结点。也就是说，如果正位于当前行的下标 i ，那么下一步可以移动到下一行的下标 i 或 i + 1 。 class Solution { public: int minimumTotal(vector\u003cvector\u003cint\u003e\u003e\u0026 grid) { int n = grid.size(); const int INF = 0x3f3f3f3f; vector\u003cvector\u003cint\u003e\u003e f(n + 1, vector\u003cint\u003e(n + 1, -1)); auto dfs = [\u0026](this auto\u0026\u0026 dfs, int x, int y) { int n = grid.size(); if (x == n - 1) return f[x][y] = grid[x][y]; // 不能继续走，f[x][y]的值就是当前点的值 if (f[x][y] != -1) return f[x][y]; int res = INF; if (x \u003e= 0 \u0026\u0026 x \u003c n \u0026\u0026 y \u003e= 0 \u0026\u0026 y \u003c= x) { res = min(dfs(x + 1, y), dfs(x + 1, y + 1)) + grid[x][y]; } return f[x][y] = res; }; return dfs(0, 0); } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:9:1","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"221.最大正方形 🔗 在一个由 ‘0’ 和 ‘1’ 组成的二维矩阵内，找到只包含 ‘1’ 的最大正方形，并返回其面积。 class Solution { static const int N = 310; int f[N][N]; public: int maximalSquare(vector\u003cvector\u003cchar\u003e\u003e\u0026 matrix) { int m = matrix.size(), n = matrix[0].size(); memset(f, 0, sizeof f); int a = 0; for (int i = 0; i \u003c m; i ++ ) { for (int j = 0; j \u003c n; j ++ ) { if (matrix[i][j] == '0') continue; if (i == 0 || j == 0) f[i][j] = 1; // 边长最大只能为1 else { // f[i][j]表示以 (i,j) 为右下角的，最大正方形的边长 f[i][j] = min(min(f[i][j - 1], f[i - 1][j]), f[i - 1][j - 1]) + 1; } a = max(a, f[i][j]); } } return a * a; } }; ","date":"2025-03-24","objectID":"/2025/be012dd/:9:2","tags":["算法"],"title":"《面试经典150题》","uri":"/2025/be012dd/"},{"categories":["找工作"],"content":"《现代C++并发编程教程》 —— C++并发编程学习笔记（三） ","date":"2025-03-24","objectID":"/2025/80f2e62/:0:0","tags":["C++"],"title":"并发编程（三）","uri":"/2025/80f2e62/"},{"categories":["找工作"],"content":"原子操作 这里只简单介绍 std::atomic\u003cbool\u003e（包含在 \u003catomic\u003e 中），最基本的整数原子类型。虽然同样不可复制不可移动，但可以使用非原子的 bool 类型进行构造，初始化为 true 或 false，并且能从非原子的 bool 对象赋值给 std::atomic\u003cbool\u003e： std::atomic\u003cbool\u003e b{ true }; b = false; ","date":"2025-03-24","objectID":"/2025/80f2e62/:1:0","tags":["C++"],"title":"并发编程（三）","uri":"/2025/80f2e62/"},{"categories":["找工作"],"content":"线程池 抽象的来说，可以当做是一个池子中存放了一堆线程，故称作线程池。简而言之，线程池是指代一组预先创建的、可以复用的线程集合。这些线程由线程池管理，用于执行多个任务而无需频繁地创建和销毁线程。 这是一个典型的线程池结构。线程池包含一个任务队列，当有新任务加入时，调度器会将任务分配给线程池中的空闲线程进行执行。线程在执行完任务后会进入休眠状态，等待调度器的下一次唤醒。当有新的任务加入队列，并且有线程处于休眠状态时，调度器会唤醒休眠的线程，并分配新的任务给它们执行。线程执行完新任务后，会再次进入休眠状态，直到有新的任务到来，调度器才可能会再次唤醒它们。 图中线程1 就是被调度器分配了任务1，执行完毕后休眠，然而新任务的到来让调度器再次将它唤醒，去执行任务6，执行完毕后继续休眠。 使用线程池的益处我们已经加粗了，然而这其实并不是“线程池”独有的，任何创建和销毁存在较大开销的设施，都可以进行所谓的“池化”。 常见的还有：套接字连接池、数据库连接池、内存池、对象池。 下面简单介绍下常用的线程池。 ","date":"2025-03-24","objectID":"/2025/80f2e62/:2:0","tags":["C++"],"title":"并发编程（三）","uri":"/2025/80f2e62/"},{"categories":["找工作"],"content":"boost::asio::thread_pool boost::asio::thread_pool 是 Boost.Asio 库提供的一种线程池实现。 Asio 是一个跨平台的 C++ 库，用于网络和低级 I/O 编程，使用 现代C++ 方法为开发人员提供一致的异步模型。 使用方法： 创建线程池对象，指定或让 Asio 自动决定线程数量。 提交任务：通过 boost::asio::post 函数模板提交任务到线程池中。 阻塞，直到池中的线程完成任务。 #include \u003cboost/asio.hpp\u003e #include \u003ciostream\u003e std::mutex m; void print_task(int n) { std::lock_guard\u003cstd::mutex\u003e lc{ m }; std::cout \u003c\u003c \"Task \" \u003c\u003c n \u003c\u003c \" is running on thr: \" \u003c\u003c std::this_thread::get_id() \u003c\u003c '\\n'; } int main() { boost::asio::thread_pool pool{ 4 }; // 创建一个包含 4 个线程的线程池 for (int i = 0; i \u003c 10; ++i) { boost::asio::post(pool, [i] { print_task(i); }); } pool.join(); // 等待所有任务执行完成 } 详情见 boost/asio 的使用，这里不再展开。 ","date":"2025-03-24","objectID":"/2025/80f2e62/:2:1","tags":["C++"],"title":"并发编程（三）","uri":"/2025/80f2e62/"},{"categories":["找工作"],"content":"实现线程池 实现一个普通的能够满足日常开发需求的线程池实际上非常简单，只需要不到一百行代码。其实绝大部分开发者使用线程池，只是为了不重复多次创建线程罢了。所以只需要一个提供一个外部接口，可以传入任务到任务队列，然后安排线程去执行。无非是使用条件变量、互斥量、原子标志位，这些东西，就足够编写一个满足绝大部分业务需求的线程池。 我们先编写一个最基础的线程池，首先确定它的数据成员： class ThreadPool { std::mutex mutex_; // 用于保护共享资源（如任务队列）在多线程环境中的访问，避免数据竞争。 std::condition_variable cv_; // 用于线程间的同步，允许线程等待特定条件（如新任务加入队列）并在条件满足时唤醒线程。 std::atomic\u003cbool\u003e stop_; // 指示线程池是否停止。 std::atomic\u003cstd::size_t\u003e num_threads_; // 表示线程池中的线程数量。 std::queue\u003cTask\u003e tasks_; // 任务队列，存储等待执行的任务，任务按提交顺序执行。 std::vector\u003cstd::thread\u003e pool_; // 线程容器，存储管理线程对象，每个线程从任务队列中获取任务并执行。 }; 标头依赖： #include \u003ciostream\u003e #include \u003cthread\u003e #include \u003cmutex\u003e #include \u003ccondition_variable\u003e #include \u003cfuture\u003e #include \u003catomic\u003e #include \u003cqueue\u003e #include \u003cvector\u003e #include \u003csyncstream\u003e #include \u003cfunctional\u003e 提供构造析构函数，以及一些外部接口：submit()、start()、stop()、join()，也就完成了： inline std::size_t default_thread_pool_size()noexcept { std::size_t num_threads = std::thread::hardware_concurrency() * 2; num_threads = num_threads == 0 ? 2 : num_threads; return num_threads; } class ThreadPool { private: std::mutex mutex_; std::condition_variable cv_; std::atomic\u003cbool\u003e stop_; std::atomic\u003cstd::size_t\u003e num_threads_; std::queue\u003cTask\u003e tasks_; std::vector\u003cstd::thread\u003e pool_; public: using Task = std::packaged_task\u003cvoid()\u003e; ThreadPool(const ThreadPool\u0026) = delete; ThreadPool\u0026 operator=(const ThreadPool\u0026) = delete; ThreadPool(std::size_t num_thread = default_thread_pool_size()) : stop_{ false }, num_threads_{ num_thread } { start(); } ~ThreadPool() { stop(); } void stop() { stop_.store(true); cv_.notify_all(); for (auto\u0026 thread : pool_) { if (thread.joinable()) { thread.join(); } } pool_.clear(); } template\u003ctypename F, typename... Args\u003e std::future\u003cstd::invoke_result_t\u003cstd::decay_t\u003cF\u003e, std::decay_t\u003cArgs\u003e...\u003e\u003e submit(F\u0026\u0026 f, Args\u0026\u0026...args) { using RetType = std::invoke_result_t\u003cstd::decay_t\u003cF\u003e, std::decay_t\u003cArgs\u003e...\u003e; if (stop_.load()) { throw std::runtime_error(\"ThreadPool is stopped\"); } auto task = std::make_shared\u003cstd::packaged_task\u003cRetType()\u003e\u003e( std::bind(std::forward\u003cF\u003e(f), std::forward\u003cArgs\u003e(args)...)); std::future\u003cRetType\u003e ret = task-\u003eget_future(); { std::lock_guard\u003cstd::mutex\u003e lc{ mutex_ }; tasks_.emplace([task] {(*task)(); }); } cv_.notify_one(); return ret; } void start() { for (std::size_t i = 0; i \u003c num_threads_; ++i) { pool_.emplace_back([this] { while (!stop_) { Task task; { std::unique_lock\u003cstd::mutex\u003e lc{ mutex_ }; cv_.wait(lc, [this] {return stop_ || !tasks_.empty(); }); if (tasks_.empty()) return; task = std::move(tasks_.front()); tasks_.pop(); } task(); } }); } } }; 测试 demo: int main() { ThreadPool pool{ 4 }; // 创建一个有 4 个线程的线程池 std::vector\u003cstd::future\u003cint\u003e\u003e futures; // future 集合，获取返回值 for (int i = 0; i \u003c 10; ++i) { futures.emplace_back(pool.submit(print_task, i)); } for (int i = 0; i \u003c 10; ++i) { futures.emplace_back(pool.submit(print_task2, i)); } int sum = 0; for (auto\u0026 future : futures) { sum += future.get(); // get() 成员函数 阻塞到任务执行完毕，获取返回值 } std::cout \u003c\u003c \"sum: \" \u003c\u003c sum \u003c\u003c '\\n'; } // 析构自动 stop() 可能的运行结果： Task 0 is running on thr: 6900 Task 1 is running on thr: 36304 Task 5 is running on thr: 36304 Task 3 is running on thr: 6900 Task 7 is running on thr: 6900 Task 2 is running on thr: 29376 Task 6 is running on thr: 36304 Task 4 is running on thr: 31416 🐢🐢🐢 1 🐉🐉🐉 Task 9 is running on thr: 29376 🐢🐢🐢 0 🐉🐉🐉 Task 8 is running on thr: 6900 🐢🐢🐢 2 🐉🐉🐉 🐢🐢🐢 6 🐉🐉🐉 🐢🐢🐢 4 🐉🐉🐉 🐢🐢🐢 5 🐉🐉🐉 🐢🐢🐢 3 🐉🐉🐉 🐢🐢🐢 7 🐉🐉🐉 🐢🐢🐢 8 🐉🐉🐉 🐢🐢🐢 9 🐉🐉🐉 sum: 90 它支持任意可调用类型，当然也包括非静态成员函数。我们使用了 std::decay_t，所以参数的传递其实是按值复制，而不是引用传递，这一点和大部分库的设计一致。示例如下： struct X { void f(const int\u0026 n) const { std::osyncstream{ std::cout } \u003c\u003c \u0026n \u003c\u003c '\\n'; } }; int main() { ThreadPool pool{ 4 }; // 创建一个有 4 个线程的线程池 X x; int n = 6; std::cout \u003c\u003c \u0026n \u003c\u003c '\\n'; auto t = pool.submit(\u0026X::f, \u0026x, n); // 默认复制，地址不同 auto t2 = pool.submit(\u0026X::f, \u0026x, std::ref(n)); t.wait(); t2.wait(); } // 析构自动 stop() 我们的线程池的 submit 成员函数在传递参数的行为上，与先前介绍的 std::thread 和 std::async 等设施基本一致。 构造函数和析构函数： 构造函数：初始化线程池并启动线程。 析构函数：停止线程池并等待所有线程结束。 外部接口： stop()：停止线程池，通知所有线程退出（不会等待所有任务执行完毕）。 submit()：将任务提交到任务","date":"2025-03-24","objectID":"/2025/80f2e62/:2:2","tags":["C++"],"title":"并发编程（三）","uri":"/2025/80f2e62/"},{"categories":["找工作"],"content":"《现代C++并发编程教程》 —— C++并发编程学习笔记（二） ","date":"2025-03-24","objectID":"/2025/4b155bd/:0:0","tags":["C++"],"title":"并发编程（二）","uri":"/2025/4b155bd/"},{"categories":["找工作"],"content":"等待事件或条件 假设你正在一辆夜间运行的地铁上，那么你要如何在正确的站点下车呢？ 1.一直不休息，每一站都能知道，这样就不会错过你要下车的站点，但是这会很疲惫。 这种方法被称为“忙等待（busy waiting）”也称 “自旋“。 bool flag = false; std::mutex m; void wait_for_flag() { std::unique_lock\u003cstd::mutex\u003e lk{ m }; while (!flag){ lk.unlock(); // 1 解锁互斥量 lk.lock(); // 2 上锁互斥量 } } 2.可以看一下时间，估算一下地铁到达目的地的时间，然后设置一个稍早的闹钟，就休息。这个方法听起来还行，但是你可能被过早的叫醒，甚至估算错误导致坐过站，又或者闹钟没电了睡过站。 第二种方法就是加个延时，这种实现进步了很多，减少浪费的执行时间，但很难确定正确的休眠时间。这会影响到程序的行为，在需要快速响应的程序中就意味着丢帧或错过了一个时间片。循环中，休眠 ② 前函数对互斥量解锁 ①，再休眠结束后再对互斥量上锁，让另外的线程有机会获取锁并设置标识（因为修改函数和等待函数共用一个互斥量）。 void wait_for_flag() { std::unique_lock\u003cstd::mutex\u003e lk{ m }; while (!flag){ lk.unlock(); // 1 解锁互斥量 std::this_thread::sleep_for(std::chrono::milliseconds(100)); // 2 休眠 lk.lock(); // 3 上锁互斥量 } } 3.事实上最简单的方式是，到站的时候有人或者其它东西能将你叫醒（比如手机的地图，到达设置的位置就提醒）。 第三种方式（也是最好的）实际上就是使用条件变量了。通过另一线程触发等待事件的机制是最基本的唤醒方式，这种机制就称为“条件变量”。 C++ 标准库对条件变量有两套实现：std::condition_variable 和 std::condition_variable_any，这两个实现都包含在 \u003ccondition_variable\u003e 这个头文件中。 condition_variable_any 类是 std::condition_variable 的泛化。相对于只在 std::unique_lock\u003cstd::mutex\u003e 上工作的 std::condition_variable，condition_variable_any 能在任何满足 可基本锁定(BasicLockable) 要求的锁上工作，所以增加了 _any 后缀。显而易见，这种区分必然是 any 版更加通用但是却有更多的性能开销。所以通常首选 std::condition_variable。有特殊需求，才会考虑 std::condition_variable_any。 std::mutex mtx; // 创建了一个互斥量，用于保护共享数据的访问，确保在多线程环境下的数据同步。 std::condition_variable cv; // 创建了一个条件变量，用于线程间的同步，当条件不满足时，线程可以等待，直到条件满足时被唤醒。 bool arrived = false; // 设置了一个标志位，表示是否到达目的地。 void wait_for_arrival() { std::unique_lock\u003cstd::mutex\u003e lck(mtx); // 使用互斥量创建了一个独占锁。 cv.wait(lck, []{ return arrived; }); // 阻塞当前线程，释放（unlock）锁，直到条件被满足。 std::cout \u003c\u003c \"到达目的地，可以下车了！\" \u003c\u003c std::endl; } void simulate_arrival() { std::this_thread::sleep_for(std::chrono::seconds(5)); // 模拟地铁到站，假设5秒后到达目的地 { std::lock_guard\u003cstd::mutex\u003e lck(mtx); arrived = true; // 设置条件变量为 true，表示到达目的地 } cv.notify_one(); // 通知等待的线程 } 这样，当 simulate_arrival 函数执行后，arrived 被设置为 true，并且通过 cv.notify_one() 唤醒了等待在条件变量上的线程，从而使得 wait_for_arrival 函数中的等待结束，可以执行后续的操作，即输出提示信息。 条件变量的 wait 成员函数有两个版本，以上代码使用的就是第二个版本，传入了一个谓词。 void wait(std::unique_lock\u003cstd::mutex\u003e\u0026 lock); // 1 template\u003cclass Predicate\u003e void wait(std::unique_lock\u003cstd::mutex\u003e\u0026 lock, Predicate pred); // 2 ②等价于： while (!pred()) wait(lock); 第二个版本只是对第一个版本的包装，等待并判断谓词，会调用第一个版本的重载。这可以避免 虚假唤醒 条件变量虚假唤醒是指在使用条件变量进行线程同步时，有时候线程可能会在没有收到通知的情况下被唤醒。问题取决于程序和系统的具体实现。解决方法很简单，在循环中等待并判断条件可一并解决。使用 C++ 标准库则没有这个烦恼了。 ","date":"2025-03-24","objectID":"/2025/4b155bd/:1:0","tags":["C++"],"title":"并发编程（二）","uri":"/2025/4b155bd/"},{"categories":["找工作"],"content":"线程安全的队列 这里介绍一个更为复杂的示例，用于巩固条件变量的学习。在实现一个线程安全的队列过程中，需要注意两点内容： 当执行 push 操作时，需要确保没有其他线程正在执行 push 或 pop 操作；同样，在执行 pop 操作时，也需要确保没有其他线程正在执行 push 或 pop 操作。 当队列为空时，不应该执行 pop 操作。因此，我们需要使用条件变量来传递一个谓词，以确保在执行 pop 操作时队列不为空。 以下是一个线程安全的模版类 threadsafe_queue： template\u003ctypename T\u003e class threadsafe_queue { mutable std::mutex m; // 互斥量，用于保护队列操作的独占访问 std::condition_variable data_cond; // 条件变量，用于在队列为空时等待 std::queue\u003cT\u003e data_queue; // 实际存储数据的队列 public: threadsafe_queue() {} // 无参构造 void push(T new_value) { { std::lock_guard\u003cstd::mutex\u003e lk { m }; data_queue.push(new_value); } data_cond.notify_one(); } // 从队列中弹出元素（阻塞直到队列不为空） void pop(T\u0026 value) { std::unique_lock\u003cstd::mutex\u003e lk{ m }; data_cond.wait(lk, [this] {return !data_queue.empty(); }); // 这里的 this 表示按值传递 this，见 lambda 表达式用法 value = data_queue.front(); data_queue.pop(); } // 从队列中弹出元素（阻塞直到队列不为空），并返回一个指向弹出元素的 shared_ptr std::shared_ptr\u003cT\u003e pop() { std::unique_lock\u003cstd::mutex\u003e lk{ m }; data_cond.wait(lk, [this] {return !data_queue.empty(); }); std::shared_ptr\u003cT\u003e res { std::make_shared\u003cT\u003e(data_queue.front()) }; data_queue.pop(); return res; } bool empty()const { std::lock_guard\u003cstd::mutex\u003e lk (m); return data_queue.empty(); } }; ","date":"2025-03-24","objectID":"/2025/4b155bd/:2:0","tags":["C++"],"title":"并发编程（二）","uri":"/2025/4b155bd/"},{"categories":["找工作"],"content":"使用 future 举个例子，我们在车站等车，你可能会做一些别的事情打发时间，比如学习现代C++并发编程教程、玩手机等，但始终在等待一件事情：车到站。 C++ 标准库将这种事件称为 future。它用于处理线程中需要等待某个事件的情况，线程知道预期结果。等待的同时也可以执行其它的任务。 C++ 标准库有两种 future，都声明在 \u003cfuture\u003e 头文件中：独占的 std::future 、共享的 std::shared_future。它们的区别与 std::unique_ptr 和 std::shared_ptr 类似。std::future 只能与单个指定事件关联，而 std::shared_future 能关联多个事件。它们都是模板，它们的模板类型参数，就是其关联的事件（函数）的返回类型。当多个线程需要访问一个独立 future 对象时， 必须使用互斥量或类似同步机制进行保护。而多个线程访问同一共享状态，若每个线程都是通过其自身的 shared_future 对象副本进行访问，则是安全的。 最简单有效的使用是，我们先前讲的 std::thread 在线程中执行任务是没有返回值的，这个问题就能使用 future 解决。 ","date":"2025-03-24","objectID":"/2025/4b155bd/:3:0","tags":["C++"],"title":"并发编程（二）","uri":"/2025/4b155bd/"},{"categories":["找工作"],"content":"创建异步任务获取返回值 假设需要执行一个耗时任务并获取其返回值，但是并不急切的需要它。那么就可以启动新线程计算，然而 std::thread 没提供直接从线程获取返回值的机制。所以我们可以使用 std::async 函数模板。 使用 std::async 启动一个异步任务，它会返回一个 std::future 对象，这个对象和任务关联，将持有最终计算出来的结果。当需要任务执行完的结果的时候，只需要调用 get() 成员函数，就会阻塞直到 future 为就绪为止（即任务执行完毕），返回执行结果。valid() 成员函数检查 future 当前是否关联共享状态，即是否当前关联任务。还未关联，或者任务已经执行完（调用了 get()、set()），都会返回 false。 #include \u003ciostream\u003e #include \u003cthread\u003e #include \u003cfuture\u003e // 引入 future 头文件 int task(int n) { std::cout \u003c\u003c \"异步任务 ID: \" \u003c\u003c std::this_thread::get_id() \u003c\u003c '\\n'; return n * n; } int main() { std::future\u003cint\u003e future = std::async(task, 10); std::cout \u003c\u003c \"main: \" \u003c\u003c std::this_thread::get_id() \u003c\u003c '\\n'; std::cout \u003c\u003c std::boolalpha \u003c\u003c future.valid() \u003c\u003c '\\n'; // true std::cout \u003c\u003c future.get() \u003c\u003c '\\n'; std::cout \u003c\u003c std::boolalpha \u003c\u003c future.valid() \u003c\u003c '\\n'; // false } 关于 std::async 的参数传递，这里不再展开记录，用时再查。 ","date":"2025-03-24","objectID":"/2025/4b155bd/:3:1","tags":["C++"],"title":"并发编程（二）","uri":"/2025/4b155bd/"},{"categories":["找工作"],"content":"信号量 信号量是一个非常轻量简单的同步设施（在 C++ 20中被引入），它维护一个计数，这个计数不能小于 0。信号量提供两种基本操作：释放（增加计数）和等待（减少计数）。如果当前信号量的计数值为 0，那么执行“等待”操作的线程将会一直阻塞，直到计数大于 0，也就是其它线程执行了 “释放” 操作。 C++ 提供了两个信号量类型：std::counting_semaphore 与 std::binary_semaphore，定义在 \u003csemaphore\u003e 中。其中 binary_semaphore 只是 counting_semaphore 的一个特化别名（其 LeastMaxValue 为1，LeastMaxValue 意思是信号量维护的计数最大值。）： using binary_semaphore = counting_semaphore\u003c1\u003e; 举个具体使用信号量的例子： // 全局二元信号量对象 // 设置对象初始计数为 0 std::binary_semaphore smph_signal_main_to_thread{ 0 }; std::binary_semaphore smph_signal_thread_to_main{ 0 }; void thread_proc() { smph_signal_main_to_thread.acquire(); std::cout \u003c\u003c \"[线程] 获得信号\" \u003c\u003c std::endl; std::this_thread::sleep_for(3s); std::cout \u003c\u003c \"[线程] 发送信号\\n\"; smph_signal_thread_to_main.release(); } int main() { std::jthread thr_worker{ thread_proc }; std::cout \u003c\u003c \"[主] 发送信号\\n\"; smph_signal_main_to_thread.release(); smph_signal_thread_to_main.acquire(); std::cout \u003c\u003c \"[主] 获得信号\\n\"; } 结果： [主] 发送信号 [线程] 获得信号 [线程] 发送信号 [主] 获得信号 acquire 函数就是我们先前说的“等待”（原子地减少计数），release 函数就是\"释放\"（原子地增加计数）。 提示 信号量常用于 发信/提醒 而非互斥，通过初始化该信号量为 0 从而阻塞尝试 acquire() 的接收者，直至提醒者通过调用 release(n) “发信”。在此方面可把信号量当作条件变量的替代品，通常它有更好的性能。 假设我们有一个 Web 服务器，它只能处理有限数量的并发请求。为了防止服务器过载，我们可以使用信号量来限制并发请求的数量。 // 定义一个信号量，最大并发数为 3 std::counting_semaphore\u003c3\u003e semaphore{ 3 }; // counting_semaphore 轻量同步原语，允许同一资源进行多个并发的访问，至少允许 LeastMaxValue 个同时访问者 void handle_request(int request_id) { // 请求到达，尝试获取信号量 std::cout \u003c\u003c \"进入 handle_request 尝试获取信号量\\n\"; semaphore.acquire(); std::cout \u003c\u003c \"成功获取信号量\\n\"; // 此处延时三秒可以方便测试，会看到先输出 3 个“成功获取信号量”，因为只有三个线程能成功调用 acquire，剩余的会被阻塞 std::this_thread::sleep_for(3s); // 模拟处理时间 std::random_device rd; std::mt19937 gen{ rd() }; std::uniform_int_distribution\u003c\u003e dis(1, 5); int processing_time = dis(gen); std::this_thread::sleep_for(std::chrono::seconds(processing_time)); std::cout \u003c\u003c std::format(\"请求 {} 已被处理\\n\", request_id); semaphore.release(); } int main() { // 模拟 10 个并发请求 std::vector\u003cstd::jthread\u003e threads; for (int i = 0; i \u003c 10; ++i) { threads.emplace_back(handle_request, i); } } 牢记信号量的基本的概念不变，计数的值不能小于 0，如果当前信号量的计数值为 0，那么执行 “等待”（acquire） 操作的线程将会一直阻塞。明白这点，那么就都不存在问题。 ","date":"2025-03-24","objectID":"/2025/4b155bd/:4:0","tags":["C++"],"title":"并发编程（二）","uri":"/2025/4b155bd/"},{"categories":["找工作"],"content":"《现代C++并发编程教程》 —— C++并发编程学习笔记（一） ","date":"2025-03-23","objectID":"/2025/acc27a1/:0:0","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"启动线程 #include \u003ciostream\u003e #include \u003cthread\u003e void hello() { printf(\"hello world!\\n\"); } int main() { std::thread my_thread(hello); } 可以传入函数对象，如上例所示。也可以传入类或者其他重载了 () （callable）运算符的对象，例如： class task { public: void operator()() const { do_something(); do_something_else(); } }; task f; std::thread my_thread(f); 但这里需要注意一个问题，由于 C++ 的语法问题，有时会造成歧义，例如： std::thread my_thread(task()); // 这会被认为是声明了一个返回值为 thread 的，名为 my_thread 的函数 这里最好使用 {} 运算符来创建一个 thread 对象，如： std::thread my_thread{task()}。同时也可以用匿名函数（lambda表达式）来创建线程： #include \u003ciostream\u003e #include \u003cthread\u003e int main() { std::thread thread{ [] {std::cout \u003c\u003c \"Hello World!\\n\"; } }; thread.join(); } 当一个线程对象创建时（即 std::thread 对象构造时）就开始执行传入的函数 f 了。 ","date":"2025-03-23","objectID":"/2025/acc27a1/:1:0","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"线程管理 启动线程后（构造 std::thread 对象），我们必须在线程的生命周期结束之前，即 std::thread::~thread 调用之前，决定它的执行策略，包括 join() 和 detach()。 ","date":"2025-03-23","objectID":"/2025/acc27a1/:2:0","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"join() 其中 join() 表示将阻塞关联的线程，直至执行完毕。内部实现会让 std::thread::joinable() 返回 false。否则会返回 true，执行 std::terminate()。 ","date":"2025-03-23","objectID":"/2025/acc27a1/:2:1","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"detach() 执行了 detach() 后，表示线程对象放弃了对线程资源的所有权，允许此线程的独立运行，在线程退出时释放所有分配的资源。通常不建议使用 detach()，可以用 join() 替代。 可以提供一个类，RAII（Resource Acquisition Initilization）地确保线程执行完成，线程对象正常析构释放资源： class thread_guard { std::thread\u0026 m_t; public: explicit thread_guard(std::thread\u0026 t) : m_t{ t } {} ~thread_guard() { std::puts(\"析构\"); // 打印日志 不用在乎 if (m_t.joinable()) { // 线程对象当前关联了活跃线程 m_t.join(); } } thread_guard(const thread_guard\u0026) = delete; thread_guard\u0026 operator=(const thread_guard\u0026) = delete; }; void f() { int n = 0; std::thread t{ func{n},10 }; thread_guard g(t); f2(); // 可能抛出异常 } ","date":"2025-03-23","objectID":"/2025/acc27a1/:2:2","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"传递参数 向可调用对象传递参数，只需要将这些参数作为 std::thread 的构造参数即可。 需要注意的是，这些参数会复制到新线程的内存空间中，即使函数中的参数是引用，依然实际是复制。 void f(int, const int\u0026 a); int n = 1; std::thread t{ f, 3, n }; 线程对象 t 的构造没有问题，可以通过编译，但是这个 n 实际上并没有按引用传递，而是按值复制的。如果我们的 f 的形参类型不是 const 的引用，则会产生一个编译错误。可以用标准库的 std::ref、std::cref 函数模版。 void f(int, int\u0026 a) { std::cout \u003c\u003c \u0026a \u003c\u003c '\\n'; } int main() { int n = 1; std::cout \u003c\u003c \u0026n \u003c\u003c '\\n'; std::thread t { f, 3, std::ref(n) }; t.join(); } ","date":"2025-03-23","objectID":"/2025/acc27a1/:3:0","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"共享数据 我们都知道线程通信的方式有临界区、互斥量、信号量、条件变量、读写锁： 临界区：每个线程中访问临界资源的那段代码称为临界区（Critical Section）（临界资源是一次仅允许一个线程使用的共享资源）。每次只准许一个线程进入临界区，进入后不允许其他线程进入。不论是硬件临界资源，还是软件临界资源，多个线程必须互斥地对它进行访问。在临界区中，通常会使用同步机制，比如我们要讲的互斥量（Mutex） 互斥量：采用互斥对象机制，只有拥有互斥对象的线程才可以访问。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。 信号量：计数器，允许多个线程同时访问同一个资源。 条件变量：通过条件变量通知操作的方式来保持多线程同步。 读写锁：读写锁与互斥量类似。但互斥量要么是锁住状态，要么就是不加锁状态。读写锁一次只允许一个线程写，但允许一次多个线程读，这样效率就比互斥锁要高。 如果有以下情况，出现数据竞争情况。 std::vector\u003cint\u003e v; void f() { v.emplace_back(1); } void f2() { v.erase(v.begin()); } int main() { std::thread t{ f }; std::thread t2{ f2 }; t.join(); t2.join(); std::cout \u003c\u003c v.size() \u003c\u003c '\\n'; // 有时出现段错误，有时输出0，不稳定的输出结果 } 这里我们可以用互斥量来解决这一问题。 #include \u003ciostream\u003e #include \u003cmutex\u003e #include \u003cthread\u003e #include \u003cvector\u003e std::mutex m; std::vector\u003cint\u003e v; void f() { m.lock(); v.emplace_back(1); m.unlock(); } void f2() { m.lock(); v.erase(v.begin()); m.unlock(); } int main() { std::thread t{ f }; std::thread t2{ f2 }; t.join(); t2.join(); std::cout \u003c\u003c v.size() \u003c\u003c '\\n'; // 稳定输出0 } 另外一个例子，使用 mutex 互斥量前： void f() { // this_thread::get_id() 表示获取当前线程的唯一标识符，以便在多线程程序中区分不同的线程。 std::cout \u003c\u003c std::this_thread::get_id() \u003c\u003c '\\n'; } int main() { std::vector\u003cstd::thread\u003e threads; for (std::size_t i = 0; i \u003c 10; ++i) threads.emplace_back(f); for (auto\u0026 thread : threads) thread.join(); } 这里有一个点，正好说明一下 push_back() 和 emplace_back() 的区别 如果要用 push_back，则需要先构造一个 thread 临时对象：threads.push_back(std::thread(f)); 而如果用 emplace_back，该方法允许在 vector 末尾直接构造对象，而无需创建临时对象。它接受构造函数的参数，并在适当的位置直接调用构造函数。这样可以减少不必要的对象创建和移动操作，提高性能：threads.emplace_back(f); std::mutex 和 std::shared_mutex 区别 std::mutex： 独占锁，同一时刻只能有一个线程访问线程资源 std::shared_mutex：读写锁，允许多个线程同时读取，但写入时会独占（C++ 17）引入 使用后： #include \u003cmutex\u003e // 必要标头 std::mutex m; void f() { m.lock(); std::cout \u003c\u003c std::this_thread::get_id() \u003c\u003c '\\n'; m.unlock(); } int main() { std::vector\u003cstd::thread\u003ethreads; for (std::size_t i = 0; i \u003c 10; ++i) threads.emplace_back(f); for (auto\u0026 thread : threads) thread.join(); } 当多个线程执行函数 f 的时候，只有一个线程能成功调用 lock() 给互斥量上锁，其他所有的线程 lock() 的调用将阻塞执行，直至获得锁。第一个调用 lock() 的线程得以继续往下执行，执行我们的 std::cout 输出语句，不会有任何其他的线程打断这个操作。直到线程执行 unlock()，就解锁了互斥量。那么其他线程此时也就能再有一个成功调用 lock。至于是哪个线程才会成功调用，这个是由操作系统调度决定的 这里我理解的是，“锁” 是一种广泛的概念，可以有多种实现方式，c++ 中的互斥量 mutex 可以用来实现锁 ","date":"2025-03-23","objectID":"/2025/acc27a1/:4:0","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"std::lock_guard 一般来说，不建议直接使用互斥量 mutex 显式地进行 lock() 和 unlock()。可以用 C++11 标准引入的管理类 std::lock_guard: void f() { std::lock_guard\u003cstd::mutex\u003e lc{ m }; // 等价于 m.lock()，超出作用域调用析构来 unlock std::cout \u003c\u003c std::this_thread::get_id() \u003c\u003c '\\n'; } std::lock_guard 实现比较简单，可以看它在 MSVC STL 中的实现。 我们要尽可能的让互斥量上锁的粒度小，只用来确保必须的共享资源的线程安全。 “粒度”通常用于描述锁定的范围大小，较小的粒度意味着锁定的范围更小，因此有更好的性能和更少的竞争。 比如有的时候可以看到这样的写法： void f() { //code.. { std::lock_guard\u003cstd::mutex\u003e lc{ m }; // 涉及共享资源的修改的代码... } //code.. } 使用 {} 创建了一个块作用域，限制了对象 lc 的生存期，进入作用域构造 lock_guard 的时候上锁（lock），离开作用域析构的时候解锁（unlock）。 举一个具体的例子： std::mutex m; void add_to_list(int n, std::list\u003cint\u003e\u0026 list) { std::vector\u003cint\u003e numbers(n + 1); std::iota(numbers.begin(), numbers.end(), 0); // iota是对vector进行递增（默认递增1）赋值的方法，0是起始值 int sum = std::accumulate(numbers.begin(), numbers.end(), 0); // 0是起始值 { std::lock_guard\u003cstd::mutex\u003e lc{ m }; list.push_back(sum); } } void print_list(const std::list\u003cint\u003e\u0026 list){ std::lock_guard\u003cstd::mutex\u003e lc{ m }; for(const auto\u0026 i : list){ std::cout \u003c\u003c i \u003c\u003c ' '; } std::cout \u003c\u003c '\\n'; } // ...... // std::list\u003cint\u003e list; std::thread t1{ add_to_list,i,std::ref(list) }; // 上面提到过，传参即使是引用，也会被复制，需要用 std::ref std::thread t2{ add_to_list,i,std::ref(list) }; std::thread t3{ print_list,std::cref(list) }; // const 引用需要用 std::cref std::thread t4{ print_list,std::cref(list) }; t1.join(); t2.join(); t3.join(); t4.join(); 这里的共享数据只有 list， 先看 add_to_list，只有 list.push_back(sum) 涉及到了对共享数据的修改，需要进行保护，因此我们用 {} 包裹。 函数 print_list() 打印 list，给整个函数上锁，同一时刻只能有一个线程执行。我们代码是多个线程执行这两个函数，两个函数共享了一个锁，这样确保了当执行函数 print_list() 打印的时候，list 的状态是确定的。打印函数 print_list 和 add_to_list 函数的修改操作同一时间只能有一个线程在执行。print_list() 不可能看到正在被 add_to_list() 修改的 list。 至于到底哪个函数哪个线程会先执行，执行多少次，这些都由操作系统调度决定，也完全有可能连续 4 次都是执行函数 print_list 的线程成功调用 lock，会打印出了一样的值，这都很正常。 ","date":"2025-03-23","objectID":"/2025/acc27a1/:4:1","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"try_lock try_lock 是互斥量中的一种尝试上锁的方式。与常规的 lock 不同，try_lock 会尝试上锁，但如果锁已经被其他线程占用，则不会阻塞当前线程，而是立即返回。 它的返回类型是 bool ，如果上锁成功就返回 true，失败就返回 false。 这种方法在多线程编程中很有用，特别是在需要保护临界区的同时，又不想线程因为等待锁而阻塞的情况下。 std::mutex mtx; void thread_function(int id) { // 尝试加锁 if (mtx.try_lock()) { std::cout \u003c\u003c \"线程：\" \u003c\u003c id \u003c\u003c \" 获得锁\" \u003c\u003c std::endl; // 临界区代码 std::this_thread::sleep_for(std::chrono::milliseconds(100)); // 模拟临界区操作 mtx.unlock(); // 解锁 std::cout \u003c\u003c \"线程：\" \u003c\u003c id \u003c\u003c \" 释放锁\" \u003c\u003c std::endl; } else { std::cout \u003c\u003c \"线程：\" \u003c\u003c id \u003c\u003c \" 获取锁失败 处理步骤\" \u003c\u003c std::endl; } } 如果有两个线程运行这段代码，必然有一个线程无法成功上锁，要走 else 的分支。 std::thread t1(thread_function, 1); std::thread t2(thread_function, 2); t1.join(); t2.join(); 可能的运行结果： 线程：1 获得锁 线程：2 获取锁失败 处理步骤 线程：1 释放锁 小心 切勿将受保护数据的指针或引用传递到互斥量作用域之外，不然保护将形同虚设。下面是一个具体例子 class Data { int a{}; std::string b{}; public: void do_something() { // 修改数据成员等... } }; class Data_wrapper { Data data; std::mutex m; public: template\u003cclass Func\u003e void process_data(Func func) { std::lock_guard\u003cstd::mutex\u003e lc{m}; func(data); // 受保护数据传递给函数 } }; Data* p = nullptr; void malicious_function(Data\u0026 protected_data) { p = \u0026protected_data; // 受保护的数据被传递到外部 } Data_wrapper d; void foo() { d.process_data(malicious_function); // 传递了一个恶意的函数 p-\u003edo_something(); // 在无保护的情况下访问保护数据 } 成员函数模板 process_data 看起来一点问题也没有，使用 std::lock_guard 对数据做了保护，但是调用方传递了 malicious_function 这样一个恶意的函数，使受保护数据传递给外部，可以在没有被互斥量保护的情况下调用 do_something()。 ","date":"2025-03-23","objectID":"/2025/acc27a1/:4:2","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"死锁：问题与解决 两个线程需要对它们所有的互斥量做一些操作，其中每个线程都有一个互斥量，且等待另一个线程的互斥量解锁。因为它们都在等待对方释放互斥量，没有线程工作。 这种情况就是死锁。一般只有多个互斥量才会遇到死锁问题 死锁发生的四个必要条件 互斥（资源一次只能被一个线程占用） 持有且等待（线程已持有资源，并等待其他资源） 不可抢占（已持有资源不能被强制释放） 循环等待（多个线程互相等待对方释放资源） 避免死锁的一般建议是让两个互斥量以相同的顺序上锁，总在互斥量 B 之前锁住互斥量 A，就通常不会死锁。反面示例： std::mutex m1,m2; std::size_t n{}; void f() { std::lock_guard\u003cstd::mutex\u003e lc1{ m1 }; std::lock_guard\u003cstd::mutex\u003e lc2{ m2 }; ++n; } void f2() { std::lock_guard\u003cstd::mutex\u003e lc1{ m2 }; std::lock_guard\u003cstd::mutex\u003e lc2{ m1 }; ++n; } f 与 f2 因为互斥量上锁顺序不同，就有死锁风险。函数 f 先锁定 m1，然后再尝试锁定 m2，而函数 f2 先锁定 m2 再锁定 m1 。如果两个线程同时运行，它们就可能（具体获得锁的顺序由操作系统调度决定，上面阐述过）会彼此等待对方释放其所需的锁，从而造成死锁。 但有时候即使固定了锁的顺序，依旧会产生问题。当有多个互斥量保护同一个类的对象时，对于相同类型的两个不同对象进行数据的交换操作，为了保证数据交换的正确性，就要避免其它线程修改，确保每个对象的互斥量都锁住自己要保护的区域。如果按照前面的的选择一个固定的顺序上锁解锁，则毫无意义，比如： struct X { X(const std::string\u0026 str) :object{ str } {} friend void swap(X\u0026 lhs, X\u0026 rhs); private: std::string object; std::mutex m; }; void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) return; std::lock_guard\u003cstd::mutex\u003e lock1{ lhs.m }; std::lock_guard\u003cstd::mutex\u003e lock2{ rhs.m }; swap(lhs.object, rhs.object); } 考虑用户调用的时候将参数交换，就会产生死锁： X a{ \"🤣\" }, b{ \"😅\" }; std::thread t{ [\u0026] {swap(a, b); } }; // 1 std::thread t2{ [\u0026] {swap(b, a); } }; // 2 1 执行的时候，先上锁 a 的互斥量，再上锁 b 的互斥量。 2 执行的时候，先上锁 b 的互斥量，再上锁 a 的互斥量。 完全可能线程 A 执行 1 的时候上锁了 a 的互斥量，线程 B 执行 2 上锁了 b 的互斥量。线程 A 往下执行需要上锁 b 的互斥量，线程 B 则要上锁 a 的互斥量执行完毕才能解锁，哪个都没办法往下执行，死锁。 如何解决？可以使用 C++ 标准库中的 std::lock，它能一次性锁住多个互斥量，并且没有死锁风险。修改后 swap 代码如下： void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) return; std::lock(lhs.m, rhs.m); // 给两个互斥量上锁 std::lock_guard\u003cstd::mutex\u003e lock1{ lhs.m,std::adopt_lock }; std::lock_guard\u003cstd::mutex\u003e lock2{ rhs.m,std::adopt_lock }; swap(lhs.object, rhs.object); } 因为前面已经使用了 std::lock 上锁，所以后面的 std::lock_guard 构造都额外传递了一个 std::adopt_lock 参数，让其选择到不上锁的构造函数。函数退出也能正常解锁。 std::lock 给 lhs.m 或 rhs.m 上锁时若抛出异常，则在重抛前对任何已锁的对象调用 unlock() 解锁，也就是 std::lock 要么将互斥量都上锁，要么一个都不锁。 C++17 新增了 std::scoped_lock ，提供此函数的 RAII 包装，通常它比裸调用 std::lock 更好。 所以我们前面的代码可以改写为： void swap(X\u0026 lhs, X\u0026 rhs) { if (\u0026lhs == \u0026rhs) return; std::scoped_lock guard{ lhs.m,rhs.m }; swap(lhs.object, rhs.object); } 使用 std::scoped_lock 可以将所有 std::lock 替换掉，减少错误发生。也可以用 std::unique_lock（可以简单理解为 std::lock_guard 的升级版，具有额外的功能，为更复杂的锁做准备），详情见标准库文档。 总结，避免死锁要注意： 避免嵌套锁：线程获取一个锁时，就别再获取第二个锁。每个线程只持有一个锁，自然不会产生死锁。如果必须要获取多个锁，使用 std::lock。 避免在持有锁时调用外部代码 使用固定顺序获取锁 ","date":"2025-03-23","objectID":"/2025/acc27a1/:5:0","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"读写锁 如果需要多线程读取写（多线程读不存在数据竞争；而写和读共存时存在竞争），使用 std::mutex 开销较大。这时可以用专门的读写锁，即 std::shared_timed_mutex (C++ 14)，std::shared_mutex (C++ 17)。示例代码： class Settings { private: std::map\u003cstd::string, std::string\u003e data_; mutable std::shared_mutex mutex_; // “M\u0026M 规则”：mutable 与 mutex 一起出现 public: void set(const std::string\u0026 key, const std::string\u0026 value) { std::lock_guard\u003cstd::shared_mutex\u003e lock{ mutex_ }; data_[key] = value; } std::string get(const std::string\u0026 key) const { std::shared_lock\u003cstd::shared_mutex\u003e lock(mutex_); auto it = data_.find(key); return (it != data_.end()) ? it-\u003esecond : \"\"; // 如果没有找到键返回空字符串 } }; ","date":"2025-03-23","objectID":"/2025/acc27a1/:6:0","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"使用互斥量实现并发读写锁（字节 Data AML 一面手撕题） 我们知道，写操作是独占的；而读操作是非独占的，即多个线程可以同时读。如果多线程访问某个共享变量时，每次访问时都加上一个互斥锁，这样开销会非常大。 所以我们期望在多个线程试图读取共享变量的时候，它们可以立刻获取因为读而加的锁，而不是需要等待前一个线程释放。当然，如果一个线程用写锁锁住了临界区，那么其他线程无论是读还是写都会发生阻塞。 #include \u003ciostream\u003e #include \u003cmutex\u003e #include \u003cthread\u003e class ReadWriteLock { private: std::mutex readMutex; // 用于保护读操作计数器的互斥量 std::mutex writeMutex; // 用于保护写操作的互斥量 int readCount = 0; // 记录当前正在进行读操作的线程数量 bool writing = false; // 表示当前是否有线程正在进行写操作 public: void readLock() { // 锁定读操作互斥量 std::unique_lock\u003cstd::mutex\u003e readLock(readMutex); // 增加读操作计数器 ++ readCount; // 等待没有写操作时进行读操作 while (writing) { readLock.unlock(); std::this_thread::yield(); // 让出CPU，避免忙等待 readLock.lock(); } } void readUnlock() { // 锁定读操作互斥量并减少读操作计数器 std::lock_guard\u003cstd::mutex\u003e readLock(readMutex); -- readCount; } void writeLock() { // 锁定写操作互斥量 std::unique_lock\u003cstd::mutex\u003e writeLock(writeMutex); // 等待没有读操作和写操作时进行写操作 while (readCount \u003e 0 || writing) { writeLock.unlock(); std::this_thread::yield(); // 让出CPU，避免忙等待 writeLock.lock(); } // 标记正在进行写操作 writing = true; } void writeUnlock() { // 锁定写操作互斥量并标记写操作完成 std::lock_guard\u003cstd::mutex\u003e writeLock(writeMutex); writing = false; } }; // 示例使用 int sharedData = 0; ReadWriteLock lock; void reader() { lock.readLock(); std::cout \u003c\u003c \"Reading: \" \u003c\u003c sharedData \u003c\u003c std::endl; lock.readUnlock(); } void writer() { lock.writeLock(); ++ sharedData; std::cout \u003c\u003c \"Writing: \" \u003c\u003c sharedData \u003c\u003c std::endl; lock.writeUnlock(); } int main() { std::thread readers[5]; std::thread writers[3]; // 创建多个读线程和写线程 for (int i = 0; i \u003c 5; ++i) { readers[i] = std::thread(reader); } for (int i = 0; i \u003c 3; ++i) { writers[i] = std::thread(writer); } // 等待所有线程执行完毕 for (auto\u0026 reader : readers) { reader.join(); } for (auto\u0026 writer : writers) { writer.join(); } return 0; } 这段代码中，锁定操作用的是 std::unique_lock，而解锁操作用的是 std::lock_guard。这两者都是 C++ 11 引入的 RAII 包装。正如上面所述，unique_lock 是 lock_guard 的升级版（更灵活，常与条件变量的 wait()、notify_one()、notify_all()配合使用），这里锁定时需要解锁和重新锁定互斥量。这对于需要在等待条件满足时解锁互斥量并让出 CPU 的场景非常有用。 ","date":"2025-03-23","objectID":"/2025/acc27a1/:6:1","tags":["C++"],"title":"并发编程（一）","uri":"/2025/acc27a1/"},{"categories":["找工作"],"content":"介绍 C++ 中的内存管理分配、管理以及智能指针的原理及使用 ","date":"2025-03-23","objectID":"/2025/01dc872/:0:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"程序地址空间 C++ 中，内存分布分为五块区域，分别是：栈；堆；全局变量和静态变量（存放于 data 段和 bss 段）；常量；代码； 上图是内核和用户的虚拟地址空间分布情况，其中，局部变量和参数等都存放在栈中，这部分空间由系统进行管理；而堆中空间主要是用于用户调用 new或 malloc 时分配的空间。这部分区域由用户管理，因此容易造成内存泄漏。 ","date":"2025-03-23","objectID":"/2025/01dc872/:1:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"代码区 也就是 .text 段， 代码区存放程序的二进制代码，它是只读的，以防止程序在运行过程中被意外修改。 #include \u003ciostream\u003e int main() { std::cout \u003c\u003c \"Hello, World!\" \u003c\u003c std::endl; return 0; } 比如上面这段代码中的 main 函数，编译为二进制后，函数的逻辑就存放在代码区。 当然这段区域也有可能包含一些只读的常数变量，例如字符串常量等。 ","date":"2025-03-23","objectID":"/2025/01dc872/:1:1","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"全局/静态存储区 全局变量和静态变量都存放在全局/静态存储区。以前在 C 语言中全局变量又分为初始化的和未初始化的，分别放在上面图中的 .bss 和 .data 段，但在 C++里面没有这个区分了，他们共同占用同一块内存区，就叫做全局存储区。这个区域的内存在程序的生命周期几乎都是全局的，举例: #include \u003ciostream\u003e int globalVar = 0; // 全局变量 void function() { static int staticVar = 0; // 静态变量 staticVar++; std::cout \u003c\u003c staticVar \u003c\u003c std::endl; } int main() { function(); function(); return 0; } globalVar 是一个全局变量，staticVar 是一个静态变量，它们都存放在全局/静态存储区。 ","date":"2025-03-23","objectID":"/2025/01dc872/:1:2","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"栈区 栈区用于存储函数调用时的局部变量、函数参数以及返回地址。 当函数调用完成后，分配给这个函数的栈空间会被释放。例如 #include \u003ciostream\u003e void function(int a, int b) { int localVar = a + b; std::cout \u003c\u003c localVar \u003c\u003c std::endl; } int main() { function(3, 4); return 0; } 在这个例子中，a、b和localVar都是局部变量，它们存放在栈区。 当 function 函数调用结束后，对应的函数栈所占用的空间(参数 a、b，局部变量 localVar等)都会被回收。 ","date":"2025-03-23","objectID":"/2025/01dc872/:1:3","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"堆区 堆区是用于动态内存分配的区域，当使用new（C++）或者malloc（C）分配内存时，分配的内存块就位于堆区。 我们需要手动释放这些内存，否则可能导致内存泄漏。例如： #include \u003ciostream\u003e int main() { int* dynamicArray = new int[10]; // 动态分配内存 // 使用动态数组... delete[] dynamicArray; // 释放内存 return 0; } ","date":"2025-03-23","objectID":"/2025/01dc872/:1:4","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"常量区 常量区用于存储常量数据，例如字符串字面量和其他编译时常量。这个区域通常也是只读的。例如： #include \u003ciostream\u003e int main() { char* c=\"abc\"; // abc在常量区，c在栈上。 return 0; } 总结：代码和数据是分开存储的；堆和栈的不同区别；全局变量和局部变量的存储区别 ","date":"2025-03-23","objectID":"/2025/01dc872/:1:5","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"C++指针与引用区别 指针和引用在 C++ 中都用于间接访问变量，但它们有一些区别 指针是一个变量，它保存了另一个变量的内存地址；引用是另一个变量的别名，与原变量共享内存地址。 指针(除指针常量)可以被重新赋值，指向不同的变量；引用在初始化后不能更改，始终指向同一个变量。 指针可以为 nullptr，表示不指向任何变量；引用必须绑定到一个变量，不能为 nullptr。 使用指针需要对其进行解引用以获取或修改其指向的变量的值；引用可以直接使用，无需解引用。 指针与引用在汇编层面完全等价 引用只是C++语法糖，可以看作编译器自动完成取地址、解引用的指针常量 引用区别于指针的特性都是编译器约束完成的，一旦编译成汇编就喝指针一样 由于引用只是指针包装了下，所以也存在风险，比如如下代码: int *a = new int; int \u0026b = *a; delete a; b = 12; // 对已经释放的内存解引用 引用由编译器保证初始化，使用起来较为方便(如不用检查空指针等) 尽量用引用代替指针 引用没有顶层const即int \u0026 const，因为引用本身就不可变，所以在加顶层const也没有意义； 但是可以有底层const即 const int\u0026，这表示引用所引用的对象本身是常量 指针既有顶层const(int * const–指针本身不可变)，也有底层const(const int *–指针所指向的对象不可变) 有指针引用–是引用，绑定到指针， 但是没有引用指针–这很显然，因为很多时候指针存在的意义就是间接改变对象的值，但是引用本身的值我们上面说过了是所引用对象的地址，但是引用不能更改所引用的对象，也就当然不能有引用指针了。 指针和引用的自增（++）和自减含义不同，指针是指针运算, 而引用是代表所指向的对象对象执行++或– ","date":"2025-03-23","objectID":"/2025/01dc872/:2:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"C++指针传递、值传递、引用传递 在 C++ 中，函数参数传递有三种常见的方式：值传递、引用传递和指针传递。以下分别给出这三种方式的示例： ","date":"2025-03-23","objectID":"/2025/01dc872/:3:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"值传递 值传递是将实参的值传递给形参。在这种情况下，函数内对形参的修改不会影响到实参。 ","date":"2025-03-23","objectID":"/2025/01dc872/:3:1","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"引用传递 引用传递是将实参的引用传递给形参。在这种情况下，函数内对形参的修改会影响到实参。 #include \u003ciostream\u003e void swap_reference(int \u0026a, int \u0026b) { int temp = a; a = b; b = temp; } int main() { int x = 10; int y = 20; swap_reference(x, y); std::cout \u003c\u003c \"x: \" \u003c\u003c x \u003c\u003c \", y: \" \u003c\u003c y \u003c\u003c std::endl; // 输出：x: 20, y: 10 return 0; } ","date":"2025-03-23","objectID":"/2025/01dc872/:3:2","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"指针传递 指针传递是将实参的地址传递给形参。在这种情况下，函数内对形参的修改会影响到实参。 #include \u003ciostream\u003e void swap_pointer(int *a, int *b) { int temp = *a; *a = *b; *b = temp; } int main() { int x = 10; int y = 20; swap_pointer(\u0026x, \u0026y); std::cout \u003c\u003c \"x: \" \u003c\u003c x \u003c\u003c \", y: \" \u003c\u003c y \u003c\u003c std::endl; // 输出：x: 20, y: 10 return 0; } ","date":"2025-03-23","objectID":"/2025/01dc872/:3:3","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"C++ RAII RAII 即 Resource Acquisition Is Initialization，资源获取即初始化。是一种 C++ 编程技术，它将在使用前获取（分配的堆内存、执行线程、打开的套接字、打开的文件、锁定的互斥量、磁盘空间、数据库连接等有限资源）的资源的生命周期与某个对象的生命周期绑定在一起。确保在控制对象的生命周期结束时，按照资源获取的相反顺序释放所有资源。同样，如果资源获取失败（构造函数退出并带有异常），则按照初始化的相反顺序释放所有已完全构造的成员和基类子对象所获取的资源。这利用了核心语言特性（对象生命周期、作用域退出、初始化顺序和堆栈展开），以消除资源泄漏并确保异常安全。 核心思想：利用栈上局部变量的自动析构来保证资源一定会被释放 实现步骤 设计一个类封装资源，资源可以是内存、文件、socket、锁等等一切 在构造函数中执行资源的初始化，比如申请内存、打开文件、申请锁 在析构函数中执行销毁操作，比如释放内存、关闭文件、释放锁 使用时声明一个该对象的类，一般在你希望的作用域声明即可，比如在函数开始，或者作为类的成员变量 下面写一个 RAII 示例，用 RAII 思想包装 mutex: #include \u003ciostream\u003e #include \u003cmutex\u003e #include \u003cthread\u003e class LockGuard { public: explicit LockGuard(std::mutex \u0026mtx) : mutex_(mtx) { mutex_.lock(); } ~LockGuard() { mutex_.unlock(); } // 禁止复制 LockGuard(const LockGuard \u0026) = delete; LockGuard \u0026operator=(const LockGuard \u0026) = delete; private: std::mutex \u0026mutex_; }; // 互斥量 std::mutex mtx; // 多线程操作的变量 int shared_data = 0; void increment() { for (int i = 0; i \u003c 10000; ++i) { // 申请锁 LockGuard lock(mtx); ++shared_data; // 作用域结束后会析构 然后释放锁 } } int main() { std::thread t1(increment); std::thread t2(increment); t1.join(); t2.join(); std::cout \u003c\u003c \"Shared data: \" \u003c\u003c shared_data \u003c\u003c std::endl; return 0; } 上面定义了一个 LockGuard 类，该类在构造函数中接收一个互斥量（mutex）引用并对其进行锁定，在析构函数中对互斥量进行解锁。这样，我们可以将互斥量传递给 LockGuard 对象，并在需要保护的代码块内创建该对象，确保在执行保护代码期间始终正确锁定和解锁互斥量。在 main 函数中，用两个线程同时更新一个共享变量，通过 RAII 包装的 LockGuard 确保互斥量的正确使用。 ","date":"2025-03-23","objectID":"/2025/01dc872/:4:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"智能指针 智能指针分为三类：shared_ptr，unique_ptr，weak_ptr（c98还引入了 auto_ptr，但已在 c++11中被废弃） unique_ptr unique_ptr 表示专属所有权，用 unique_ptr 管理的内存，只能被一个对象持有。故不支持复制和赋值。 auto w = std::make_unique\u003cWidget\u003e(); // 在 c++14 中，可以用 make_unique 方法来构造。 auto w2 = w; // 编译错误 因此只能通过移动来更改专属所有权： auto w = std::make_unique\u003cWidget\u003e(); auto w2 = std::move(w); // w2 获得内存所有权，w 此时等于 nullptr 用法：需要引入头文件 \u003cmemory\u003e，可以使用右值拷贝构造或 make 方法来构造指针。 unique_ptr\u003cint\u003e p1 = make_unique\u003cint\u003e(100); unique_ptr\u003cstring\u003e ps1(new string(\"good luck\")); 适用场景 忘记 delete class Box { public: Box() : w(new Widget()) {} ~Box() { // 析构函数中忘记 delete w } private: Widget* w; }; 异常安全 void process() { Widget* w = new Widget(); w-\u003edo_something(); // 如果发生异常，那么 delete w 将不会执行，此时就会发生内存泄露 delete w; // 也可以用 try...catch 块捕捉异常，并在 catch 语句中 delete，但是不太美观 + 容易漏写 } shared_ptr shared_ptr 代表的是共享所有权，即多个 shared_ptr 可以共享同一块内存。shared_ptr 内部是利用引用计数来实现内存的自动管理，每当复制一个 shared_ptr，引用计数会 + 1。当一个 shared_ptr 离开作用域时，引用计数会 - 1。当引用计数为 0 的时候，则 delete 内存。 auto w = std::make_shared\u003cWidget\u003e(); auto w2 = w; cout \u003c\u003c w.use_count() \u003c\u003c endl; // g++ -std=c++11 main main.cc output-\u003e2 同时，shared_ptr 也支持移动。从语义上来看，移动指的是所有权的传递。如下： auto w = std::make_shared\u003cWidget\u003e(); auto w2 = std::move(w); // 此时 w 等于 nullptr，w2.use_count() 等于 1 注意 shared_ptr 性能开销更大，几乎是 unique_ptr 的两倍（因为还要维护一个计数） 考虑到线程安全问题，引用计数的增减必须是原子操作。而原子操作一般情况下都比非原子操作慢 使用移动优化性能，尽量使用 std::move 来将 shared_ptr 转移给新对象。因为移动不用增加引用计数，性能更好 使用场景：通常用于指定，有可能多个对象同时管理一个内存的时候。 weak_ptr weak_ptr 是为了解决 shared_ptr 双向引用的问题。即： class B; struct A { shared_ptr\u003cB\u003e b; }; struct B { shared_ptr\u003cA\u003e a; }; auto pa = make_shared\u003cA\u003e(); auto pb = make_shared\u003cB\u003e(); pa-\u003eb = pb; pb-\u003ea = pa; pa 和 pb 存在着循环引用，根据 shared_ptr 引用计数的原理，pa 和 pb 都无法被正常的释放。 对于这种情况, 我们可以使用 weak_ptr： class B; struct A { shared_ptr\u003cB\u003e b; }; struct B { weak_ptr\u003cA\u003e a; }; auto pa = make_shared\u003cA\u003e(); auto pb = make_shared\u003cB\u003e(); pa-\u003eb = pb; pb-\u003ea = pa; weak_ptr 不会增加引用计数，因此可以打破 shared_ptr 的循环引用。 通常做法是 parent 类持有 child 的 shared_ptr, child 持有指向 parent 的 weak_ptr。这样也更符合语义。 实现过程 鉴于看到面经中有同学被问到过智能指针的底层实现，因此这里给出两种智能指针（weak_ptr略）的简单实现方式。 Tips this 本身是一个指针，指向该类实例化后的对象本身；*this表示解引用，C++中对一个指针进行解引用，得到的是当前对象的引用，也就是对象本身 注意这里 (*this-\u003e_count) ++ 的用法 注意这里 = delete 的语法，用于显示地禁用特定的函数 shared_ptr： template\u003ctypename T\u003e class shared_ptr { private: T* _ptr; int* _count; // 引用计数 public: // 构造函数 shared_ptr(T* ptr = nullptr) : _ptr(ptr) { if (_ptr) _count = new int(1); else _count = new int(10); } // 拷贝构造 shared_ptr(const shared_ptr\u0026 ptr) { if (this != ptr) { this-\u003e_ptr = ptr._ptr; this-\u003e_count = ptr._count; (*this-\u003e_count) ++ ; } } // 重载operator= shared_ptr\u0026 operator=(const shared_ptr \u0026 ptr) { if (this-\u003e_ptr == ptr._ptr) { return *this; } if (this-\u003e_ptr) { (*this-\u003e_count) -- ; if (*this-\u003e_count == 0) { delete this-\u003e_ptr; delete this-\u003e_count; } } this-\u003e_ptr = ptr._ptr; this-\u003e_count = ptr._count; (*this-\u003e_count) ++ ; return *this; } // operator*重载 T\u0026 operator*() { if (this-\u003e_ptr) { return *(this-\u003e_ptr); } } // operator-\u003e重载 T* operator-\u003e() { if (this-\u003e_ptr) { return this-\u003e_ptr; } } // 析构函数 ~shared_ptr() { (*this-\u003e_count) -- ; if (*this-\u003e_count == 0) { delete this-\u003e_ptr; delete this-\u003e_count; } } // 返回引用计数 int use_count() { return *this-\u003e_count; } }; unique_ptr： template\u003ctypename T\u003e class unique_ptr { private: T* _ptr; public: // 构造函数 unique_ptr(T* ptr = nullptr) : _ptr(ptr) {} // 析构函数 ~unique_ptr() { del() }; // 先释放资源（如果持有），再持有资源 void reset(T* ptr) { del(); _ptr = ptr; } // 返回资源，资源的释放由调用方处理 T* release() { T* ptr = _ptr; _ptr = nullptr; return ptr; } // 获取资源，调用方应该只使用不释放，否则会两次delete资源 T* get() { return _ptr; } private: // 释放 void del() { if (_ptr == nullptr) return; delete _ptr; _ptr = nullptr; } // 禁用拷贝构造 unique_ptr(const unique_ptr \u0026) = delete; // 禁用拷贝赋值 unique_ptr\u0026 operator = (const unique_ptr \u0026) = delete; }; ","date":"2025-03-23","objectID":"/2025/01dc872/:5:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"malloc-free 内存分配原理 关于动态内存管理，主要就是两块内容： 虚拟内存机制：物理和虚拟地址空间、TLB 页表、内存映射 动态内存管理：内存管理、分配方式、内存回收、GC等等 主要通过两种方式： step1：从内存池中分配。若所需要内存 \u003c 128KB，则从内存池中尝试分配。若无，则进行 brk 系统调用，从堆上申请内存。 step2：若 \u003e 128KB，不看内存池，直接使用 mmap 系统调用，从文件映射区（同时还存放动态库）中获得内存。 ","date":"2025-03-23","objectID":"/2025/01dc872/:6:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"malloc() 分配的是物理内存吗？（重点） 不是的，malloc() 分配的是虚拟内存。 如果分配后的虚拟内存没有被访问的话，虚拟内存是不会映射到物理内存的，这样就不会占用物理内存了。 只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发缺页中断，然后操作系统会建立虚拟内存和物理内存之间的映射关系。 ","date":"2025-03-23","objectID":"/2025/01dc872/:6:1","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"malloc(1) 会分配多大的虚拟内存？ malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是会预分配更大的空间作为内存池。 ","date":"2025-03-23","objectID":"/2025/01dc872/:6:2","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"free 释放内存，会归还给操作系统吗？ malloc 通过 brk() 方式申请的内存，free 释放内存的时候，并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用； malloc 通过 mmap() 方式申请的内存，free 释放内存的时候，会把内存归还给操作系统，内存得到真正的释放。 ","date":"2025-03-23","objectID":"/2025/01dc872/:6:3","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"为什么不全部使用 mmap 来分配内存？ 因为向操作系统申请内存，是要通过系统调用的，执行系统调用是要进入内核态的，然后在回到用户态，运行态的切换会耗费不少时间。 所以，申请内存的操作应该避免频繁的系统调用，如果都用 mmap 来分配内存，等于每次都要执行系统调用。 另外，因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在第一次访问该虚拟地址的时候，就会触发缺页中断。 也就是说，频繁通过 mmap 分配的内存话，不仅每次都会发生运行态的切换，还会发生缺页中断（在第一次访问虚拟地址后），这样会导致 CPU 消耗较大。 ","date":"2025-03-23","objectID":"/2025/01dc872/:6:4","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"为什么不全部使用 brk 来分配内存？ 前面我们提到通过 brk 从堆空间分配的内存，并不会归还给操作系统，那么我们那考虑这样一个场景。 如果我们连续申请了 10k，20k，30k 这三片内存，如果 10k 和 20k 这两片释放了，变为了空闲内存空间（不会归还给操作系统），如果下次申请的内存小于 30k，那么就可以重用这个空闲内存空间。 但是如果下次申请的内存大于 30k，没有可用的空闲内存空间，必须向 OS 申请，实际使用内存继续增大。 因此，随着系统频繁地 malloc 和 free，尤其对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 valgrind 是无法检测出来的。 所以，malloc 实现中，充分考虑了 brk 和 mmap 行为上的差异及优缺点，默认分配大块内存 (128KB) 才使用 mmap 分配内存空间。 ","date":"2025-03-23","objectID":"/2025/01dc872/:6:5","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"free() 函数只传入一个内存地址，为什么能知道要释放多大的内存？ malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多了 16 字节，这样当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。 ","date":"2025-03-23","objectID":"/2025/01dc872/:6:6","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"malloc/free 与 new/delete区别 语法不同：malloc/free是一个C语言的函数，而new/delete是C++的运算符。 分配内存的方式不同：malloc只分配内存，而new会分配内存并且调用对象的构造函数来初始化对象。 返回值不同：malloc返回一个 void 指针，需要自己强制类型转换，而new返回一个指向对象类型的指针。 malloc 需要传入需要分配的大小，而 new 编译器会自动计算所构造对象的大小 ","date":"2025-03-23","objectID":"/2025/01dc872/:7:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"申请的内存所在位置 new 操作符从自由存储区（free store）上为对象动态分配内存空间，而malloc函数从堆上动态分配内存。自由存储区是 C++ 基于new操作符的一个抽象概念，凡是通过 new 操作符进行内存申请，该内存即为自由存储区。而堆是操作系统中的术语，是操作系统所维护的一块特殊内存，用于程序的内存动态分配，C语言使用malloc从堆上分配内存，使用free释放已分配的对应内存。那么自由存储区是否能够是堆（问题等价于new是否能在堆上动态分配内存），这取决于operator new 的实现细节。自由存储区不仅可以是堆，还可以是静态存储区，这都看operator new在哪里为对象分配内存。 ","date":"2025-03-23","objectID":"/2025/01dc872/:7:1","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"内存分配失败时返回值 new内存分配失败时，会抛出bac_alloc异常，它不会返回NULL；malloc分配内存失败时返回NULL // malloc int *a = (int *)malloc ( sizeof (int )); if(NULL == a) { ... } else { ... } // new try { int *a = new int(); } catch (bad_alloc) { ... } ","date":"2025-03-23","objectID":"/2025/01dc872/:7:2","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"是否调用构造函数/析构函数 使用new操作符来分配对象内存时会经历三个步骤： 第一步：调用operator new 函数（对于数组是operator new[]）分配一块足够大的，原始的，未命名的内存空间以便存储特定类型的对象。 第二步：编译器运行相应的构造函数以构造对象，并为其传入初值。 第三步：对象构造完成后，返回一个指向该对象的指针。 使用delete操作符来释放对象内存时会经历两个步骤： 第一步：调用对象的析构函数。 第二步：编译器调用operator delete(或operator delete[])函数释放内存空间 ","date":"2025-03-23","objectID":"/2025/01dc872/:7:3","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"对数组的处理 C++ 提供了 new[] 与 delete[] 来专门处理数组类型: A * ptr = new A[10];//分配10个A对象 使用 new[] 分配的内存必须使用 delete[] 进行释放： delete [] ptr; new 对数组的支持体现在它会分别调用构造函数函数初始化每一个数组元素，释放对象时为每个对象调用析构函数。注意 delete[] 要与new[] 配套使用，不然会找出数组对象部分释放的现象，造成内存泄漏。至于 malloc，它并知道你在这块内存上要放的数组还是啥别的东西，反正它就给你一块原始的内存，在给你个内存的地址就完事。所以如果要动态分配一个数组的内存，还需要我们手动自定数组的大小： int * ptr = (int *) malloc( sizeof(int)* 10 );//分配一个10个int元素的数组 ","date":"2025-03-23","objectID":"/2025/01dc872/:7:4","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"new 和 malloc 是否可以相互调用 operator new /operator delete的实现可以基于malloc，而malloc的实现不可以去调用new。下面是编写operator new /operator delete 的一种简单方式，其他版本也与之类似： void * operator new (sieze_t size) { if(void * mem = malloc(size) return mem; else throw bad_alloc(); } void operator delete(void *mem) noexcept { free(mem); } ","date":"2025-03-23","objectID":"/2025/01dc872/:7:5","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"能够直观地重新分配内存 使用 malloc 分配的内存后，如果在使用过程中发现内存不足，可以使用 realloc 函数进行内存重新分配实现内存的扩充。 realloc 先判断当前的指针所指内存是否有足够的连续空间，如果有，原地扩大可分配的内存地址，并且返回原来的地址指针； 如果空间不够，先按照新指定的大小分配空间，将原有数据从头到尾拷贝到新分配的内存区域，而后释放原来的内存区域。 new 没有这样直观的配套设施来扩充内存。 总结 特征 new/delete malloc/free 分配内存的位置 自由存储区 堆 内存分配成功的返回值 完整类型指针 void* 内存分配失败的返回值 默认抛出异常，为bad_alloc类型 返回NULL 分配内存的大小 由编译器根据类型计算得出 必须显式指定字节数 处理数组 有处理数组的new版本new[] 需要用户计算数组的大小后进行内存分配 已分配内存的扩充 无法直观地处理 使用realloc简单完成 是否相互调用 可以，看具体的operator new/delete实现 不可调用new 分配内存时内存不足 客户能够指定处理函数或重新制定分配器 无法通过用户代码进行处理 函数重载 允许 不允许 构造函数与析构函数 调用 不调用 ","date":"2025-03-23","objectID":"/2025/01dc872/:7:6","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"C/C++ 内存泄露如何定位、检测以及避免 ","date":"2025-03-23","objectID":"/2025/01dc872/:8:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"内存泄露是什么？ 简单来说就是：在程序中申请了动态内存，却没有释放，如果程序长期运行下去，最终会导致没有内存可供分配。 ","date":"2025-03-23","objectID":"/2025/01dc872/:8:1","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"如何检测？ 手动检查代码：仔细检查代码中的内存分配和释放，确保每次分配内存后都有相应的释放操作。比如 malloc和free、new和delete是否配对使用了。 使用调试器和工具：有一些工具可以帮助检测内存泄露。例如： Valgrind（仅限于Linux和macOS）：Valgrind是一个功能强大的内存管理分析工具，可以检测内存泄露、未初始化的内存访问、数组越界等问题。使用Valgrind分析程序时，只需在命令行中输入valgrind –leak-check=yes your_program即可。 Visual Studio中的CRT（C Runtime）调试功能：Visual Studio提供了一些用于检测内存泄露的C Runtime库调试功能。例如，_CrtDumpMemoryLeaks函数可以在程序结束时报告内存泄露。 AddressSanitizer：AddressSanitizer是一个用于检测内存错误的编译器插件，适用于GCC和Clang。要启用AddressSanitizer，只需在编译时添加-fsanitize=address选项。 ","date":"2025-03-23","objectID":"/2025/01dc872/:8:2","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"如何避免内存泄露 使用智能指针（C++）：在C++中，可以使用智能指针（如std::unique_ptr和std::shared_ptr）来自动管理内存。这些智能指针在作用域结束时会自动释放所指向的内存，从而降低忘记释放内存或者程序异常导致内存泄露的风险。 异常安全：在C++中，如果程序抛出异常，需要确保在异常处理过程中正确释放已分配的内存。使用try-catch块来捕获异常并在适当的位置释放内存。 或者使用RAII（Resource Acquisition Is Initialization）技术（关于RAII可以看这篇文章: 如何理解RAII，将资源（如内存）的管理与对象的生命周期绑定。 ","date":"2025-03-23","objectID":"/2025/01dc872/:8:3","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"C/C++ 野指针和空悬指针 野指针（Wild Pointer）和空悬指针（Dangling Pointer）都是指向无效内存的指针，但它们的成因和表现有所不同，区别如下： ","date":"2025-03-23","objectID":"/2025/01dc872/:9:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"野指针 (Wild Pointer) 野指针是一个未被初始化或已被释放的指针。 所以它的值是不确定的，可能指向任意内存地址。 访问野指针可能导致未定义行为，如程序崩溃、数据损坏等。 以下是一个野指针的例子： #include \u003ciostream\u003e int main() { int *wild_ptr; // 未初始化的指针，值不确定 std::cout \u003c\u003c *wild_ptr \u003c\u003c std::endl; // 访问野指针，可能导致未定义行为 return 0; } ","date":"2025-03-23","objectID":"/2025/01dc872/:9:1","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"空悬指针（Dangling Pointer） 空悬指针是指向已经被释放（如删除、回收）的内存的指针。 这种指针仍然具有以前分配的内存地址，但是这块内存可能已经被其他对象或数据占用。 访问空悬指针同样会导致未定义行为。 以下是一个空悬指针的例子： #include \u003ciostream\u003e int main() { int *ptr = new int(42); delete ptr; // 释放内存 // 此时，ptr成为一个空悬指针，因为它指向的内存已经被释放 std::cout \u003c\u003c *ptr \u003c\u003c std::endl; // 访问空悬指针，可能导致未定义行为 return 0; } 为了避免野指针和空悬指针引发的问题，我们应该： 在使用指针前对其进行初始化，如将其初始化为nullptr。 在释放指针指向的内存后，将指针设为nullptr，避免误访问已释放的内存。 在使用指针前检查其有效性，确保指针指向合法内存。 ","date":"2025-03-23","objectID":"/2025/01dc872/:9:2","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"常见的 C/C++ 内存错误 ","date":"2025-03-23","objectID":"/2025/01dc872/:10:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"间接引用坏指针 程的虚拟地址空间中有较大的空洞，没有映射到任何有意义的数据。如果我们试图间接引用一个指向这些洞的指针，那么操作系统就会以段异常中止程序。而且，虚拟内存的某些区域是只读的，试图写这些区域将会以保护异常中止这个程序。间接引用坏指针的一个常见示例是经典的 scanf 错误。假设我们想要使用 scanf 从 stdin 读一个整数到一个变量。正确的方法是传递给 scanf 一个格式串和变量的地址： int val; scanf(\"%d\", \u0026val) 在这种情况下，scanf 将把 val 的内容解释为一个地址，并试图将一个字写到这个位置。在最好的情况下，程序立即以异常终止。在最糟糕的情况下，val 的内容对应于虚拟内存的某个合法的读/写区域，于是我们就覆盖了这块内存。 ","date":"2025-03-23","objectID":"/2025/01dc872/:10:1","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"读未初始化的内存 虽然 bss 内存位置（诸如未初始化的全局 C 变量）总是被加载器初始化为零，但是对于堆内存却并不是这样的。 一个常见的错误就是假设堆内存被初始化为零： /* Return y = Ax */ int *matvec(int **A, int *x, int n) { int i, j; int *y = (int *)Malloc(n * sizeof(int)); for (i = 0; i \u003c n; i++) for (j = 0; j \u003c n; j++) y[i] += A[i][j] * x[j]; return y; } 在这个示例中，程序员不正确地假设向量 y 被初始化为零。正确的实现方式是显式地将 y[i] 设置为零，或者使用 calloc。 ","date":"2025-03-23","objectID":"/2025/01dc872/:10:2","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"栈缓冲区溢出 如果一个程序不检查输入串的大小就写入栈中的目标缓冲区，那么这个程序就会有缓冲区溢出错误（buffer overflow bug）。例如，下面的函数就有缓冲区溢出错误，因为 gets 函数复制一个任意长度的串到缓冲区。为了纠正这个错误，必须使用 fgets 函数，这个函数限制了输入串的大小： void bufoverflow() { char buf[64]; gets(buf); /* Here is the stack buffer overflow bug */ return; } ","date":"2025-03-23","objectID":"/2025/01dc872/:10:3","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"误解指针运算 另一种常见的错误是忘记了指针的算术操作是以它们指向的对象的大小为单位来进行的，而这种大小単位并不一定是字节。 例如，下面函数的目的是扫描一个 int 的数组，并返回一个指针，指向 val 的首次出现： int *search(int *p, int val) { while (*p \u0026\u0026 *p != val) p += sizeof(int); /* Should be p++ */ return p; } ","date":"2025-03-23","objectID":"/2025/01dc872/:10:4","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"引用不存在的变量 int *stackref () { int val; return \u0026val; } 这个函数返回一个指针（比如说是 p），指向栈里的一个局部变量，然后弹出它的栈帧。 尽管 p 仍然指向一个合法的内存地址，但是它已经不再指向一个合法的变量了。 当以后在程序中调用其他函数时，内存将重用它们的栈帧。再后来，如果程序分配某个值给 *p，那么它可能实际上正在修改另一个函数的栈帧中的一个条目，从而潜在地带来灾难性的、令人困惑的后果。 ","date":"2025-03-23","objectID":"/2025/01dc872/:10:5","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"引起内存泄漏 内存泄漏是缓慢、隐性的杀手，当程序员不小心忘记释放已分配块，而在堆里创建了垃圾时，会发生这种问题。例如，下面的函数分配了一个堆块 x，然后不释放它就返回： void leak(int n) { int *x = (int *)Malloc(n * sizeof(int)); return; /* x is garbage at this point */ } ","date":"2025-03-23","objectID":"/2025/01dc872/:10:6","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"C++ nullptr 和 NULL的区别 在 C++11 之前，我们通常使用 NULL 来表示空指针。 然而，在 C++ 中，NULL 的定义实际上是一个整数值 0，而不是一个真正的指针类型。 在函数重载和模板编程中这可能会导致一些问题和歧义。 为了解决这个问题，C++11 引入了一个新的关键字 nullptr，用于表示空指针。 nullptr 是一种特殊类型的字面值，类型为 std::nullptr_t，定义为: typedef decltype(nullptr) nullptr_t，可以隐式转换为任何指针类型。 与 NULL 不同，nullptr 是一个真正的指针类型，因此可以避免一些由于 NULL 是整数类型而引起的问题。 以下是 nullptr 和 NULL 之间区别的一些例子： 函数重载 #include \u003ciostream\u003e void foo(int x) { std::cout \u003c\u003c \"foo() called with an int: \" \u003c\u003c x \u003c\u003c std::endl; } void foo(char* x) { std::cout \u003c\u003c \"foo() called with a char*: \" \u003c\u003c x \u003c\u003c std::endl; } int main() { // foo(NULL); // 编译错误：因为 NULL 会被解析为整数 0，导致二义性 foo(nullptr); // 无歧义：调用 void foo(char* x) } 函数模版 #include \u003ciostream\u003e #include \u003ctype_traits\u003e template \u003ctypename T\u003e void bar(T x) { if (std::is_same\u003cT, std::nullptr_t\u003e::value) { std::cout \u003c\u003c \"bar() called with nullptr\" \u003c\u003c std::endl; } else { std::cout \u003c\u003c \"bar() called with a non-nullptr value\" \u003c\u003c std::endl; } } int main() { bar(NULL); // 输出：bar() called with a non-nullptr value，因为 NULL 被解析为整数 0 bar(nullptr); // 输出：bar() called with nullptr } 总之，C++ 11 引入了 nullptr 作为一个更安全、更明确的空指针表示，可以避免与整数 0（即 NULL）相关的一些问题。在 C++11 及以后的代码中，建议使用 nullptr 代替 NULL 表示空指针。 ","date":"2025-03-23","objectID":"/2025/01dc872/:11:0","tags":["C++"],"title":"C++内存管理","uri":"/2025/01dc872/"},{"categories":["找工作"],"content":"C++多态的实现方法及原理 ","date":"2025-03-23","objectID":"/2025/b97727d/:0:0","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"虚函数 https://zhuanlan.zhihu.com/p/54145222 https://zhuanlan.zhihu.com/p/629281871 ","date":"2025-03-23","objectID":"/2025/b97727d/:1:0","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"概念解释 用一个例子理解虚函数的作用： Animal* catAnimal = \u0026cat Animal\u0026 dogAnimal = dog; catAnimal-\u003espeak() dogAnimal.speak() // 调用的还是基类 Animal 本身的方法 // 为什么要用基类指针或引用来完成？基类能够动态确定其实际所指向的派生类对象，并调用合适版本的方法， // 那么一个函数就可以解决上面的问题 // 用虚函数来完成上述功能 class Animal { public: // ... // virtual string speak() const { return \"???\"; } } class Cat { public: // ... // virtual string speak() const { return \"Meow\"; } } class Dog { public: // ... // virtual string speak() const { return \"Woof\"; } } Animal 类被 Cat 和 Dog类继承并覆盖了 speak 函数以实现不同的行为。当使用 Animal的指针或引用来调用 speak 函数时，会根据运行时的对象类型来动态地决定调用哪个子类的函数，从而实现多态性。 ","date":"2025-03-23","objectID":"/2025/b97727d/:1:1","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"实现原理 C++ 中，虚函数的实现原理基于两个概念：虚函数表和虚函数指针。 虚函数表 每个包含虚函数的类，都会生成一个虚函数表（Virtual Table），存储着该类中所有的虚函数的地址。虚函数表是一个由指针构成的数组，每个指针指向一个虚函数的实现代码。 虚函数指针 在对象内存布局中，编译器会添加一个额外的指针，称为虚函数指针或虚表指针（Virtual Table Pointer，a.k.a VTable指针）。这个指针指向该对象对应的虚函数表，从而让程序能够动态地调用正确的虚函数。 虚函数指针可以类比操作系统中，虚拟内存映射中的页表基址，存储在页表基址寄存器（xv6 是 satp 寄存器）中，有了页表基址，就可以找到一级页表，从而找到二级页表，进而找到物理地址。 当一个基类指针或引用调用虚函数时，编译器会使用虚表指针来查找该对象对应的虚函数表，并根据函数在虚函数表中的位置来调用正确的虚函数。但同时由于虚函数表的存在，导致需要额外的存储空间来存储虚函数表及其指针，导致 C++ 在调用虚函数时比其他语言成本要高。 虚函数指针是实现多级继承的关键，在多级继承中，每个子类都需要维护自己的虚函数表及其虚函数指针 虚函数的调用过程 在编译期间，编译器会根据函数调用的类型和对象的类型确定要调用的函数。 在运行期间，程序会根据对象的实际类型来决定调用哪个函数。这个过程叫做动态绑定或者后期绑定。 程序通过虚函数表（vtable）来实现动态绑定。每个含有虚函数的类都有自己的虚函数表，存储了指向实际函数地址的指针。在对象被创建时，它的指针会指向所属类的虚函数表。 当调用虚函数时，在对象中存储的指针会被解引用，获取到虚函数表的地址。然后根据函数调用的类型，从虚函数表中获取相应的函数地址。 最后，程序跳转到函数地址处执行实际的代码。由于是动态绑定，所以调用的函数是根据对象实际类型来决定的。 ","date":"2025-03-23","objectID":"/2025/b97727d/:1:2","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"虚函数的使用 在 C++ 中，派生类可以重写 (override) 它继承的虚函数，这被称为函数的覆盖 (overriding)。当然，子类也可以选择不重写基类的虚函数，那么它将默认继承基类的实现，这就是虚函数的重载 (overloading)。 class Base { public: virtual void foo() { std::cout \u003c\u003c \"Base::foo()\" \u003c\u003c std::endl; } }; class Derived : public Base { public: void foo() { std::cout \u003c\u003c \"Derived::foo()\" \u003c\u003c std::endl; } }; int main() { Derived obj; Base* ptr = \u0026obj; ptr-\u003efoo(); // 输出：Derived::foo() return 0; } 可以看到，不论是基类版本还是派生类版本，我们都在函数前面使用了 virtual 关键字，事实上，派生类中的 virtual 关键字并不是必要的。一旦基类中的方法打上了 virtual 标签，那么派生类中匹配的函数也是虚函数。但是，还是建议在后面的派生类中加上 virtual 关键字，作为虚函数的一种提醒，以便后面可能还会有更远的派生。 子类中重写虚函数时，访问权限不能更严格（即不能由 public 变为 private 或 protected），否则编译器会报错； 虚函数的覆盖实际上是通过指定 override 关键字显示声明来实现的。例如： class Base { public: virtual void foo() { std::cout \u003c\u003c \"Base::foo()\" \u003c\u003c std::endl; } }; class Derived : public Base { public: void foo() override { std::cout \u003c\u003c \"Derived::foo()\" \u003c\u003c std::endl; } }; int main() { Derived obj; Base* ptr = \u0026obj; ptr-\u003efoo(); // 输出：Derived::foo() return 0; } 进一步地，一般来说派生类需要重写基类的方法，以便于用基类指针动态调用不同派生类的成员方法，但是一旦函数签名不同，就会导致重写失败。为了避免可能发生的小错误导致重写失败无法调用派生类的成员方法，需要在派生类的成员方法后添加 override： class Super { public: virtual string getName1(int x) { return \"Super\"; } virtual string getName2(int x) { return \"Super\"; } }; class Sub: public Super{ public: virtual string getName1(double x) override { return \"Sub\"; } virtual string getName2(int x) const override { return \"Sub\"; }// 此时无法编译 }; ","date":"2025-03-23","objectID":"/2025/b97727d/:1:3","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"纯虚函数 纯虚函数是指在基类中定义的，没有实现的虚函数。这里的 “=0” 表示该函数为虚函数。 virtual void func() = 0; 纯虚函数的作用是让子类必须实现该函数，并且不能直接创建该类的对象（即该类为抽象类）。 抽象类是包含纯虚函数的类，它们不能被实例化，只能被继承。 抽象类只能用作其他类的基类。如果一个类继承了抽象类，则必须实现所有的纯虚函数，否则该类也会成为抽象类。 示例代码： class Shape{ public: // 纯虚函数 virtual double getArea() = 0; }; // 继承自抽象类Shape class Rectangle: public Shape { public: double width; double height; double getArea() {return width * height;} }; // 继承自抽象类Shape class Circle: public Shape { public: double radius; double getArea() {return 3.14*radius*radius;} }; ","date":"2025-03-23","objectID":"/2025/b97727d/:2:0","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"动态绑定与静态绑定 通过以上描述，我们可以得知虚函数可以用来进行动态绑定（区分于静态绑定）。 // 静态绑定示例 class Shape { public: void draw() { cout \u003c\u003c \"Drawing a shape.\" \u003c\u003c endl; } }; class Circle : public Shape { public: void draw() { cout \u003c\u003c \"Drawing a circle.\" \u003c\u003c endl; } }; int main() { Shape* shapeObj = new Circle(); shapeObj-\u003edraw(); // 编译时期确定方法调用，输出 \"Drawing a shape.\" } // 动态绑定示例 class Shape { public: virtual void draw() { cout \u003c\u003c \"Drawing a shape.\" \u003c\u003c endl; } }; class Circle : public Shape { public: void draw() { cout \u003c\u003c \"Drawing a circle.\" \u003c\u003c endl; } }; int main() { Shape* shapeObj = new Circle(); shapeObj-\u003edraw(); // 运行时期确定方法调用，输出 \"Drawing a circle.\" } ","date":"2025-03-23","objectID":"/2025/b97727d/:3:0","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"静态多态与动态多态 静态多态（也称为编译时多态）是指在编译时就能够确定函数或方法的调用对象，即函数或方法的重载。在静态多态中，函数或方法的重载是通过参数类型、参数数量或参数顺序来区分的。 int add(int a, int b){ return a + b; } double add(double a, double b){ return a + b; } 当调用 add() 方法时，编译器会根据传递给方法的参数类型来决定使用哪个重载版本。 动态多态（也称为运行时多态）是指在程序运行时才能确定函数或方法的调用对象，即虚函数或抽象类。在动态多态中，函数或方法的重载是通过继承和多态来实现的。见上面的虚函数代码样例。 ","date":"2025-03-23","objectID":"/2025/b97727d/:4:0","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"一些常见问题 ","date":"2025-03-23","objectID":"/2025/b97727d/:5:0","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"虚析构函数 我们知道析构函数存在的必要性之一就是，如果类内有指针类型变量，需要在析构函数中进行手动释放（delete ptr）。但是如果用基类指针指向子类对象，当子类实例被删除时，只会调用基类的析构函数，而不会调用子类的析构函数，从而使得子类中动态分配的内存无法被释放造成内存泄漏。这个时候需要使用虚析构函数来释放内存。 ","date":"2025-03-23","objectID":"/2025/b97727d/:5:1","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"虚函数的性能影响 根据上面所述，使用虚函数能够达到动态绑定的目的，这同时会增加一些开销，降低执行效率。但是现代编译器能够将开销优化至可以忽略不计。 ","date":"2025-03-23","objectID":"/2025/b97727d/:5:2","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["找工作"],"content":"多重继承中的虚函数 class Base1 { public: virtual void func() { cout \u003c\u003c \"Base1::func()\" \u003c\u003c endl; } }; class Base2 { public: virtual void func() { cout \u003c\u003c \"Base2::func()\" \u003c\u003c endl; } }; class Derived : public Base1, public Base2 { public: virtual void func() { Base1::func(); Base2::func(); } }; 一个类同时继承多个基类，并且这些基类中有多个同名虚函数，那么子类中必须对这些虚函数进行重写。 我理解是，如果是单继承，那么可以重写也可以不重写，不重写相当于就是继承基类的实现；而多继承中为了避免未知的错误，必须对每个基类虚函数进行重写。 ","date":"2025-03-23","objectID":"/2025/b97727d/:5:3","tags":["C++"],"title":"虚函数与多态","uri":"/2025/b97727d/"},{"categories":["算法"],"content":"由 randA() 实现 randB()：万能构造法 randA() 构造 randB() 时，需要找一个最大质因子不超过 A 的数 n (n\u003e=B），然后对 n 分解质因子就能找到每个采样需要取多少种结果。实际到具体数字时，可以把部分质因子合并成不超过 A 的数，从而减少采样次数。 举个具体例子，如何用 rand7() 来构造 rand10() ","date":"2025-03-22","objectID":"/2025/b44918a/:0:0","tags":["构造"],"title":"构造 RandX()","uri":"/2025/b44918a/"},{"categories":["算法"],"content":"确定采样参数 步骤 1：选择一个合适的数 n 我们选择 n = 30，因为它大于或等于 10，并且它的最大质因子是 5，不超过 7。 步骤 2：对 n 进行质因子分解 将 30 分解为质因子的乘积形式： $$ 30 = 2^1 \\times 3^1 \\times 5^1 $$ 步骤 3：确定采样次数和结果数量 根据质因子分解的结果，我们需要进行 3 次采样，每次采样的结果数量分别为 2、3 和 5。（分别对应三个质因子） ","date":"2025-03-22","objectID":"/2025/b44918a/:1:0","tags":["构造"],"title":"构造 RandX()","uri":"/2025/b44918a/"},{"categories":["算法"],"content":"进行采样 我们将进行 3 次采样，每次采样使用 rand7() 函数来生成一个介于 1 和 7 之间的随机数。 第一次采样 使用 rand7() 来生成一个随机数，如果结果在 [1, 2] 范围内，则将其作为第一次采样的结果。否则，重新采样。 int first; while (true) { first = rand7(); if (first \u003c= 2) break; } 第二次采样 使用 rand7() 来生成一个随机数，如果结果在 [1, 3] 范围内，则将其作为第一次采样的结果。否则，重新采样。 int second; while (true) { second = rand7(); if (second \u003c= 3) break; } 第三次采样 使用 rand7() 来生成一个随机数，如果结果在 [1, 5] 范围内，则将其作为第一次采样的结果。否则，重新采样。 int third; while (true) { third = rand7(); if (third \u003c= 5) break; } ","date":"2025-03-22","objectID":"/2025/b44918a/:2:0","tags":["构造"],"title":"构造 RandX()","uri":"/2025/b44918a/"},{"categories":["算法"],"content":"组合结果 将每次采样的结果组合起来，得到一个长度为 30 的序列。具体来说： first 的取值范围是 [1, 2]，总共有 2 种可能。 second 的取值范围是 [1, 3]，总共有 3 种可能。 third 的取值范围是 [1, 5]，总共有 5 种可能。 我们通过三次采样得到三个值：first、second 和 third。我们的目标是将这三个值组合成一个唯一的索引，这个索引应该对应于一个长度为 30 的序列中的一个位置。 这三个值的所有可能组合数是：$ 2\\times 3\\times 5 = 30 $， 这正好等于我们预定义的序列长度。 ","date":"2025-03-22","objectID":"/2025/b44918a/:3:0","tags":["构造"],"title":"构造 RandX()","uri":"/2025/b44918a/"},{"categories":["算法"],"content":"映射 为了将这三个值组合成一个唯一的索引，我们需要为每个值分配一个权重，使得它们的组合能够覆盖从 1 到 30 的所有整数。 公式为： $$ index = (first - 1)\\times 3 \\times 5 + (second - 1)\\times 5 + (third - 1) + 1 $$代表： $$[0, 1]\\times 3\\times 5 \\rightarrow [0, 15]$$$$[0, 2]\\times 5 \\rightarrow [0, 10]$$$$[0, 4] + 1 \\rightarrow [1, 5]$$","date":"2025-03-22","objectID":"/2025/b44918a/:4:0","tags":["构造"],"title":"构造 RandX()","uri":"/2025/b44918a/"},{"categories":["算法"],"content":"完整代码 int rand10() { int first, second, third; while (true) { first = rand7(); // 第一次采样 if (first \u003c= 2) break; // 如果结果在 [1, 2] 范围内，则退出循环 } while (true) { second = rand7(); // 第二次采样 if (second \u003c= 3) break; // 如果结果在 [1, 3] 范围内，则退出循环 } while (true) { third = rand7(); // 第三次采样 if (third \u003c= 5) break; // 如果结果在 [1, 5] 范围内，则退出循环 } // 将结果组合并映射到 [1, 10] 范围内 int index = (first - 1) * 3 * 5 + (second - 1) * 5 + (third - 1) + 1; if (index \u003c= 10) return index; else return rand10(); // 如果结果超出范围，则重新采样 } ","date":"2025-03-22","objectID":"/2025/b44918a/:5:0","tags":["构造"],"title":"构造 RandX()","uri":"/2025/b44918a/"},{"categories":["随记"],"content":"建站踩坑过程 ","date":"2025-03-22","objectID":"/2025/b487c0a/:0:0","tags":null,"title":"建站踩坑记录","uri":"/2025/b487c0a/"},{"categories":["随记"],"content":"集成 latex 方法一 ","date":"2025-03-22","objectID":"/2025/b487c0a/:1:0","tags":null,"title":"建站踩坑记录","uri":"/2025/b487c0a/"},{"categories":["随记"],"content":"优秀参考 https://github.com/shuzang/shuzang.github.io ","date":"2025-03-22","objectID":"/2025/b487c0a/:2:0","tags":null,"title":"建站踩坑记录","uri":"/2025/b487c0a/"},{"categories":["随记"],"content":"content 目录位置 移动到左边：https://blog.csdn.net/Xuyiming564445/article/details/122011603 ","date":"2025-03-22","objectID":"/2025/b487c0a/:3:0","tags":null,"title":"建站踩坑记录","uri":"/2025/b487c0a/"},{"categories":["随记"],"content":"评论系统 使用 giscus，将 repo 的discussion 模块用于评论。详情见： http://www.icharm.me/hugo-loveit-using/index.zh_cn.html https://giscus.app/zh-CN ","date":"2025-03-22","objectID":"/2025/b487c0a/:4:0","tags":null,"title":"建站踩坑记录","uri":"/2025/b487c0a/"},{"categories":null,"content":"关于我 MLSys 入门小菜鸡 ","date":"0001-01-01","objectID":"/about/:1:0","tags":null,"title":"关于","uri":"/about/"}]